=~=~=~=~=~=~=~=~=~=~=~= PuTTY log 2020.01.24 23:29:12 =~=~=~=~=~=~=~=~=~=~=~=
Using username "root".
Authenticating with public key "rsa-key-20200124" from agent
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 16:45:50 2020 from 49.37.206.100
root@master1:~# root@master1:~# 
root@master1:~# kubectl get nodes
NAME      STATUS     ROLES    AGE   VERSION
master3   Ready      <none>   13m   v1.13.0
worker1   NotReady   <none>   31m   v1.13.0
root@master1:~# 
root@master1:~# 
root@master1:~# kubeclkubectl get nodes
NAME      STATUS     ROLES    AGE   VERSION
master3   Ready      <none>   16m   v1.13.0
worker1   NotReady   <none>   35m   v1.13.0
root@master1:~# 
root@master1:~# 
root@master1:~# kkubectl getjournalctl `\-=xu kkewtetcd.service -b
-- Logs begin at Fri 2020-01-24 17:56:31 UTC, end at Fri 2020-01-24 18:01:43 UTC. --
Jan 24 17:56:41 master1 systemd[1]: Started etcd.
Jan 24 17:56:42 master1 etcd[1320]: etcd Version: 3.3.9
Jan 24 17:56:42 master1 etcd[1320]: Git SHA: fca8add78
Jan 24 17:56:42 master1 etcd[1320]: Go Version: go1.10.3
Jan 24 17:56:42 master1 etcd[1320]: Go OS/Arch: linux/amd64
Jan 24 17:56:42 master1 etcd[1320]: setting maximum number of CPUs to 2, total number of available CPUs is 2
Jan 24 17:56:42 master1 etcd[1320]: the server is already initialized as member before, starting as etcd member...
Jan 24 17:56:42 master1 etcd[1320]: peerTLS: cert = /etc/etcd/etcd-server.crt, key = /etc/etcd/etcd-server.key, ca = , truste
Jan 24 17:56:42 master1 etcd[1320]: listening for peers on https://172.31.122.25:2380
Jan 24 17:56:42 master1 etcd[1320]: listening for client requests on 127.0.0.1:2379
Jan 24 17:56:42 master1 etcd[1320]: listening for client requests on 172.31.122.25:2379
Jan 24 17:56:42 master1 etcd[1320]: name = master1
Jan 24 17:56:42 master1 etcd[1320]: data dir = /var/lib/etcd
Jan 24 17:56:42 master1 etcd[1320]: member dir = /var/lib/etcd/member
Jan 24 17:56:42 master1 etcd[1320]: heartbeat = 100ms
Jan 24 17:56:42 master1 etcd[1320]: election = 1000ms
Jan 24 17:56:42 master1 etcd[1320]: snapshot count = 100000
Jan 24 17:56:42 master1 etcd[1320]: advertise client URLs = https://172.31.122.25:2379
Jan 24 17:56:42 master1 etcd[1320]: restarting member c54eae14ec78233 in cluster 1f9cddabdc85ecd2 at commit index 4898
Jan 24 17:56:42 master1 etcd[1320]: c54eae14ec78233 became follower at term 216
Jan 24 17:56:42 master1 etcd[1320]: newRaft c54eae14ec78233 [peers: [], term: 216, commit: 4898, applied: 0, lastindex: 4899,
Jan 24 17:56:42 master1 etcd[1320]: restore compact to 2804
Jan 24 17:56:42 master1 etcd[1320]: simple token is not cryptographically signed
Jan 24 17:56:42 master1 etcd[1320]: starting server... [version: 3.3.9, cluster version: to_be_decided]
Jan 24 17:56:42 master1 etcd[1320]: added member c54eae14ec78233 [https://172.31.122.25:2380] to cluster 1f9cddabdc85ecd2
Jan 24 17:56:42 master1 etcd[1320]: added member 2356cc8de1dc7a23 [https://172.31.115.2:2380] to cluster 1f9cddabdc85ecd2
Jan 24 17:56:42 master1 etcd[1320]: starting peer 2356cc8de1dc7a23...
Jan 24 17:56:42 master1 etcd[1320]: started HTTP pipelining with peer 2356cc8de1dc7a23
Jan 24 17:56:42 master1 etcd[1320]: ClientTLS: cert = /etc/etcd/etcd-server.crt, key = /etc/etcd/etcd-server.key, ca = , trus
Jan 24 17:56:42 master1 etcd[1320]: started streaming with peer 2356cc8de1dc7a23 (writer)
Jan 24 17:56:42 master1 etcd[1320]: started peer 2356cc8de1dc7a23
Jan 24 17:56:42 master1 etcd[1320]: added peer 2356cc8de1dc7a23
Jan 24 17:56:42 master1 etcd[1320]: started streaming with peer 2356cc8de1dc7a23 (stream Message reader)
Jan 24 17:56:42 master1 etcd[1320]: started streaming with peer 2356cc8de1dc7a23 (stream MsgApp v2 reader)
Jan 24 17:56:42 master1 etcd[1320]: added member 83b0c9b79ada9b8d [https://172.31.125.181:2380] to cluster 1f9cddabdc85ecd2
Jan 24 17:56:42 master1 etcd[1320]: starting peer 83b0c9b79ada9b8d...
Jan 24 17:56:42 master1 etcd[1320]: started HTTP pipelining with peer 83b0c9b79ada9b8d
Jan 24 17:56:42 master1 etcd[1320]: started streaming with peer 2356cc8de1dc7a23 (writer)
Jan 24 17:56:42 master1 etcd[1320]: started streaming with peer 83b0c9b79ada9b8d (writer)
Jan 24 17:56:42 master1 etcd[1320]: started streaming with peer 83b0c9b79ada9b8d (writer)
Jan 24 17:56:42 master1 etcd[1320]: started peer 83b0c9b79ada9b8d
Jan 24 17:56:42 master1 etcd[1320]: added peer 83b0c9b79ada9b8d
lines 1-43Jan 24 17:56:42 master1 etcd[1320]: set the initial cluster version to 3.0
Jan 24 17:56:42 master1 etcd[1320]: enabled capabilities for version 3.0
Jan 24 17:56:42 master1 etcd[1320]: updated the cluster version from 3.0 to 3.3
Jan 24 17:56:42 master1 etcd[1320]: enabled capabilities for version 3.3
Jan 24 17:56:42 master1 etcd[1320]: started streaming with peer 83b0c9b79ada9b8d (stream MsgApp v2 reader)
Jan 24 17:56:42 master1 etcd[1320]: started streaming with peer 83b0c9b79ada9b8d (stream Message reader)
Jan 24 17:56:44 master1 etcd[1320]: c54eae14ec78233 is starting a new election at term 216
Jan 24 17:56:44 master1 etcd[1320]: c54eae14ec78233 became candidate at term 217
Jan 24 17:56:44 master1 etcd[1320]: c54eae14ec78233 received MsgVoteResp from c54eae14ec78233 at term 217
Jan 24 17:56:44 master1 etcd[1320]: c54eae14ec78233 [logterm: 216, index: 4899] sent MsgVote request to 2356cc8de1dc7a23 at t
Jan 24 17:56:44 master1 etcd[1320]: c54eae14ec78233 [logterm: 216, index: 4899] sent MsgVote request to 83b0c9b79ada9b8d at t
Jan 24 17:56:44 master1 etcd[1320]: peer 83b0c9b79ada9b8d became active
Jan 24 17:56:44 master1 etcd[1320]: established a TCP streaming connection with peer 83b0c9b79ada9b8d (stream Message reader)
Jan 24 17:56:44 master1 etcd[1320]: established a TCP streaming connection with peer 83b0c9b79ada9b8d (stream MsgApp v2 reade
Jan 24 17:56:44 master1 etcd[1320]: established a TCP streaming connection with peer 83b0c9b79ada9b8d (stream MsgApp v2 write
Jan 24 17:56:44 master1 etcd[1320]: established a TCP streaming connection with peer 83b0c9b79ada9b8d (stream Message writer)
Jan 24 17:56:44 master1 etcd[1320]: c54eae14ec78233 received MsgVoteResp rejection from 83b0c9b79ada9b8d at term 217
Jan 24 17:56:44 master1 etcd[1320]: c54eae14ec78233 [quorum:2] has received 1 MsgVoteResp votes and 1 vote rejections
Jan 24 17:56:45 master1 etcd[1320]: c54eae14ec78233 is starting a new election at term 217
Jan 24 17:56:45 master1 etcd[1320]: c54eae14ec78233 became candidate at term 218
Jan 24 17:56:45 master1 etcd[1320]: c54eae14ec78233 received MsgVoteResp from c54eae14ec78233 at term 218
Jan 24 17:56:45 master1 etcd[1320]: c54eae14ec78233 [logterm: 216, index: 4899] sent MsgVote request to 2356cc8de1dc7a23 at t
Jan 24 17:56:45 master1 etcd[1320]: c54eae14ec78233 [logterm: 216, index: 4899] sent MsgVote request to 83b0c9b79ada9b8d at t
Jan 24 17:56:45 master1 etcd[1320]: c54eae14ec78233 received MsgVoteResp rejection from 83b0c9b79ada9b8d at term 218
Jan 24 17:56:45 master1 etcd[1320]: c54eae14ec78233 [quorum:2] has received 1 MsgVoteResp votes and 1 vote rejections
Jan 24 17:56:46 master1 etcd[1320]: c54eae14ec78233 [term: 218] received a MsgVote message with higher term from 83b0c9b79ada
Jan 24 17:56:46 master1 etcd[1320]: c54eae14ec78233 became follower at term 219
Jan 24 17:56:46 master1 etcd[1320]: c54eae14ec78233 [logterm: 216, index: 4899, vote: 0] cast MsgVote for 83b0c9b79ada9b8d [l
Jan 24 17:56:46 master1 etcd[1320]: raft.node: c54eae14ec78233 elected leader 83b0c9b79ada9b8d at term 219
Jan 24 17:56:46 master1 etcd[1320]: published {Name:master1 ClientURLs:[https://172.31.122.25:2379]} to cluster 1f9cddabdc85e
Jan 24 17:56:46 master1 etcd[1320]: forgot to set Type=notify in systemd service file?
Jan 24 17:56:46 master1 etcd[1320]: ready to serve client requests
Jan 24 17:56:46 master1 etcd[1320]: serving client requests on 127.0.0.1:2379
Jan 24 17:56:46 master1 etcd[1320]: ready to serve client requests
Jan 24 17:56:46 master1 etcd[1320]: serving client requests on 172.31.122.25:2379
Jan 24 17:56:47 master1 etcd[1320]: rejected connection from "172.31.122.25:60462" (error "EOF", ServerName "")
Jan 24 17:56:47 master1 etcd[1320]: rejected connection from "172.31.122.25:60542" (error "EOF", ServerName "")
Jan 24 17:56:47 master1 etcd[1320]: health check for peer 2356cc8de1dc7a23 could not connect: dial tcp 172.31.115.2:2380: con
Jan 24 17:56:47 master1 etcd[1320]: rejected connection from "172.31.122.25:60580" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: read-only range request "key:\"/registry/certificatesigningrequests\" range_end:\"/regist
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.122.25:60614" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: peer 2356cc8de1dc7a23 became active
Jan 24 17:56:48 master1 etcd[1320]: established a TCP streaming connection with peer 2356cc8de1dc7a23 (stream MsgApp v2 reade
lines 44-86Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.122.25:60618" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.125.181:39232" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.122.25:60632" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.122.25:60674" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.122.25:60644" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.122.25:60638" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: established a TCP streaming connection with peer 2356cc8de1dc7a23 (stream Message reader)
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.122.25:60656" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.122.25:60686" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: established a TCP streaming connection with peer 2356cc8de1dc7a23 (stream MsgApp v2 write
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.125.181:39256" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.122.25:60650" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.122.25:60668" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.125.181:39262" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.122.25:60678" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.125.181:39236" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.122.25:60662" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: established a TCP streaming connection with peer 2356cc8de1dc7a23 (stream Message writer)
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.125.181:39270" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.125.181:39280" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.125.181:39286" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.125.181:39290" (error "EOF", ServerName "")
Jan 24 17:56:48 master1 etcd[1320]: rejected connection from "172.31.125.181:39274" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60704" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39308" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60716" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39304" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60710" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60692" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60698" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39248" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60722" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39242" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39332" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39342" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60728" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60750" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60742" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39338" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60782" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39316" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60766" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60772" (error "EOF", ServerName "")
lines 87-129Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39348" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60774" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60758" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60800" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60788" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39324" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60792" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60812" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60818" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39352" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39370" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39382" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39366" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39390" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60806" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39402" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39394" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39406" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39414" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60836" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39424" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39418" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39444" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39456" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60842" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39466" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39500" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39480" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39486" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39438" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39450" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39494" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39464" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39516" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39474" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39534" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39508" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.122.25:60850" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39432" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39504" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39520" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39526" (error "EOF", ServerName "")
Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39540" (error "EOF", ServerName "")
lines 130-172Jan 24 17:56:49 master1 etcd[1320]: rejected connection from "172.31.125.181:39548" (error "EOF", ServerName "")
Jan 24 17:56:50 master1 etcd[1320]: rejected connection from "172.31.125.181:39582" (error "read tcp 172.31.122.25:2379->172.
Jan 24 17:56:50 master1 etcd[1320]: rejected connection from "172.31.125.181:39594" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56510" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56562" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56620" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56646" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56652" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56660" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56668" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56678" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56682" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56688" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56704" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56694" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56700" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56718" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56710" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56730" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56724" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56754" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56736" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56740" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56746" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56758" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56766" (error "read tcp 172.31.122.25:2379->172.31
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56772" (error "read tcp 172.31.122.25:2379->172.31
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56778" (error "EOF", ServerName "")
Jan 24 17:56:52 master1 etcd[1320]: rejected connection from "172.31.115.2:56802" (error "EOF", ServerName "")
Jan 24 17:56:53 master1 etcd[1320]: rejected connection from "172.31.115.2:56842" (error "read tcp 172.31.122.25:2379->172.31
Jan 24 17:56:53 master1 etcd[1320]: rejected connection from "172.31.115.2:56850" (error "EOF", ServerName "")
Jan 24 17:56:54 master1 etcd[1320]: rejected connection from "172.31.125.181:39630" (error "read tcp 172.31.122.25:2379->172.
Jan 24 17:56:54 master1 etcd[1320]: rejected connection from "172.31.125.181:39638" (error "EOF", ServerName "")
lines 163-205/205 (END)lines 163-205/205 (END)lines 163-205/205 (END)root@master1:~# cat /etc/hossts
127.0.0.1 localhost

# The following lines are desirable for IPv6 capable hosts
::1 ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
ff02::3 ip6-allhosts
# Cloud Server Hostname mapping
172.31.122.25   rameshhms1c.mylabserver.com
172.31.122.25master1
172.31.125.181master2
172.31.115.2master3
172.31.122.147worker1
172.31.123.142loadbalancer
root@master1:~# ETCDCTL_API=3 etcdctl -w table endpoint --cluster status --endpoints=https://127.0.0.1:2379  --cacert=/etc/et cd/ca.crt  --cert=/etc/etcd/etcd-server.crt --key=/etc/etcd/etcd-server.key
+-----------------------------+------------------+---------+---------+-----------+-----------+------------+
|          ENDPOINT           |        ID        | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |
+-----------------------------+------------------+---------+---------+-----------+-----------+------------+
|  https://172.31.122.25:2379 |  c54eae14ec78233 |   3.3.9 |  1.5 MB |     false |       219 |       5819 |
|   https://172.31.115.2:2379 | 2356cc8de1dc7a23 |   3.3.9 |  1.5 MB |     false |       219 |       5819 |
| https://172.31.125.181:2379 | 83b0c9b79ada9b8d |   3.3.9 |  1.5 MB |      true |       219 |       5819 |
+-----------------------------+------------------+---------+---------+-----------+-----------+------------+
root@master1:~# 
root@master1:~# ETCDCTL_API=3 etcdctl -w table endpoint --cluster health  --endpoints=https://127.0.0.1:2379  --cacert=/etc/e tcd/ca.crt  --cert=/etc/etcd/etcd-server.crt --key=/etc/etcd/etcd-server.key
https://172.31.125.181:2379 is healthy: successfully committed proposal: took = 1.580458ms
https://172.31.122.25:2379 is healthy: successfully committed proposal: took = 9.961869ms
https://172.31.115.2:2379 is healthy: successfully committed proposal: took = 8.396702ms
root@master1:~# 
root@master1:~# cat /curl -k  https://172.31.16.151:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt  --cacert /root/keys/ca.crt
^C
root@master1:~# curl -k  https://172.31.16.151:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt ---cacert /root/keys/ca.crt:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --root@master1:~# :6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --croot@master1:~# :6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --caroot@master1:~# :6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --cacroot@master1:~# :6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --caceroot@master1:~# :6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --cacerroot@master1:~# :6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --cacert /root/keys/ca.crt:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --cacert /root/keys/ca.crt:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --cacert /root/keys/ca.crt:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --cacert /root/keys/ca.crt:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --cacert /rot/keys/ca.crtroot@master1:~# :6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --cacert /root/keys/ca.crtroot@master1:~# :6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --cacert /root/keys/ca.crtroot@master1:~# 1:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --cacert /root/keys/ca.crtroot@master1:~# 7:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --cacert /root/keys/ca.crtroot@master1:~# 2:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --cacert /rot/keys/ca.crt.:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --cacert /root/keys/ca.crt3:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --cacert /root/keys/ca.crt1:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --cacert /root/keys/ca.crt.:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --cacert /root/keys/ca.crt1:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --cacerroot@master1:~# 2:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --caceroot@master1:~# 3:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --cacroot@master1:~# .:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --caroot@master1:~# 1:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --croot@master1:~# 4:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt --root@master1:~# 2:6443/api/v1/namespaces --key /root/keys/admin.key --cert /root/keys/admin.crt -root@master1:~# 

{
  "kind": "NamespaceList",
  "apiVersion": "v1",
  "metadata": {
    "selfLink": "/api/v1/namespaces",
    "resourceVersion": "4480"
  },
  "items": [
    {
      "metadata": {
        "name": "default",
        "selfLink": "/api/v1/namespaces/default",
        "uid": "4caba4be-3ecd-11ea-9d57-020f08380e46",
        "resourceVersion": "9",
        "creationTimestamp": "2020-01-24T17:16:46Z"
      },
      "spec": {
        "finalizers": [
          "kubernetes"
        ]
      },
      "status": {
        "phase": "Active"
      }
    },
    {
      "metadata": {
        "name": "kube-public",
        "selfLink": "/api/v1/namespaces/kube-public",
        "uid": "4cc3213b-3ecd-11ea-9d57-020f08380e46",
        "resourceVersion": "37",
        "creationTimestamp": "2020-01-24T17:16:46Z"
      },
      "spec": {
        "finalizers": [
          "kubernetes"
        ]
      },
      "status": {
        "phase": "Active"
      }
    },
    {
      "metadata": {
        "name": "kube-system",
        "selfLink": "/api/v1/namespaces/kube-system",
        "uid": "4cc16a50-3ecd-11ea-9d57-020f08380e46",
        "resourceVersion": "35",
        "creationTimestamp": "2020-01-24T17:16:46Z"
      },
      "spec": {
        "finalizers": [
          "kubernetes"
        ]
      },
      "status": {
        "phase": "Active"
      }
    }
  ]
}root@master1:~# curl -k  https://172.31.16.151:6443/healthz:6443/healthz:6443/healthz:6443/healthz:6443/healthz:6443/healthz:6443/healthz:6443/healthz:6443/healthz:6443/healthz:6443/healthz:6443/healthz:6443/healthz:6443/healthz1:6443/healthz7:6443/healthz2:6443/healthz.:6443/healthz3:6443/healthz1:6443/healthz.:6443/healthz1:6443/healthz2:6443/healthz3:6443/healthz.:6443/healthz1:6443/healthz4:6443/healthz2:6443/healthz
okroot@master1:~# 
root@master1:~# 
root@master1:~# kubectl gety nodesd
error: the server doesn't have a resource type "nodesd"
root@master1:~# kubectl get nodesd
NAME      STATUS     ROLES    AGE   VERSION
master3   Ready      <none>   22m   v1.13.0
worker1   NotReady   <none>   41m   v1.13.0
root@master1:~# 
root@master1:~# cd ssh mworker1
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 17:46:08 2020 from 172.31.122.25
root@worker1:~# kl;sls -l /avar/lib/kubeer
kubelet/    kube-proxy/ kubernetes/ 
root@worker1:~# ls -l /var/lib/kube
kubelet/    kube-proxy/ kubernetes/ 
root@worker1:~# ls -l /var/lib/kubelet/
total 48
-rw------- 1 root root   62 Jan 24 17:26 cpu_manager_state
drwxr-xr-x 2 root root 4096 Jan 24 17:56 device-plugins
-rw------- 1 root root 5460 Jan 24 17:21 kubeconfig
-rw-r--r-- 1 root root  368 Jan 24 17:22 kubelet-config.yaml
drwx------ 2 root root 4096 Jan 24 17:26 plugin-containers
drwxr-x--- 2 root root 4096 Jan 24 17:26 plugins
drwxr-x--- 2 root root 4096 Jan 24 17:26 plugins_registry
drwxr-x--- 2 root root 4096 Jan 24 17:26 pod-resources
drwxr-x--- 3 root root 4096 Jan 24 17:33 pods
-rw-r--r-- 1 root root 1147 Jan 24 17:21 worker1.crt
-rw-r--r-- 1 root root 1679 Jan 24 17:21 worker1.key
root@worker1:~# cat /var/lib/kubelet/kubeconfig
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNyRENDQVpRQ0NRRDhaRE9DSUs1UXNEQU5CZ2txaGtpRzl3MEJBUXNGQURBWU1SWXdGQVlEVlFRRERBMUwKVlVKRlVrNUZWRVZUTFVOQk1CNFhEVEl3TURFeU5ERXlOVGMwTTFvWERUSXlNVEF5TURFeU5UYzBNMW93R0RFVwpNQlFHQTFVRUF3d05TMVZDUlZKT1JWUkZVeTFEUVRDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDCkFRb0NnZ0VCQU41VkU4ZG1RZllUMzEvUU9qOVJuLy84Q2tWaXp6VWQ2MkhWUmt0QmxlUWt3MGhsUG9jeHlhT0wKbGlpRHhrbjZTZ3c5ek44Y21ScUljbEt3a3lEcmtTR0pvWHo4S0xyekdvM2xUTUZidC9FRUZtc280cGxwbFdMTQpXNVByeXlodmkxUWVHNE14SVI3REtlajI0M0JsaThhWDBtc3hZVkxDYWNqTmhRM1hEUFJnSHJyaUkwczVkbDQwCmdtRmRPTGM4TkJoTFZuMDNlVlpJdk1wOUpRcGR3UWZuNjA1akdWWXpSalB2bDJ1N2tDeldMdjQwUmNFRExOYlQKdHlJSlI2VXFZeWNpQTVkZXVRYTQvcER4ZlBYVHdibDQ4eXJ3SVM1YWpIbDYxNHJVOTd3VUNhdUk3R3VmQ0lWdwovcnVtcUkyTjBGS2F5NjhjbmhWT3U5UmdQZG4zZzhzQ0F3RUFBVEFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBClltY1phQXRRZnF2YUN1N1pGVlVzY2pJZXRRUVQ1Nmw4VTUvSWNnUnV4MGhaTkF3bFBqbmZlVkVjMXN5ckVpY2oKL2MrTVJkLzB1WENhMjhtQTlTcStITVp1UmtYaEVXNVB3YWRGNGpOVVBlQkc2Y3ppS3ZzUTExT3RYTVhZWldnTgpXNDhISGVaU2MvMW9lZFJkMEJKYkZZT2ZCd1lpcXNNNVQ1K01RVUlxaHBBVmF1T2ozSUtVbGFQQzljK3RJR0U2CkV3K2xPckZnRTFhRndyTVJ3VXUyUlZzbXVya3MwUTloaXI5VTgwWHp0NmpscE9oOWNIZzFKbmE5UnNON3hTVTEKU05oaE14Mm1rNkNjUGtzNjA0S1pKakhZbUtqVDNEM2FSeXJSSUpNSGdmOFdtdHNDSXQzNGZQV05MQ2ZMV3YrMwpVaktTa3h4TVArVXAvR2NhOWtSRjNRPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    server: https://172.31.123.142:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: system:node:worker1
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: system:node:worker1
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lKQUpHSFF3TGZsb05iTUEwR0NTcUdTSWIzRFFFQkN3VUFNQmd4RmpBVUJnTlYKQkFNTURVdFZRa1ZTVGtWVVJWTXRRMEV3SGhjTk1qQXdNVEkwTVRNd05USTBXaGNOTWpJeE1ESXdNVE13TlRJMApXakExTVJ3d0dnWURWUVFEREJOemVYTjBaVzA2Ym05a1pUcDNiM0pyWlhJeE1SVXdFd1lEVlFRS0RBeHplWE4wClpXMDZibTlrWlhNd2dnRWlNQTBHQ1NxR1NJYjNEUUVCQVFVQUE0SUJEd0F3Z2dFS0FvSUJBUURzTlg0WDhweTkKUHYwU1JXYUdJVTltV3lsNDZmSExsWVBlS0poejh4QXN0QlBQRGN6TEhneWtpQkN5VStpVkVTbER3c0owdFRnTAplSytsdk1YSncrRzV3NFdLYXBSbHdPdjdWcGFST0pjbldCSnNVM1o3TEhnaXZMUUFRaVprOXZjK1czY2w4UkNuCjlPVWQrNHpBVHRCTmpkalJrOW1uR1VDS05Tb3pOME1QZXhWZWUrK0VzT3grL0ZXTnYvc09Kd1JrWFpQMUhrMWcKTE1YQ2NiRUVrWVFsWHgxcFRGZUo1VWZxR1VJNFlEUUZxblg3WDliM1JETzR5ZVl3K1VhUENGQVZvekdSNExEcAorWTI2blIvdE5wazR0NFdodXpQZlpjVlNLQ3p3RTltRHFIUVExcSs4NjNwT1BIRUlmcjRQQnBZM2pEUHdQeFRiCmZIL29wRTNoL01GbEFnTUJBQUdqVVRCUE1Ba0dBMVVkRXdRQ01BQXdDd1lEVlIwUEJBUURBZ1hnTURVR0ExVWQKRVFRdU1DeUNCM2R2Y210bGNqR0NHM0poYldWemFHaHRjelJqTG0xNWJHRmljMlZ5ZG1WeUxtTnZiWWNFckI5NgprekFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBSnhTVVZZRkJIZ0dhbHdianB5cG1ndGMrbU1rMFpjb2tBZmN2ClB1N3paR2FuMWd4SGRxb2VIWTlXZy95dE41MVdteVNQQS9lUkxyVGZ2aDh2dW95dG1mdjVZZjFUMDFrR1I1UDEKeFV3bVVxOTFZdVNlTm51WW9JWDJ4YW1mb2dndEhoNFdDQ0RhR3JQZDlpOVVCK0dUTkZpVlF1T215c2VBbS9ISgp5SWt3dTNWK0VXdFNQOHpTY3B2T2twNSs0WjQ2SXZFaWlWaGVWUHBTaStOZjlVVkdJa0VCd0srNzFSSHFhU0Y4CjZsUldUQ0R0UHR5dHpLSmpwTnZGN0lzbXVZZndKOC93ZVZ3cEpJQ2MweUpDdnRySHIzaVRPTTJleUp1aHdsbmsKbldEM1BwSW1FZmZlK0FqYzlicUhZOW1EY1QwT0FQY2JMYUJ0ckxIbWI3cDljd0E4anc9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    client-key-data: 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
root@worker1:~# cat /var/lib/kubelet/kubeconfigkubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.crt"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.96.0.10"
resolvConf: "/run/systemd/resolve/resolv.conf"
runtimeRequestTimeout: "15m"
root@worker1:~# 
root@worker1:~# 
root@worker1:~# ls -l /var/lib/kubelet/-proxy/
total 12
-rw------- 1 root root 5248 Jan 24 17:23 kubeconfig
-rw-r--r-- 1 root root  185 Jan 24 17:23 kube-proxy-config.yaml
root@worker1:~# ls -l /var/lib/kube-proxy/cat /var/lib/kube-proxy/kubeconfig
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNyRENDQVpRQ0NRRDhaRE9DSUs1UXNEQU5CZ2txaGtpRzl3MEJBUXNGQURBWU1SWXdGQVlEVlFRRERBMUwKVlVKRlVrNUZWRVZUTFVOQk1CNFhEVEl3TURFeU5ERXlOVGMwTTFvWERUSXlNVEF5TURFeU5UYzBNMW93R0RFVwpNQlFHQTFVRUF3d05TMVZDUlZKT1JWUkZVeTFEUVRDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDCkFRb0NnZ0VCQU41VkU4ZG1RZllUMzEvUU9qOVJuLy84Q2tWaXp6VWQ2MkhWUmt0QmxlUWt3MGhsUG9jeHlhT0wKbGlpRHhrbjZTZ3c5ek44Y21ScUljbEt3a3lEcmtTR0pvWHo4S0xyekdvM2xUTUZidC9FRUZtc280cGxwbFdMTQpXNVByeXlodmkxUWVHNE14SVI3REtlajI0M0JsaThhWDBtc3hZVkxDYWNqTmhRM1hEUFJnSHJyaUkwczVkbDQwCmdtRmRPTGM4TkJoTFZuMDNlVlpJdk1wOUpRcGR3UWZuNjA1akdWWXpSalB2bDJ1N2tDeldMdjQwUmNFRExOYlQKdHlJSlI2VXFZeWNpQTVkZXVRYTQvcER4ZlBYVHdibDQ4eXJ3SVM1YWpIbDYxNHJVOTd3VUNhdUk3R3VmQ0lWdwovcnVtcUkyTjBGS2F5NjhjbmhWT3U5UmdQZG4zZzhzQ0F3RUFBVEFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBClltY1phQXRRZnF2YUN1N1pGVlVzY2pJZXRRUVQ1Nmw4VTUvSWNnUnV4MGhaTkF3bFBqbmZlVkVjMXN5ckVpY2oKL2MrTVJkLzB1WENhMjhtQTlTcStITVp1UmtYaEVXNVB3YWRGNGpOVVBlQkc2Y3ppS3ZzUTExT3RYTVhZWldnTgpXNDhISGVaU2MvMW9lZFJkMEJKYkZZT2ZCd1lpcXNNNVQ1K01RVUlxaHBBVmF1T2ozSUtVbGFQQzljK3RJR0U2CkV3K2xPckZnRTFhRndyTVJ3VXUyUlZzbXVya3MwUTloaXI5VTgwWHp0NmpscE9oOWNIZzFKbmE5UnNON3hTVTEKU05oaE14Mm1rNkNjUGtzNjA0S1pKakhZbUtqVDNEM2FSeXJSSUpNSGdmOFdtdHNDSXQzNGZQV05MQ2ZMV3YrMwpVaktTa3h4TVArVXAvR2NhOWtSRjNRPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    server: https://172.31.123.142:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: system:kube-proxy
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: system:kube-proxy
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNzRENDQVpnQ0NRQ1JoME1DMzVhRFZqQU5CZ2txaGtpRzl3MEJBUXNGQURBWU1SWXdGQVlEVlFRRERBMUwKVlVKRlVrNUZWRVZUTFVOQk1CNFhEVEl3TURFeU5ERXlOVGd6TTFvWERUSXlNVEF5TURFeU5UZ3pNMW93SERFYQpNQmdHQTFVRUF3d1JjM2x6ZEdWdE9tdDFZbVV0Y0hKdmVIa3dnZ0VpTUEwR0NTcUdTSWIzRFFFQkFRVUFBNElCCkR3QXdnZ0VLQW9JQkFRRHcwZldiaFdQdkJqay9xOWRUVUV5TWU1UktrYUZXck9zN2kvUXY3eGs2QjhGVnk4SFkKRS9UME9JbHMrT0ptZzFlVWkyMEc1UWxWOGNiTEFTT2pKZGIwcDIxbDhBcWd0WDNGL2d4cVBxUzlTV0lnM2FFNQpJOTJldjNDbWp1VjN6OFkwVFFpaVZ0bWJjcTFRVC9ZYjJrVUtPcVpHb0lscmxZK1NDbXdINEgvakJ1Nk9JbWRkClhkYUh3QkZwdmplaVlPZkJ4NWZqVC8rU3E3ZUdvRVZVcGwyUEIycjlFL0piVHQ4UGdQdnd1S1JKZUlES3ZZRXcKYjRQcER1T2lKcXlPS2xVNTRyN09pbXZFWVlSYW82ZFJvM2syY3I5cHl5akMzRXl0ZWVwdlI1a0ZmamoyNHZ3YgpPUk5aN2QxdWJqNGw2QndOTUZ4ZnRRd0FEYmlBcGZ2MHlwcUhBZ01CQUFFd0RRWUpLb1pJaHZjTkFRRUxCUUFECmdnRUJBREFjbG1tNnZhY00wM25sZWVMV24xUHM2ZW1TZ0pHb0xPUDI4bXpJUnZsL0NmRTNIVkR5QkhYK1hLMXAKczVDMzdnVlAxdmIrMjhZNjZ0VWtva3NMeE4yMjZHWVBpYWdha0t0b0puZ3JkYWFOcVAvYnJrQ1BNbmdRcWkySgpKSGhvK2lIWVZCQTlrYmswT3diZUVrNHpkbDZrVlVackthcEpTazVScXlQWVBXV2Nkb2R1cGFCMjlwTUJUZTgrCkpnSThMWVA0TFRjUGRiQ1k0V1F0M2FJUksveHhCYlJ1UmpDL21yRnhOOGJZQ215V2pvWkNnYjRpZm1IbFovY0kKUjVVa0hRcm5JS3BDTEZDa3hCZXVvcDRmOGZBM29rTHk1bEUyQTNCZDBJVjZKR09wN2tTRHV6bWlGcGtoZFUrQgpXRXU5TEFYcStuL1Z2d0QvMEYxUURUdVZkakU9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    client-key-data: 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
root@worker1:~# cat /var/lib/kube-proxy/kubeconfigkube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
mode: "iptables"
clusterCIDR: "172.31.0.0/20"
root@worker1:~# 
root@worker1:~# 
root@worker1:~# kubesystemctl status kiuubelet.service 
 kubelet.service - Kubernetes Kubelet
   Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2020-01-24 17:56:51 UTC; 12min ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 2103 (kubelet)
    Tasks: 15
   Memory: 90.8M
      CPU: 15.694s
   CGroup: /system.slice/kubelet.service
           2103 /usr/local/bin/kubelet --config=/var/lib/kubelet/kubelet-config.yaml --image-pull-progress-deadline=2m --ku

Jan 24 18:09:33 worker1 kubelet[2103]: W0124 18:09:33.121936    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:09:33 worker1 kubelet[2103]: E0124 18:09:33.122323    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:09:38 worker1 kubelet[2103]: W0124 18:09:38.139692    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:09:38 worker1 kubelet[2103]: E0124 18:09:38.141423    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:09:43 worker1 kubelet[2103]: W0124 18:09:43.159125    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:09:43 worker1 kubelet[2103]: E0124 18:09:43.159405    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:09:43 worker1 kubelet[2103]: I0124 18:09:43.904080    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:09:43 worker1 kubelet[2103]: E0124 18:09:43.904152    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:09:43 worker1 kubelet[2103]: E0124 18:09:43.904166    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:09:43 worker1 kubelet[2103]: E0124 18:09:43.904205    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
lines 1-21/21 (END)lines 1-21/21 (END)root@worker1:~# systemctl status kubelet.service kube-proxy.service 
 kube-proxy.service - Kubernetes Kube Proxy
   Loaded: loaded (/etc/systemd/system/kube-proxy.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2020-01-24 17:56:46 UTC; 13min ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 1373 (kube-proxy)
    Tasks: 0
   Memory: 30.4M
      CPU: 147ms
   CGroup: /system.slice/kube-proxy.service
            1373 /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/kube-proxy-config.yaml

Jan 24 17:56:49 worker1 kube-proxy[1373]: I0124 17:56:49.383424    1373 config.go:102] Starting endpoints config controller
Jan 24 17:56:49 worker1 kube-proxy[1373]: I0124 17:56:49.383439    1373 controller_utils.go:1027] Waiting for caches to sync 
Jan 24 17:56:49 worker1 kube-proxy[1373]: I0124 17:56:49.383467    1373 config.go:202] Starting service config controller
Jan 24 17:56:49 worker1 kube-proxy[1373]: I0124 17:56:49.383472    1373 controller_utils.go:1027] Waiting for caches to sync 
Jan 24 17:56:49 worker1 kube-proxy[1373]: E0124 17:56:49.401188    1373 reflector.go:134] k8s.io/client-go/informers/factory.
Jan 24 17:56:49 worker1 kube-proxy[1373]: E0124 17:56:49.401933    1373 reflector.go:134] k8s.io/client-go/informers/factory.
Jan 24 17:56:49 worker1 kube-proxy[1373]: E0124 17:56:49.404797    1373 event.go:212] Unable to write event: 'Post https://17
Jan 24 17:56:58 worker1 kube-proxy[1373]: E0124 17:56:58.042380    1373 reflector.go:134] k8s.io/client-go/informers/factory.
Jan 24 17:56:59 worker1 kube-proxy[1373]: I0124 17:56:59.083678    1373 controller_utils.go:1034] Caches are synced for endpo
Jan 24 17:56:59 worker1 kube-proxy[1373]: I0124 17:56:59.083678    1373 controller_utils.go:1034] Caches are synced for servi
lines 1-21/21 (END)lines 1-21/21 (END)root@worker1:~# s -ls -l /etc/c
ca-certificates/               cloud/                         cracklib/                      crontab
ca-certificates.conf           cni/                           cron.d/                        cron.weekly/
ca-certificates.conf.dpkg-old  compizconfig/                  cron.daily/                    crypttab
calendar/                      console-setup/                 cron.hourly/                   cups/
chatscripts/                   containerd/                    cron.monthly/                  cupshelpers/
root@worker1:~# ls -l /etc/c
ca-certificates/               cloud/                         cracklib/                      crontab
ca-certificates.conf           cni/                           cron.d/                        cron.weekly/
ca-certificates.conf.dpkg-old  compizconfig/                  cron.daily/                    crypttab
calendar/                      console-setup/                 cron.hourly/                   cups/
chatscripts/                   containerd/                    cron.monthly/                  cupshelpers/
root@worker1:~# ls -l /etc/ccni/net.d/
total 0
root@worker1:~# ls -l /opt/cni/bin/
total 49008
-rwxr-xr-x 1 root root  4028260 Mar 15  2019 bridge
-rwxr-xr-x 1 root root 10232415 Mar 15  2019 dhcp
-rwxr-xr-x 1 root root  2856252 Mar 15  2019 flannel
-rwxr-xr-x 1 root root  3127363 Mar 15  2019 host-device
-rwxr-xr-x 1 root root  3036768 Mar 15  2019 host-local
-rwxr-xr-x 1 root root  3572685 Mar 15  2019 ipvlan
-rwxr-xr-x 1 root root  3084347 Mar 15  2019 loopback
-rwxr-xr-x 1 root root  3613497 Mar 15  2019 macvlan
-rwxr-xr-x 1 root root  3551125 Mar 15  2019 portmap
-rwxr-xr-x 1 root root  3993428 Mar 15  2019 ptp
-rwxr-xr-x 1 root root  2641877 Mar 15  2019 sample
-rwxr-xr-x 1 root root  2850029 Mar 15  2019 tuning
-rwxr-xr-x 1 root root  3568537 Mar 15  2019 vlan
root@worker1:~# ls -l /etc/cni/net.d/
total 0
root@worker1:~# 
root@worker1:~# 
root@worker1:~# logout
Connection to worker1 closed.
root@master1:~# ls -l /etc/cni/net.d/
ls: cannot access '/etc/cni/net.d/': No such file or directory
root@master1:~# 
root@master1:~# 
root@master1:~# ssh master3
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 17:39:07 2020 from 172.31.122.25
root@master3:~# ls -l /etc/cni/net.d/
total 4
-rw-r--r-- 1 root root 318 Jan 24 17:45 10-weave.conflist
root@master3:~# cat /etc/cni/net.d/10-weave.conflist
{
    "cniVersion": "0.3.0",
    "name": "weave",
    "plugins": [
        {
            "name": "weave",
            "type": "weave-net",
            "hairpinMode": true
        },
        {
            "type": "portmap",
            "capabilities": {"portMappings": true},
            "snat": true
        }
    ]
}
root@master3:~# logout
Connection to master3 closed.
root@master1:~# 
root@master1:~# 
root@master1:~# ls -l /opt/cni/bin/
ls: cannot access '/opt/cni/bin/': No such file or directory
root@master1:~# 
root@master1:~# 
root@master1:~# ssh worker1
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 18:08:03 2020 from 172.31.122.25
root@worker1:~# ls -l /opt/cni/bin/
total 49008
-rwxr-xr-x 1 root root  4028260 Mar 15  2019 bridge
-rwxr-xr-x 1 root root 10232415 Mar 15  2019 dhcp
-rwxr-xr-x 1 root root  2856252 Mar 15  2019 flannel
-rwxr-xr-x 1 root root  3127363 Mar 15  2019 host-device
-rwxr-xr-x 1 root root  3036768 Mar 15  2019 host-local
-rwxr-xr-x 1 root root  3572685 Mar 15  2019 ipvlan
-rwxr-xr-x 1 root root  3084347 Mar 15  2019 loopback
-rwxr-xr-x 1 root root  3613497 Mar 15  2019 macvlan
-rwxr-xr-x 1 root root  3551125 Mar 15  2019 portmap
-rwxr-xr-x 1 root root  3993428 Mar 15  2019 ptp
-rwxr-xr-x 1 root root  2641877 Mar 15  2019 sample
-rwxr-xr-x 1 root root  2850029 Mar 15  2019 tuning
-rwxr-xr-x 1 root root  3568537 Mar 15  2019 vlan
root@worker1:~# logout
Connection to worker1 closed.
root@master1:~# kubectl get pods -n kube-system
NAME              READY   STATUS              RESTARTS   AGE
weave-net-v2d29   0/2     ContainerCreating   0          40m
weave-net-xfnfn   2/2     Running             3          29m
root@master1:~# kubectl get pods -n kube-system -o wide
NAME              READY   STATUS              RESTARTS   AGE   IP               NODE      NOMINATED NODE   READINESS GATES
weave-net-v2d29   0/2     ContainerCreating   0          40m   172.31.122.147   worker1   <none>           <none>
weave-net-xfnfn   2/2     Running             3          29m   172.31.115.2     master3   <none>           <none>
root@master1:~# 
root@master1:~# 
root@master1:~# 
root@master1:~# 
root@master1:~# cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
> apiVersion: rbac.authorization.k8s.io/v1beta1
> kind: ClusterRole
> metadata:
>   annotations:
>     rbac.authorization.kubernetes.io/autoupdate: "true"
>   labels:
>     kubernetes.io/bootstrapping: rbac-defaults
>   name: system:kube-apiserver-to-kubelet
> rules:
>   - apiGroups:
>       - ""
>     resources:
>       - nodes/proxy
>       - nodes/stats
>       - nodes/log
>       - nodes/spec
>       - nodes/metrics
>     verbs:
>       - "*"
> EOF^C
root@master1:~# cd package/
root@master1:~/package# cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
> apiVersion: rbac.authorization.k8s.io/v1beta1
> kind: ClusterRole
> metadata:
>   annotations:
>     rbac.authorization.kubernetes.io/autoupdate: "true"
>   labels:
>     kubernetes.io/bootstrapping: rbac-defaults
>   name: system:kube-apiserver-to-kubelet
> rules:
>   - apiGroups:
>       - ""
>     resources:
>       - nodes/proxy
>       - nodes/stats
>       - nodes/log
>       - nodes/spec
>       - nodes/metrics
>     verbs:
>       - "*"
> EOF
error: stat admin.kubeconfig: no such file or directory
root@master1:~/package# cd ../keys/
root@master1:~/keys# cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
> apiVersion: rbac.authorization.k8s.io/v1beta1
> kind: ClusterRole
> metadata:
>   annotations:
>     rbac.authorization.kubernetes.io/autoupdate: "true"
>   labels:
>     kubernetes.io/bootstrapping: rbac-defaults
>   name: system:kube-apiserver-to-kubelet
> rules:
>   - apiGroups:
>       - ""
>     resources:
>       - nodes/proxy
>       - nodes/stats
>       - nodes/log
>       - nodes/spec
>       - nodes/metrics
>     verbs:
>       - "*"
> EOF
clusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created
root@master1:~/keys# cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
> apiVersion: rbac.authorization.k8s.io/v1beta1
> kind: ClusterRoleBinding
> metadata:
>   name: system:kube-apiserver
>   namespace: ""
> roleRef:
>   apiGroup: rbac.authorization.k8s.io
>   kind: ClusterRole
>   name: system:kube-apiserver-to-kubelet
> subjects:
>   - apiGroup: rbac.authorization.k8s.io
>     kind: User
>     name: kube-apiserver
> EOF
clusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created
root@master1:~/keys# 
root@master1:~/keys# kubectl get sa
NAME      SECRETS   AGE
default   1         61m
root@master1:~/keys# kubectl get sa -n kube-system
NAME                                 SECRETS   AGE
attachdetach-controller              1         62m
certificate-controller               1         62m
clusterrole-aggregation-controller   1         62m
cronjob-controller                   1         62m
daemon-set-controller                1         62m
default                              1         62m
deployment-controller                1         62m
disruption-controller                1         62m
endpoint-controller                  1         62m
expand-controller                    1         62m
generic-garbage-collector            1         62m
horizontal-pod-autoscaler            1         62m
job-controller                       1         62m
namespace-controller                 1         62m
node-controller                      1         62m
persistent-volume-binder             1         62m
pod-garbage-collector                1         62m
pv-protection-controller             1         62m
pvc-protection-controller            1         62m
replicaset-controller                1         62m
replication-controller               1         62m
resourcequota-controller             1         62m
service-account-controller           1         62m
service-controller                   1         62m
statefulset-controller               1         62m
ttl-controller                       1         62m
weave-net                            1         45m
root@master1:~/keys# kubectl get sa -n kube-systemclusterrole
NAME                                                                   AGE
admin                                                                  62m
cluster-admin                                                          62m
edit                                                                   62m
system:aggregate-to-admin                                              62m
system:aggregate-to-edit                                               62m
system:aggregate-to-view                                               62m
system:auth-delegator                                                  62m
system:aws-cloud-provider                                              62m
system:basic-user                                                      62m
system:certificates.k8s.io:certificatesigningrequests:nodeclient       62m
system:certificates.k8s.io:certificatesigningrequests:selfnodeclient   62m
system:controller:attachdetach-controller                              62m
system:controller:certificate-controller                               62m
system:controller:clusterrole-aggregation-controller                   62m
system:controller:cronjob-controller                                   62m
system:controller:daemon-set-controller                                62m
system:controller:deployment-controller                                62m
system:controller:disruption-controller                                62m
system:controller:endpoint-controller                                  62m
system:controller:expand-controller                                    62m
system:controller:generic-garbage-collector                            62m
system:controller:horizontal-pod-autoscaler                            62m
system:controller:job-controller                                       62m
system:controller:namespace-controller                                 62m
system:controller:node-controller                                      62m
system:controller:persistent-volume-binder                             62m
system:controller:pod-garbage-collector                                62m
system:controller:pv-protection-controller                             62m
system:controller:pvc-protection-controller                            62m
system:controller:replicaset-controller                                62m
system:controller:replication-controller                               62m
system:controller:resourcequota-controller                             62m
system:controller:route-controller                                     62m
system:controller:service-account-controller                           62m
system:controller:service-controller                                   62m
system:controller:statefulset-controller                               62m
system:controller:ttl-controller                                       62m
system:csi-external-attacher                                           62m
system:csi-external-provisioner                                        62m
system:discovery                                                       62m
system:heapster                                                        62m
system:kube-aggregator                                                 62m
system:kube-apiserver-to-kubelet                                       57s
system:kube-controller-manager                                         62m
system:kube-dns                                                        62m
system:kube-scheduler                                                  62m
system:kubelet-api-admin                                               62m
system:node                                                            62m
system:node-bootstrapper                                               62m
system:node-problem-detector                                           62m
system:node-proxier                                                    62m
system:persistent-volume-provisioner                                   62m
system:volume-scheduler                                                62m
view                                                                   62m
weave-net                                                              45m
root@master1:~/keys# kubectl get clusterrole -n kube-systemsaEOF    name: kube-apiserverkind: Username: kube-apiserverEOFkubectl get sa -n kube-systemclusterrolekubetlctl get n\o nodes
NAME      STATUS     ROLES    AGE   VERSION
master3   Ready      <none>   34m   v1.13.0
worker1   NotReady   <none>   53m   v1.13.0
root@master1:~/keys# 
root@master1:~/keys# (reverse-i-search)`':p':     name: kube-apiservero': kubectl get pods -n kube-system -o widedroot@master1:~/keys#root@master1:~/keys# 
NAME              READY   STATUS              RESTARTS   AGE   IP               NODE      NOMINATED NODE   READINESS GATES
weave-net-v2d29   0/2     ContainerCreating   0          46m   172.31.122.147   worker1   <none>           <none>
weave-net-xfnfn   2/2     Running             3          34m   172.31.115.2     master3   <none>           <none>
root@master1:~/keys# 
root@master1:~/keys# ssh worker1
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 18:14:11 2020 from 172.31.122.25
root@worker1:~# kubectl systejournalctl -xu kubelet.service -l
-- Logs begin at Fri 2020-01-24 17:56:37 UTC, end at Fri 2020-01-24 18:20:49 UTC. --
Jan 24 17:56:51 worker1 systemd[1]: Started Kubernetes Kubelet.
Jan 24 17:56:55 worker1 kubelet[2103]: Flag --tls-cert-file has been deprecated, This parameter should be set via the config 
Jan 24 17:56:55 worker1 kubelet[2103]: Flag --tls-private-key-file has been deprecated, This parameter should be set via the 
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.312855    2103 flags.go:33] FLAG: --address="0.0.0.0"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.312912    2103 flags.go:33] FLAG: --allow-privileged="true"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.312921    2103 flags.go:33] FLAG: --allowed-unsafe-sysctls="[]"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.312931    2103 flags.go:33] FLAG: --alsologtostderr="false"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.312938    2103 flags.go:33] FLAG: --anonymous-auth="true"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.312943    2103 flags.go:33] FLAG: --application-metrics-count-limit="10
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.312950    2103 flags.go:33] FLAG: --authentication-token-webhook="false
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.312955    2103 flags.go:33] FLAG: --authentication-token-webhook-cache-
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.312963    2103 flags.go:33] FLAG: --authorization-mode="AlwaysAllow"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.312970    2103 flags.go:33] FLAG: --authorization-webhook-cache-authori
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.312976    2103 flags.go:33] FLAG: --authorization-webhook-cache-unautho
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.312981    2103 flags.go:33] FLAG: --azure-container-registry-config=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.312988    2103 flags.go:33] FLAG: --boot-id-file="/proc/sys/kernel/rand
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.312994    2103 flags.go:33] FLAG: --bootstrap-checkpoint-path=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.313000    2103 flags.go:33] FLAG: --bootstrap-kubeconfig=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.313005    2103 flags.go:33] FLAG: --cert-dir="/var/lib/kubelet/pki"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.313011    2103 flags.go:33] FLAG: --cgroup-driver="cgroupfs"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.313017    2103 flags.go:33] FLAG: --cgroup-root=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.313022    2103 flags.go:33] FLAG: --cgroups-per-qos="true"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.313027    2103 flags.go:33] FLAG: --chaos-chance="0"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.313037    2103 flags.go:33] FLAG: --client-ca-file=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.313042    2103 flags.go:33] FLAG: --cloud-config=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.313047    2103 flags.go:33] FLAG: --cloud-provider=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.313052    2103 flags.go:33] FLAG: --cluster-dns="[]"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.313063    2103 flags.go:33] FLAG: --cluster-domain=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.317905    2103 flags.go:33] FLAG: --cni-bin-dir="/opt/cni/bin"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.317934    2103 flags.go:33] FLAG: --cni-conf-dir="/etc/cni/net.d"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.317941    2103 flags.go:33] FLAG: --config="/var/lib/kubelet/kubelet-co
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.317949    2103 flags.go:33] FLAG: --container-hints="/etc/cadvisor/cont
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.317956    2103 flags.go:33] FLAG: --container-log-max-files="5"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.317967    2103 flags.go:33] FLAG: --container-log-max-size="10Mi"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.317972    2103 flags.go:33] FLAG: --container-runtime="docker"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.317977    2103 flags.go:33] FLAG: --container-runtime-endpoint="unix://
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.317983    2103 flags.go:33] FLAG: --containerd="unix:///var/run/contain
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.317989    2103 flags.go:33] FLAG: --containerized="false"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.317996    2103 flags.go:33] FLAG: --contention-profiling="false"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318002    2103 flags.go:33] FLAG: --cpu-cfs-quota="true"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318007    2103 flags.go:33] FLAG: --cpu-cfs-quota-period="100ms"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318013    2103 flags.go:33] FLAG: --cpu-manager-policy="none"
lines 1-43Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318018    2103 flags.go:33] FLAG: --cpu-manager-reconcile-period="10s"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318024    2103 flags.go:33] FLAG: --docker="unix:///var/run/docker.sock
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318030    2103 flags.go:33] FLAG: --docker-endpoint="unix:///var/run/do
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318036    2103 flags.go:33] FLAG: --docker-env-metadata-whitelist=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318041    2103 flags.go:33] FLAG: --docker-only="false"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318047    2103 flags.go:33] FLAG: --docker-root="/var/lib/docker"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318053    2103 flags.go:33] FLAG: --docker-tls="false"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318059    2103 flags.go:33] FLAG: --docker-tls-ca="ca.pem"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318064    2103 flags.go:33] FLAG: --docker-tls-cert="cert.pem"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318069    2103 flags.go:33] FLAG: --docker-tls-key="key.pem"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318074    2103 flags.go:33] FLAG: --dynamic-config-dir=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318082    2103 flags.go:33] FLAG: --enable-controller-attach-detach="tr
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318086    2103 flags.go:33] FLAG: --enable-debugging-handlers="true"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318091    2103 flags.go:33] FLAG: --enable-load-reader="false"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318096    2103 flags.go:33] FLAG: --enable-server="true"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318100    2103 flags.go:33] FLAG: --enforce-node-allocatable="[pods]"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318112    2103 flags.go:33] FLAG: --event-burst="10"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318117    2103 flags.go:33] FLAG: --event-qps="5"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318122    2103 flags.go:33] FLAG: --event-storage-age-limit="default=0"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318127    2103 flags.go:33] FLAG: --event-storage-event-limit="default=
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318132    2103 flags.go:33] FLAG: --eviction-hard="imagefs.available<15
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318155    2103 flags.go:33] FLAG: --eviction-max-pod-grace-period="0"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318160    2103 flags.go:33] FLAG: --eviction-minimum-reclaim=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318170    2103 flags.go:33] FLAG: --eviction-pressure-transition-period
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318175    2103 flags.go:33] FLAG: --eviction-soft=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318180    2103 flags.go:33] FLAG: --eviction-soft-grace-period=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318186    2103 flags.go:33] FLAG: --exit-on-lock-contention="false"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318191    2103 flags.go:33] FLAG: --experimental-allocatable-ignore-evi
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318196    2103 flags.go:33] FLAG: --experimental-bootstrap-kubeconfig="
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318201    2103 flags.go:33] FLAG: --experimental-check-node-capabilitie
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318206    2103 flags.go:33] FLAG: --experimental-dockershim="false"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318211    2103 flags.go:33] FLAG: --experimental-dockershim-root-direct
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318217    2103 flags.go:33] FLAG: --experimental-fail-swap-on="true"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318222    2103 flags.go:33] FLAG: --experimental-kernel-memcg-notificat
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318226    2103 flags.go:33] FLAG: --experimental-mounter-path=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318231    2103 flags.go:33] FLAG: --fail-swap-on="true"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318236    2103 flags.go:33] FLAG: --feature-gates=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318244    2103 flags.go:33] FLAG: --file-check-frequency="20s"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318249    2103 flags.go:33] FLAG: --global-housekeeping-interval="1m0s"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318256    2103 flags.go:33] FLAG: --hairpin-mode="promiscuous-bridge"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318261    2103 flags.go:33] FLAG: --healthz-bind-address="127.0.0.1"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318267    2103 flags.go:33] FLAG: --healthz-port="10248"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318272    2103 flags.go:33] FLAG: --help="false"
lines 44-86Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318277    2103 flags.go:33] FLAG: --host-ipc-sources="[*]"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318289    2103 flags.go:33] FLAG: --host-network-sources="[*]"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318295    2103 flags.go:33] FLAG: --host-pid-sources="[*]"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318302    2103 flags.go:33] FLAG: --hostname-override=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318307    2103 flags.go:33] FLAG: --housekeeping-interval="10s"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318312    2103 flags.go:33] FLAG: --http-check-frequency="20s"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318317    2103 flags.go:33] FLAG: --image-gc-high-threshold="85"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318322    2103 flags.go:33] FLAG: --image-gc-low-threshold="80"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318327    2103 flags.go:33] FLAG: --image-pull-progress-deadline="2m0s"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318332    2103 flags.go:33] FLAG: --image-service-endpoint=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318337    2103 flags.go:33] FLAG: --iptables-drop-bit="15"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318342    2103 flags.go:33] FLAG: --iptables-masquerade-bit="14"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318348    2103 flags.go:33] FLAG: --keep-terminated-pod-volumes="false"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318353    2103 flags.go:33] FLAG: --kube-api-burst="10"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318358    2103 flags.go:33] FLAG: --kube-api-content-type="application/
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318363    2103 flags.go:33] FLAG: --kube-api-qps="5"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318368    2103 flags.go:33] FLAG: --kube-reserved=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318373    2103 flags.go:33] FLAG: --kube-reserved-cgroup=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318378    2103 flags.go:33] FLAG: --kubeconfig="/var/lib/kubelet/kubeco
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318383    2103 flags.go:33] FLAG: --kubelet-cgroups=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318388    2103 flags.go:33] FLAG: --lock-file=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318393    2103 flags.go:33] FLAG: --log-backtrace-at=":0"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318399    2103 flags.go:33] FLAG: --log-cadvisor-usage="false"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318404    2103 flags.go:33] FLAG: --log-dir=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318409    2103 flags.go:33] FLAG: --log-file=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318414    2103 flags.go:33] FLAG: --log-flush-frequency="5s"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318420    2103 flags.go:33] FLAG: --logtostderr="true"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318425    2103 flags.go:33] FLAG: --machine-id-file="/etc/machine-id,/v
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318432    2103 flags.go:33] FLAG: --make-iptables-util-chains="true"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318437    2103 flags.go:33] FLAG: --manifest-url=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318441    2103 flags.go:33] FLAG: --manifest-url-header=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318450    2103 flags.go:33] FLAG: --master-service-namespace="default"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318455    2103 flags.go:33] FLAG: --max-open-files="1000000"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318462    2103 flags.go:33] FLAG: --max-pods="110"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318467    2103 flags.go:33] FLAG: --maximum-dead-containers="-1"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318472    2103 flags.go:33] FLAG: --maximum-dead-containers-per-contain
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318477    2103 flags.go:33] FLAG: --minimum-container-ttl-duration="0s"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318482    2103 flags.go:33] FLAG: --minimum-image-ttl-duration="2m0s"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318487    2103 flags.go:33] FLAG: --network-plugin="cni"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318492    2103 flags.go:33] FLAG: --network-plugin-mtu="0"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318496    2103 flags.go:33] FLAG: --node-ip=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318501    2103 flags.go:33] FLAG: --node-labels=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318509    2103 flags.go:33] FLAG: --node-status-max-images="50"
lines 87-129Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318514    2103 flags.go:33] FLAG: --node-status-update-frequency="10s"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318522    2103 flags.go:33] FLAG: --non-masquerade-cidr="10.0.0.0/8"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318527    2103 flags.go:33] FLAG: --oom-score-adj="-999"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318532    2103 flags.go:33] FLAG: --pod-cidr=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318537    2103 flags.go:33] FLAG: --pod-infra-container-image="k8s.gcr.
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318543    2103 flags.go:33] FLAG: --pod-manifest-path=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318547    2103 flags.go:33] FLAG: --pod-max-pids="-1"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318552    2103 flags.go:33] FLAG: --pods-per-core="0"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318557    2103 flags.go:33] FLAG: --port="10250"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318562    2103 flags.go:33] FLAG: --protect-kernel-defaults="false"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318567    2103 flags.go:33] FLAG: --provider-id=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318571    2103 flags.go:33] FLAG: --qos-reserved=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318576    2103 flags.go:33] FLAG: --read-only-port="10255"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318581    2103 flags.go:33] FLAG: --really-crash-for-testing="false"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318586    2103 flags.go:33] FLAG: --redirect-container-streaming="false
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318591    2103 flags.go:33] FLAG: --register-node="true"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318596    2103 flags.go:33] FLAG: --register-schedulable="true"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318602    2103 flags.go:33] FLAG: --register-with-taints=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318608    2103 flags.go:33] FLAG: --registry-burst="10"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318613    2103 flags.go:33] FLAG: --registry-qps="5"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318618    2103 flags.go:33] FLAG: --resolv-conf="/etc/resolv.conf"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318623    2103 flags.go:33] FLAG: --root-dir="/var/lib/kubelet"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318628    2103 flags.go:33] FLAG: --rotate-certificates="false"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318633    2103 flags.go:33] FLAG: --rotate-server-certificates="false"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318638    2103 flags.go:33] FLAG: --runonce="false"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318643    2103 flags.go:33] FLAG: --runtime-cgroups=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318648    2103 flags.go:33] FLAG: --runtime-request-timeout="2m0s"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318653    2103 flags.go:33] FLAG: --seccomp-profile-root="/var/lib/kube
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318658    2103 flags.go:33] FLAG: --serialize-image-pulls="true"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318663    2103 flags.go:33] FLAG: --stderrthreshold="2"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318668    2103 flags.go:33] FLAG: --storage-driver-buffer-duration="1m0
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318674    2103 flags.go:33] FLAG: --storage-driver-db="cadvisor"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318679    2103 flags.go:33] FLAG: --storage-driver-host="localhost:8086
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318685    2103 flags.go:33] FLAG: --storage-driver-password="root"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318690    2103 flags.go:33] FLAG: --storage-driver-secure="false"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318695    2103 flags.go:33] FLAG: --storage-driver-table="stats"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318700    2103 flags.go:33] FLAG: --storage-driver-user="root"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318704    2103 flags.go:33] FLAG: --streaming-connection-idle-timeout="
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318710    2103 flags.go:33] FLAG: --sync-frequency="1m0s"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318715    2103 flags.go:33] FLAG: --system-cgroups=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318719    2103 flags.go:33] FLAG: --system-reserved=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318724    2103 flags.go:33] FLAG: --system-reserved-cgroup=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318729    2103 flags.go:33] FLAG: --tls-cert-file="/var/lib/kubelet/wor
lines 130-172Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318735    2103 flags.go:33] FLAG: --tls-cipher-suites="[]"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318741    2103 flags.go:33] FLAG: --tls-min-version=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318746    2103 flags.go:33] FLAG: --tls-private-key-file="/var/lib/kube
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318751    2103 flags.go:33] FLAG: --v="2"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318757    2103 flags.go:33] FLAG: --version="false"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318765    2103 flags.go:33] FLAG: --vmodule=""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318771    2103 flags.go:33] FLAG: --volume-plugin-dir="/usr/libexec/kub
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318778    2103 flags.go:33] FLAG: --volume-stats-agg-period="1m0s"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.318825    2103 feature_gate.go:206] feature gates: &{map[]}
Jan 24 17:56:55 worker1 kubelet[2103]: Flag --tls-cert-file has been deprecated, This parameter should be set via the config 
Jan 24 17:56:55 worker1 kubelet[2103]: Flag --tls-private-key-file has been deprecated, This parameter should be set via the 
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.377969    2103 feature_gate.go:206] feature gates: &{map[]}
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.378035    2103 feature_gate.go:206] feature gates: &{map[]}
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.413652    2103 mount_linux.go:179] Detected OS with systemd
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.413706    2103 server.go:407] Version: v1.13.0
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.413773    2103 feature_gate.go:206] feature gates: &{map[]}
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.413843    2103 feature_gate.go:206] feature gates: &{map[]}
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.413932    2103 plugins.go:103] No cloud provider specified.
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.413946    2103 server.go:523] No cloud provider specified: "" from the 
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.442202    2103 manager.go:155] cAdvisor running in container: "/sys/fs/
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.579553    2103 fs.go:142] Filesystem UUIDs: map[4573eb39-57f3-439b-9a73
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.579579    2103 fs.go:143] Filesystem partitions: map[tmpfs:{mountpoint:
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.583295    2103 manager.go:229] Machine: {NumCores:2 CpuFrequency:250000
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.642487    2103 manager.go:235] Version: {KernelVersion:4.4.0-1100-aws C
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.646171    2103 server.go:666] --cgroups-per-qos enabled, but --cgroup-r
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.648467    2103 container_manager_linux.go:248] container manager verifi
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.648498    2103 container_manager_linux.go:253] Creating Container Manag
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.648637    2103 container_manager_linux.go:272] Creating device plugin m
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.648649    2103 manager.go:109] Creating Device Plugin manager at /var/l
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.648696    2103 state_mem.go:36] [cpumanager] initializing new in-memory
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.650486    2103 state_mem.go:84] [cpumanager] updated default cpuset: ""
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.650501    2103 state_mem.go:92] [cpumanager] updated cpuset assignments
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.650511    2103 state_checkpoint.go:100] [cpumanager] state checkpoint: 
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.650518    2103 state_checkpoint.go:101] [cpumanager] state checkpoint: 
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.650570    2103 server.go:941] Using root directory: /var/lib/kubelet
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.655715    2103 kubelet.go:306] Watching apiserver
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.659004    2103 client.go:75] Connecting to docker on unix:///var/run/do
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.678184    2103 client.go:104] Start docker client with request timeout=
Jan 24 17:56:55 worker1 kubelet[2103]: W0124 17:56:55.714405    2103 docker_service.go:540] Hairpin mode set to "promiscuous-
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.714435    2103 docker_service.go:236] Hairpin mode set to "hairpin-veth
Jan 24 17:56:55 worker1 kubelet[2103]: W0124 17:56:55.715477    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:56:55 worker1 kubelet[2103]: W0124 17:56:55.740604    2103 hostport_manager.go:68] The binary conntrack is not inst
Jan 24 17:56:55 worker1 kubelet[2103]: W0124 17:56:55.740729    2103 cni.go:203] Unable to update cni config: No networks fou
lines 173-215Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.740744    2103 plugins.go:159] Loaded network plugin "cni"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.740790    2103 docker_service.go:251] Docker cri networking managed by 
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.755565    2103 docker_service.go:256] Docker Info: &{ID:JEQW:BX6C:T4JV:
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.755662    2103 docker_service.go:269] Setting cgroupDriver to cgroupfs
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.755720    2103 kubelet.go:636] Starting the GRPC server for the docker 
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.755738    2103 docker_server.go:59] Start dockershim grpc server
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.879954    2103 kuberuntime_manager.go:198] Container runtime docker ini
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.885241    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/aws-
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.885276    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/empt
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.885285    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/gce-
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.885294    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/git-
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.885302    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/host
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.885310    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/nfs"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.885319    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/secr
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.885328    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/iscs
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.885350    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/glus
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.886708    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/rbd"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.886731    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/cind
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.886741    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/quob
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.886770    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/ceph
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.886784    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/down
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.886792    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/fc"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.886801    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/floc
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.886809    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/azur
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.886819    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/conf
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.886841    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/vsph
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.886851    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/azur
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.886859    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/phot
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.886870    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/proj
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.886879    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/port
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.886893    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/scal
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.886902    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/loca
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.886925    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/stor
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.886939    2103 plugins.go:547] Loaded volume plugin "kubernetes.io/csi"
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.898971    2103 server.go:999] Started kubelet
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.903497    2103 fs_resource_analyzer.go:66] Starting FS ResourceAnalyzer
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.903534    2103 status_manager.go:152] Starting to sync pod status with 
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.903564    2103 kubelet.go:1829] Starting kubelet main sync loop.
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.903588    2103 kubelet.go:1846] skipping pod synchronization - [contain
Jan 24 17:56:55 worker1 kubelet[2103]: E0124 17:56:55.906735    2103 kubelet.go:1308] Image garbage collection failed once. S
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.906973    2103 volume_manager.go:246] The desired_state_of_world popula
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.906986    2103 volume_manager.go:248] Starting Kubelet Volume Manager
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.915650    2103 desired_state_of_world_populator.go:130] Desired state p
lines 216-258Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.921390    2103 server.go:137] Starting to listen on 0.0.0.0:10250
Jan 24 17:56:55 worker1 kubelet[2103]: I0124 17:56:55.930400    2103 server.go:333] Adding debug handlers to kubelet server.
Jan 24 17:56:55 worker1 kubelet[2103]: W0124 17:56:55.977549    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:56:55 worker1 kubelet[2103]: E0124 17:56:55.977977    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.003656    2103 factory.go:356] Registering Docker factory
Jan 24 17:56:56 worker1 kubelet[2103]: E0124 17:56:56.011584    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.013525    2103 kubelet.go:1846] skipping pod synchronization - [contain
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.013946    2103 kubelet_node_status.go:278] Setting node annotation to e
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.033384    2103 factory.go:136] Registering containerd factory
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.062780    2103 factory.go:54] Registering systemd factory
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.063377    2103 factory.go:97] Registering Raw factory
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.063829    2103 manager.go:1222] Started watching for new ooms in manage
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.066309    2103 manager.go:365] Starting recovery of all containers
Jan 24 17:56:56 worker1 kubelet[2103]: E0124 17:56:56.133272    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.176929    2103 manager.go:370] Recovery completed
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.180696    2103 kubelet_node_status.go:446] Recording NodeHasSufficientM
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.181012    2103 kubelet_node_status.go:446] Recording NodeHasNoDiskPress
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.181243    2103 kubelet_node_status.go:446] Recording NodeHasSufficientP
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.181451    2103 kubelet_node_status.go:72] Attempting to register node w
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.232690    2103 kubelet.go:1846] skipping pod synchronization - [contain
Jan 24 17:56:56 worker1 kubelet[2103]: E0124 17:56:56.240011    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.261820    2103 kubelet_node_status.go:278] Setting node annotation to e
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.285442    2103 kubelet_node_status.go:446] Recording NodeHasSufficientM
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.285469    2103 kubelet_node_status.go:446] Recording NodeHasNoDiskPress
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.285480    2103 kubelet_node_status.go:446] Recording NodeHasSufficientP
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.285526    2103 cpu_manager.go:155] [cpumanager] starting with none poli
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.285532    2103 cpu_manager.go:156] [cpumanager] reconciling every 10s
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.285540    2103 policy_none.go:42] [cpumanager] none policy: Start
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.285845    2103 container_manager_linux.go:376] Updating kernel flag: vm
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.285899    2103 container_manager_linux.go:376] Updating kernel flag: ke
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.285933    2103 container_manager_linux.go:376] Updating kernel flag: ke
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.307857    2103 manager.go:196] Starting Device Plugin manager
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.310289    2103 container_manager_linux.go:434] [ContainerManager]: Disc
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.315503    2103 manager.go:231] Serving device plugin registration serve
Jan 24 17:56:56 worker1 kubelet[2103]: I0124 17:56:56.317150    2103 plugin_watcher.go:90] Plugin Watcher Start at /var/lib/k
Jan 24 17:56:56 worker1 kubelet[2103]: E0124 17:56:56.318246    2103 eviction_manager.go:243] eviction manager: failed to get
Jan 24 17:56:56 worker1 kubelet[2103]: E0124 17:56:56.340174    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:56 worker1 kubelet[2103]: E0124 17:56:56.440427    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:56 worker1 kubelet[2103]: E0124 17:56:56.540630    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:56 worker1 kubelet[2103]: E0124 17:56:56.640785    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:56 worker1 kubelet[2103]: E0124 17:56:56.740950    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:56 worker1 kubelet[2103]: E0124 17:56:56.841128    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:56 worker1 kubelet[2103]: E0124 17:56:56.941336    2103 kubelet.go:2266] node "worker1" not found
lines 259-301Jan 24 17:56:57 worker1 kubelet[2103]: E0124 17:56:57.041529    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:57 worker1 kubelet[2103]: E0124 17:56:57.141716    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:57 worker1 kubelet[2103]: E0124 17:56:57.241913    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:57 worker1 kubelet[2103]: E0124 17:56:57.342102    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:57 worker1 kubelet[2103]: E0124 17:56:57.442307    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:57 worker1 kubelet[2103]: E0124 17:56:57.542505    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:57 worker1 kubelet[2103]: E0124 17:56:57.642709    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:57 worker1 kubelet[2103]: E0124 17:56:57.742913    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:57 worker1 kubelet[2103]: E0124 17:56:57.843088    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:57 worker1 kubelet[2103]: E0124 17:56:57.943295    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:58 worker1 kubelet[2103]: E0124 17:56:58.043499    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.111710    2103 kubelet.go:1908] SyncLoop (ADD, "api"): "weave-net-v2d29
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.112203    2103 kubelet_node_status.go:278] Setting node annotation to e
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.137449    2103 kubelet_node_status.go:446] Recording NodeHasSufficientM
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.137493    2103 kubelet_node_status.go:446] Recording NodeHasNoDiskPress
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.137502    2103 kubelet_node_status.go:446] Recording NodeHasSufficientP
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.143634    2103 kubelet_node_status.go:278] Setting node annotation to e
Jan 24 17:56:58 worker1 kubelet[2103]: E0124 17:56:58.143924    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.150460    2103 reconciler.go:207] operationExecutor.VerifyControllerAtt
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.150502    2103 reconciler.go:207] operationExecutor.VerifyControllerAtt
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.150534    2103 reconciler.go:207] operationExecutor.VerifyControllerAtt
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.150577    2103 reconciler.go:207] operationExecutor.VerifyControllerAtt
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.150604    2103 reconciler.go:207] operationExecutor.VerifyControllerAtt
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.150639    2103 reconciler.go:207] operationExecutor.VerifyControllerAtt
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.150661    2103 reconciler.go:207] operationExecutor.VerifyControllerAtt
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.150685    2103 reconciler.go:207] operationExecutor.VerifyControllerAtt
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.166523    2103 kubelet_node_status.go:446] Recording NodeHasSufficientM
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.166554    2103 kubelet_node_status.go:446] Recording NodeHasNoDiskPress
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.166562    2103 kubelet_node_status.go:446] Recording NodeHasSufficientP
Jan 24 17:56:58 worker1 kubelet[2103]: E0124 17:56:58.244061    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.250906    2103 reconciler.go:252] operationExecutor.MountVolume started
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.250957    2103 reconciler.go:252] operationExecutor.MountVolume started
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.250992    2103 reconciler.go:252] operationExecutor.MountVolume started
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.251020    2103 reconciler.go:252] operationExecutor.MountVolume started
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.251049    2103 reconciler.go:252] operationExecutor.MountVolume started
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.251077    2103 reconciler.go:252] operationExecutor.MountVolume started
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.251108    2103 reconciler.go:252] operationExecutor.MountVolume started
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.251135    2103 reconciler.go:252] operationExecutor.MountVolume started
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.251149    2103 reconciler.go:154] Reconciler: start to sync state
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.252053    2103 operation_generator.go:567] MountVolume.SetUp succeeded 
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.252198    2103 operation_generator.go:567] MountVolume.SetUp succeeded 
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.252235    2103 operation_generator.go:567] MountVolume.SetUp succeeded 
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.252258    2103 operation_generator.go:567] MountVolume.SetUp succeeded 
lines 302-344Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.252280    2103 operation_generator.go:567] MountVolume.SetUp succeeded 
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.252303    2103 operation_generator.go:567] MountVolume.SetUp succeeded 
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.252324    2103 operation_generator.go:567] MountVolume.SetUp succeeded 
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.278772    2103 operation_generator.go:567] MountVolume.SetUp succeeded 
Jan 24 17:56:58 worker1 kubelet[2103]: E0124 17:56:58.344211    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:58 worker1 kubelet[2103]: E0124 17:56:58.444384    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:58 worker1 kubelet[2103]: I0124 17:56:58.471757    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 17:56:58 worker1 kubelet[2103]: E0124 17:56:58.471844    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 17:56:58 worker1 kubelet[2103]: E0124 17:56:58.471862    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 17:56:58 worker1 kubelet[2103]: E0124 17:56:58.471960    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 17:56:58 worker1 kubelet[2103]: E0124 17:56:58.545003    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:58 worker1 kubelet[2103]: E0124 17:56:58.645165    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:58 worker1 kubelet[2103]: E0124 17:56:58.745415    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:58 worker1 kubelet[2103]: E0124 17:56:58.845681    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:58 worker1 kubelet[2103]: E0124 17:56:58.945874    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:59 worker1 kubelet[2103]: E0124 17:56:59.046029    2103 kubelet.go:2266] node "worker1" not found
Jan 24 17:56:59 worker1 kubelet[2103]: I0124 17:56:59.087074    2103 kubelet_node_status.go:114] Node worker1 was previously 
Jan 24 17:56:59 worker1 kubelet[2103]: I0124 17:56:59.087481    2103 kubelet_node_status.go:75] Successfully registered node 
Jan 24 17:56:59 worker1 kubelet[2103]: I0124 17:56:59.121442    2103 kubelet_node_status.go:446] Recording NodeHasSufficientM
Jan 24 17:56:59 worker1 kubelet[2103]: I0124 17:56:59.121478    2103 kubelet_node_status.go:446] Recording NodeHasNoDiskPress
Jan 24 17:56:59 worker1 kubelet[2103]: I0124 17:56:59.121735    2103 kubelet_node_status.go:446] Recording NodeHasSufficientP
Jan 24 17:56:59 worker1 kubelet[2103]: I0124 17:56:59.121764    2103 kubelet_node_status.go:446] Recording NodeNotReady event
Jan 24 17:56:59 worker1 kubelet[2103]: I0124 17:56:59.121774    2103 setters.go:518] Node became not ready: {Type:Ready Statu
Jan 24 17:57:01 worker1 kubelet[2103]: W0124 17:57:01.333619    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:57:01 worker1 kubelet[2103]: E0124 17:57:01.333760    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:57:05 worker1 kubelet[2103]: E0124 17:57:05.660211    2103 reflector.go:134] k8s.io/kubernetes/pkg/kubelet/kubelet.
Jan 24 17:57:06 worker1 kubelet[2103]: W0124 17:57:06.349654    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:57:06 worker1 kubelet[2103]: E0124 17:57:06.349992    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:57:11 worker1 kubelet[2103]: W0124 17:57:11.365428    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:57:11 worker1 kubelet[2103]: E0124 17:57:11.365568    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:57:12 worker1 kubelet[2103]: I0124 17:57:12.904031    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 17:57:12 worker1 kubelet[2103]: E0124 17:57:12.904115    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 17:57:12 worker1 kubelet[2103]: E0124 17:57:12.904131    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 17:57:12 worker1 kubelet[2103]: E0124 17:57:12.904202    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 17:57:16 worker1 kubelet[2103]: W0124 17:57:16.374071    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:57:16 worker1 kubelet[2103]: E0124 17:57:16.374216    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:57:21 worker1 kubelet[2103]: W0124 17:57:21.383185    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:57:21 worker1 kubelet[2103]: E0124 17:57:21.383438    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:57:26 worker1 kubelet[2103]: W0124 17:57:26.403272    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:57:26 worker1 kubelet[2103]: E0124 17:57:26.403431    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:57:27 worker1 kubelet[2103]: I0124 17:57:27.904139    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 17:57:27 worker1 kubelet[2103]: E0124 17:57:27.904542    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 17:57:27 worker1 kubelet[2103]: E0124 17:57:27.904760    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
lines 345-387Jan 24 17:57:27 worker1 kubelet[2103]: E0124 17:57:27.904973    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 17:57:31 worker1 kubelet[2103]: W0124 17:57:31.411612    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:57:31 worker1 kubelet[2103]: E0124 17:57:31.412054    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:57:36 worker1 kubelet[2103]: W0124 17:57:36.423421    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:57:36 worker1 kubelet[2103]: E0124 17:57:36.423566    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:57:40 worker1 kubelet[2103]: I0124 17:57:40.903999    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 17:57:40 worker1 kubelet[2103]: E0124 17:57:40.904154    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 17:57:40 worker1 kubelet[2103]: E0124 17:57:40.904171    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 17:57:40 worker1 kubelet[2103]: E0124 17:57:40.904217    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 17:57:41 worker1 kubelet[2103]: W0124 17:57:41.436265    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:57:41 worker1 kubelet[2103]: E0124 17:57:41.436816    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:57:46 worker1 kubelet[2103]: W0124 17:57:46.445393    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:57:46 worker1 kubelet[2103]: E0124 17:57:46.445906    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:57:51 worker1 kubelet[2103]: W0124 17:57:51.458086    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:57:51 worker1 kubelet[2103]: E0124 17:57:51.458226    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:57:53 worker1 kubelet[2103]: I0124 17:57:53.906497    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 17:57:53 worker1 kubelet[2103]: E0124 17:57:53.906991    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 17:57:53 worker1 kubelet[2103]: E0124 17:57:53.907236    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 17:57:53 worker1 kubelet[2103]: E0124 17:57:53.907486    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 17:57:56 worker1 kubelet[2103]: W0124 17:57:56.468793    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:57:56 worker1 kubelet[2103]: E0124 17:57:56.468940    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:58:01 worker1 kubelet[2103]: W0124 17:58:01.484027    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:58:01 worker1 kubelet[2103]: E0124 17:58:01.484193    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:58:06 worker1 kubelet[2103]: W0124 17:58:06.493218    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:58:06 worker1 kubelet[2103]: E0124 17:58:06.493853    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:58:06 worker1 kubelet[2103]: I0124 17:58:06.904035    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 17:58:06 worker1 kubelet[2103]: E0124 17:58:06.904120    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 17:58:06 worker1 kubelet[2103]: E0124 17:58:06.904137    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 17:58:06 worker1 kubelet[2103]: E0124 17:58:06.904185    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 17:58:11 worker1 kubelet[2103]: W0124 17:58:11.507346    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:58:11 worker1 kubelet[2103]: E0124 17:58:11.507959    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:58:16 worker1 kubelet[2103]: W0124 17:58:16.540606    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:58:16 worker1 kubelet[2103]: E0124 17:58:16.541037    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:58:17 worker1 kubelet[2103]: I0124 17:58:17.904150    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 17:58:17 worker1 kubelet[2103]: E0124 17:58:17.904683    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 17:58:17 worker1 kubelet[2103]: E0124 17:58:17.904979    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 17:58:17 worker1 kubelet[2103]: E0124 17:58:17.905836    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 17:58:21 worker1 kubelet[2103]: W0124 17:58:21.552014    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:58:21 worker1 kubelet[2103]: E0124 17:58:21.552172    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:58:26 worker1 kubelet[2103]: W0124 17:58:26.563994    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:58:26 worker1 kubelet[2103]: E0124 17:58:26.564155    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:58:28 worker1 kubelet[2103]: I0124 17:58:28.904394    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 17:58:28 worker1 kubelet[2103]: E0124 17:58:28.904574    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
lines 388-430Jan 24 17:58:28 worker1 kubelet[2103]: E0124 17:58:28.904591    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 17:58:28 worker1 kubelet[2103]: E0124 17:58:28.904642    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 17:58:31 worker1 kubelet[2103]: W0124 17:58:31.572245    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:58:31 worker1 kubelet[2103]: E0124 17:58:31.572538    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:58:36 worker1 kubelet[2103]: W0124 17:58:36.586741    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:58:36 worker1 kubelet[2103]: E0124 17:58:36.586884    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:58:41 worker1 kubelet[2103]: W0124 17:58:41.595881    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:58:41 worker1 kubelet[2103]: E0124 17:58:41.596137    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:58:43 worker1 kubelet[2103]: I0124 17:58:43.904221    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 17:58:43 worker1 kubelet[2103]: E0124 17:58:43.904293    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 17:58:43 worker1 kubelet[2103]: E0124 17:58:43.904307    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 17:58:43 worker1 kubelet[2103]: E0124 17:58:43.904351    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 17:58:46 worker1 kubelet[2103]: W0124 17:58:46.607453    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:58:46 worker1 kubelet[2103]: E0124 17:58:46.607723    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:58:51 worker1 kubelet[2103]: W0124 17:58:51.640426    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:58:51 worker1 kubelet[2103]: E0124 17:58:51.641656    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:58:56 worker1 kubelet[2103]: W0124 17:58:56.656770    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:58:56 worker1 kubelet[2103]: E0124 17:58:56.656932    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:58:57 worker1 kubelet[2103]: I0124 17:58:57.904308    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 17:58:57 worker1 kubelet[2103]: E0124 17:58:57.904382    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 17:58:57 worker1 kubelet[2103]: E0124 17:58:57.904398    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 17:58:57 worker1 kubelet[2103]: E0124 17:58:57.904435    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 17:59:01 worker1 kubelet[2103]: W0124 17:59:01.665581    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:59:01 worker1 kubelet[2103]: E0124 17:59:01.668163    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:59:06 worker1 kubelet[2103]: W0124 17:59:06.676810    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:59:06 worker1 kubelet[2103]: E0124 17:59:06.677305    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:59:11 worker1 kubelet[2103]: W0124 17:59:11.686367    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:59:11 worker1 kubelet[2103]: E0124 17:59:11.686537    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:59:11 worker1 kubelet[2103]: I0124 17:59:11.904035    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 17:59:11 worker1 kubelet[2103]: E0124 17:59:11.904881    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 17:59:11 worker1 kubelet[2103]: E0124 17:59:11.905176    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 17:59:11 worker1 kubelet[2103]: E0124 17:59:11.905429    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 17:59:16 worker1 kubelet[2103]: W0124 17:59:16.694329    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:59:16 worker1 kubelet[2103]: E0124 17:59:16.694533    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:59:21 worker1 kubelet[2103]: W0124 17:59:21.707103    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:59:21 worker1 kubelet[2103]: E0124 17:59:21.707252    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:59:26 worker1 kubelet[2103]: W0124 17:59:26.715743    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:59:26 worker1 kubelet[2103]: E0124 17:59:26.715892    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:59:26 worker1 kubelet[2103]: I0124 17:59:26.904036    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 17:59:26 worker1 kubelet[2103]: E0124 17:59:26.904120    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 17:59:26 worker1 kubelet[2103]: E0124 17:59:26.904137    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 17:59:26 worker1 kubelet[2103]: E0124 17:59:26.904186    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 17:59:31 worker1 kubelet[2103]: W0124 17:59:31.725030    2103 cni.go:203] Unable to update cni config: No networks fou
lines 431-473Jan 24 17:59:31 worker1 kubelet[2103]: E0124 17:59:31.725242    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:59:36 worker1 kubelet[2103]: W0124 17:59:36.734842    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:59:36 worker1 kubelet[2103]: E0124 17:59:36.735663    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:59:41 worker1 kubelet[2103]: W0124 17:59:41.746381    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:59:41 worker1 kubelet[2103]: E0124 17:59:41.746550    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:59:41 worker1 kubelet[2103]: I0124 17:59:41.904206    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 17:59:41 worker1 kubelet[2103]: E0124 17:59:41.904747    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 17:59:41 worker1 kubelet[2103]: E0124 17:59:41.905025    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 17:59:41 worker1 kubelet[2103]: E0124 17:59:41.905325    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 17:59:46 worker1 kubelet[2103]: W0124 17:59:46.763613    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:59:46 worker1 kubelet[2103]: E0124 17:59:46.764263    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:59:51 worker1 kubelet[2103]: W0124 17:59:51.773017    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:59:51 worker1 kubelet[2103]: E0124 17:59:51.773193    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:59:56 worker1 kubelet[2103]: W0124 17:59:56.787592    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 17:59:56 worker1 kubelet[2103]: E0124 17:59:56.787738    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 17:59:56 worker1 kubelet[2103]: I0124 17:59:56.904421    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 17:59:56 worker1 kubelet[2103]: E0124 17:59:56.904500    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 17:59:56 worker1 kubelet[2103]: E0124 17:59:56.904515    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 17:59:56 worker1 kubelet[2103]: E0124 17:59:56.904559    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:00:01 worker1 kubelet[2103]: W0124 18:00:01.796377    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:00:01 worker1 kubelet[2103]: E0124 18:00:01.796536    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:00:06 worker1 kubelet[2103]: W0124 18:00:06.804260    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:00:06 worker1 kubelet[2103]: E0124 18:00:06.804482    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:00:07 worker1 kubelet[2103]: I0124 18:00:07.904164    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:00:07 worker1 kubelet[2103]: E0124 18:00:07.904709    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:00:07 worker1 kubelet[2103]: E0124 18:00:07.904979    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:00:07 worker1 kubelet[2103]: E0124 18:00:07.905294    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:00:11 worker1 kubelet[2103]: W0124 18:00:11.824316    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:00:11 worker1 kubelet[2103]: E0124 18:00:11.824486    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:00:16 worker1 kubelet[2103]: W0124 18:00:16.833493    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:00:16 worker1 kubelet[2103]: E0124 18:00:16.833660    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:00:19 worker1 kubelet[2103]: I0124 18:00:19.904118    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:00:19 worker1 kubelet[2103]: E0124 18:00:19.904634    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:00:19 worker1 kubelet[2103]: E0124 18:00:19.905421    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:00:19 worker1 kubelet[2103]: E0124 18:00:19.905770    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:00:21 worker1 kubelet[2103]: W0124 18:00:21.843920    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:00:21 worker1 kubelet[2103]: E0124 18:00:21.845362    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:00:26 worker1 kubelet[2103]: W0124 18:00:26.854165    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:00:26 worker1 kubelet[2103]: E0124 18:00:26.854488    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:00:30 worker1 kubelet[2103]: I0124 18:00:30.904131    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:00:30 worker1 kubelet[2103]: E0124 18:00:30.904219    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:00:30 worker1 kubelet[2103]: E0124 18:00:30.904235    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:00:30 worker1 kubelet[2103]: E0124 18:00:30.904292    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
lines 474-516Jan 24 18:00:31 worker1 kubelet[2103]: W0124 18:00:31.862561    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:00:31 worker1 kubelet[2103]: E0124 18:00:31.862740    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:00:36 worker1 kubelet[2103]: W0124 18:00:36.877882    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:00:36 worker1 kubelet[2103]: E0124 18:00:36.878028    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:00:41 worker1 kubelet[2103]: W0124 18:00:41.888181    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:00:41 worker1 kubelet[2103]: E0124 18:00:41.888350    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:00:42 worker1 kubelet[2103]: I0124 18:00:42.904039    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:00:42 worker1 kubelet[2103]: E0124 18:00:42.904117    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:00:42 worker1 kubelet[2103]: E0124 18:00:42.904134    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:00:42 worker1 kubelet[2103]: E0124 18:00:42.904181    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:00:46 worker1 kubelet[2103]: W0124 18:00:46.898027    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:00:46 worker1 kubelet[2103]: E0124 18:00:46.898457    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:00:51 worker1 kubelet[2103]: W0124 18:00:51.908236    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:00:51 worker1 kubelet[2103]: E0124 18:00:51.908540    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:00:56 worker1 kubelet[2103]: I0124 18:00:56.904039    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:00:56 worker1 kubelet[2103]: E0124 18:00:56.904119    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:00:56 worker1 kubelet[2103]: E0124 18:00:56.904136    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:00:56 worker1 kubelet[2103]: E0124 18:00:56.904181    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:00:56 worker1 kubelet[2103]: W0124 18:00:56.916750    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:00:56 worker1 kubelet[2103]: E0124 18:00:56.916924    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:01:01 worker1 kubelet[2103]: W0124 18:01:01.926714    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:01:01 worker1 kubelet[2103]: E0124 18:01:01.927031    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:01:06 worker1 kubelet[2103]: W0124 18:01:06.936135    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:01:06 worker1 kubelet[2103]: E0124 18:01:06.936622    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:01:10 worker1 kubelet[2103]: I0124 18:01:10.904071    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:01:10 worker1 kubelet[2103]: E0124 18:01:10.904153    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:01:10 worker1 kubelet[2103]: E0124 18:01:10.904169    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:01:10 worker1 kubelet[2103]: E0124 18:01:10.904218    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:01:11 worker1 kubelet[2103]: W0124 18:01:11.947394    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:01:11 worker1 kubelet[2103]: E0124 18:01:11.947604    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:01:16 worker1 kubelet[2103]: W0124 18:01:16.956092    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:01:16 worker1 kubelet[2103]: E0124 18:01:16.956490    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:01:21 worker1 kubelet[2103]: W0124 18:01:21.964987    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:01:21 worker1 kubelet[2103]: E0124 18:01:21.965148    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:01:23 worker1 kubelet[2103]: I0124 18:01:23.904131    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:01:23 worker1 kubelet[2103]: E0124 18:01:23.904205    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:01:23 worker1 kubelet[2103]: E0124 18:01:23.904219    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:01:23 worker1 kubelet[2103]: E0124 18:01:23.904260    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:01:26 worker1 kubelet[2103]: W0124 18:01:26.976496    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:01:26 worker1 kubelet[2103]: E0124 18:01:26.976804    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:01:31 worker1 kubelet[2103]: W0124 18:01:31.987123    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:01:31 worker1 kubelet[2103]: E0124 18:01:31.987305    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:01:36 worker1 kubelet[2103]: I0124 18:01:36.904040    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
lines 517-559Jan 24 18:01:36 worker1 kubelet[2103]: E0124 18:01:36.904120    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:01:36 worker1 kubelet[2103]: E0124 18:01:36.904187    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:01:36 worker1 kubelet[2103]: E0124 18:01:36.904246    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:01:37 worker1 kubelet[2103]: W0124 18:01:37.001782    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:01:37 worker1 kubelet[2103]: E0124 18:01:37.001955    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:01:42 worker1 kubelet[2103]: W0124 18:01:42.013892    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:01:42 worker1 kubelet[2103]: E0124 18:01:42.014050    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:01:47 worker1 kubelet[2103]: W0124 18:01:47.023509    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:01:47 worker1 kubelet[2103]: E0124 18:01:47.023675    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:01:49 worker1 kubelet[2103]: I0124 18:01:49.904284    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:01:49 worker1 kubelet[2103]: E0124 18:01:49.904809    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:01:49 worker1 kubelet[2103]: E0124 18:01:49.905067    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:01:49 worker1 kubelet[2103]: E0124 18:01:49.905356    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:01:52 worker1 kubelet[2103]: W0124 18:01:52.032353    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:01:52 worker1 kubelet[2103]: E0124 18:01:52.032520    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:01:55 worker1 kubelet[2103]: I0124 18:01:55.908259    2103 kubelet.go:1318] Image garbage collection succeeded
Jan 24 18:01:56 worker1 kubelet[2103]: I0124 18:01:56.311045    2103 container_manager_linux.go:434] [ContainerManager]: Disc
Jan 24 18:01:57 worker1 kubelet[2103]: W0124 18:01:57.040258    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:01:57 worker1 kubelet[2103]: E0124 18:01:57.041022    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:02:02 worker1 kubelet[2103]: W0124 18:02:02.052737    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:02:02 worker1 kubelet[2103]: E0124 18:02:02.052882    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:02:02 worker1 kubelet[2103]: I0124 18:02:02.904191    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:02:02 worker1 kubelet[2103]: E0124 18:02:02.904272    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:02:02 worker1 kubelet[2103]: E0124 18:02:02.904288    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:02:02 worker1 kubelet[2103]: E0124 18:02:02.904332    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:02:07 worker1 kubelet[2103]: W0124 18:02:07.062662    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:02:07 worker1 kubelet[2103]: E0124 18:02:07.063446    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:02:12 worker1 kubelet[2103]: W0124 18:02:12.071458    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:02:12 worker1 kubelet[2103]: E0124 18:02:12.072006    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:02:16 worker1 kubelet[2103]: I0124 18:02:16.904036    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:02:16 worker1 kubelet[2103]: E0124 18:02:16.904109    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:02:16 worker1 kubelet[2103]: E0124 18:02:16.904124    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:02:16 worker1 kubelet[2103]: E0124 18:02:16.904169    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:02:17 worker1 kubelet[2103]: W0124 18:02:17.083440    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:02:17 worker1 kubelet[2103]: E0124 18:02:17.084971    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:02:22 worker1 kubelet[2103]: W0124 18:02:22.095909    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:02:22 worker1 kubelet[2103]: E0124 18:02:22.096191    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:02:27 worker1 kubelet[2103]: W0124 18:02:27.104570    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:02:27 worker1 kubelet[2103]: E0124 18:02:27.104712    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:02:28 worker1 kubelet[2103]: I0124 18:02:28.904232    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:02:28 worker1 kubelet[2103]: E0124 18:02:28.904355    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:02:28 worker1 kubelet[2103]: E0124 18:02:28.904373    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:02:28 worker1 kubelet[2103]: E0124 18:02:28.904656    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
lines 560-602Jan 24 18:02:32 worker1 kubelet[2103]: W0124 18:02:32.115029    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:02:32 worker1 kubelet[2103]: E0124 18:02:32.115172    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:02:37 worker1 kubelet[2103]: W0124 18:02:37.144511    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:02:37 worker1 kubelet[2103]: E0124 18:02:37.145015    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:02:42 worker1 kubelet[2103]: W0124 18:02:42.154269    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:02:42 worker1 kubelet[2103]: E0124 18:02:42.154417    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:02:43 worker1 kubelet[2103]: I0124 18:02:43.904111    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:02:43 worker1 kubelet[2103]: E0124 18:02:43.904187    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:02:43 worker1 kubelet[2103]: E0124 18:02:43.904202    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:02:43 worker1 kubelet[2103]: E0124 18:02:43.904244    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:02:47 worker1 kubelet[2103]: W0124 18:02:47.163673    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:02:47 worker1 kubelet[2103]: E0124 18:02:47.164060    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:02:52 worker1 kubelet[2103]: W0124 18:02:52.172421    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:02:52 worker1 kubelet[2103]: E0124 18:02:52.172569    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:02:57 worker1 kubelet[2103]: W0124 18:02:57.181273    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:02:57 worker1 kubelet[2103]: E0124 18:02:57.181424    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:02:57 worker1 kubelet[2103]: I0124 18:02:57.904097    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:02:57 worker1 kubelet[2103]: E0124 18:02:57.904599    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:02:57 worker1 kubelet[2103]: E0124 18:02:57.904853    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:02:57 worker1 kubelet[2103]: E0124 18:02:57.905115    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:03:02 worker1 kubelet[2103]: W0124 18:03:02.194066    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:03:02 worker1 kubelet[2103]: E0124 18:03:02.194294    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:03:07 worker1 kubelet[2103]: W0124 18:03:07.203277    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:03:07 worker1 kubelet[2103]: E0124 18:03:07.203634    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:03:08 worker1 kubelet[2103]: I0124 18:03:08.904235    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:03:08 worker1 kubelet[2103]: E0124 18:03:08.904358    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:03:08 worker1 kubelet[2103]: E0124 18:03:08.904375    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:03:08 worker1 kubelet[2103]: E0124 18:03:08.904565    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:03:12 worker1 kubelet[2103]: W0124 18:03:12.213286    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:03:12 worker1 kubelet[2103]: E0124 18:03:12.213436    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:03:17 worker1 kubelet[2103]: W0124 18:03:17.226151    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:03:17 worker1 kubelet[2103]: E0124 18:03:17.226349    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:03:21 worker1 kubelet[2103]: I0124 18:03:21.904282    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:03:21 worker1 kubelet[2103]: E0124 18:03:21.904654    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:03:21 worker1 kubelet[2103]: E0124 18:03:21.904907    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:03:21 worker1 kubelet[2103]: E0124 18:03:21.905205    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:03:22 worker1 kubelet[2103]: W0124 18:03:22.235918    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:03:22 worker1 kubelet[2103]: E0124 18:03:22.236507    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:03:27 worker1 kubelet[2103]: W0124 18:03:27.246014    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:03:27 worker1 kubelet[2103]: E0124 18:03:27.246164    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:03:32 worker1 kubelet[2103]: W0124 18:03:32.256712    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:03:32 worker1 kubelet[2103]: E0124 18:03:32.256861    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:03:35 worker1 kubelet[2103]: I0124 18:03:35.904116    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
lines 603-645Jan 24 18:03:35 worker1 kubelet[2103]: E0124 18:03:35.904642    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:03:35 worker1 kubelet[2103]: E0124 18:03:35.904916    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:03:35 worker1 kubelet[2103]: E0124 18:03:35.905231    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:03:37 worker1 kubelet[2103]: W0124 18:03:37.266232    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:03:37 worker1 kubelet[2103]: E0124 18:03:37.266416    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:03:42 worker1 kubelet[2103]: W0124 18:03:42.276628    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:03:42 worker1 kubelet[2103]: E0124 18:03:42.276781    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:03:47 worker1 kubelet[2103]: W0124 18:03:47.288480    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:03:47 worker1 kubelet[2103]: E0124 18:03:47.288628    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:03:49 worker1 kubelet[2103]: I0124 18:03:49.904150    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:03:49 worker1 kubelet[2103]: E0124 18:03:49.904712    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:03:49 worker1 kubelet[2103]: E0124 18:03:49.905019    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:03:49 worker1 kubelet[2103]: E0124 18:03:49.905340    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:03:52 worker1 kubelet[2103]: W0124 18:03:52.299802    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:03:52 worker1 kubelet[2103]: E0124 18:03:52.299979    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:03:57 worker1 kubelet[2103]: W0124 18:03:57.311705    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:03:57 worker1 kubelet[2103]: E0124 18:03:57.312172    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:04:01 worker1 kubelet[2103]: I0124 18:04:01.904129    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:04:01 worker1 kubelet[2103]: E0124 18:04:01.904653    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:04:01 worker1 kubelet[2103]: E0124 18:04:01.904912    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:04:01 worker1 kubelet[2103]: E0124 18:04:01.905201    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:04:02 worker1 kubelet[2103]: W0124 18:04:02.323998    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:04:02 worker1 kubelet[2103]: E0124 18:04:02.324219    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:04:07 worker1 kubelet[2103]: W0124 18:04:07.334018    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:04:07 worker1 kubelet[2103]: E0124 18:04:07.334186    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:04:12 worker1 kubelet[2103]: W0124 18:04:12.342832    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:04:12 worker1 kubelet[2103]: E0124 18:04:12.343001    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:04:16 worker1 kubelet[2103]: I0124 18:04:16.904024    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:04:16 worker1 kubelet[2103]: E0124 18:04:16.904099    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:04:16 worker1 kubelet[2103]: E0124 18:04:16.904113    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:04:16 worker1 kubelet[2103]: E0124 18:04:16.904296    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:04:17 worker1 kubelet[2103]: W0124 18:04:17.351222    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:04:17 worker1 kubelet[2103]: E0124 18:04:17.351389    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:04:22 worker1 kubelet[2103]: W0124 18:04:22.361337    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:04:22 worker1 kubelet[2103]: E0124 18:04:22.361696    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:04:27 worker1 kubelet[2103]: W0124 18:04:27.370628    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:04:27 worker1 kubelet[2103]: E0124 18:04:27.371380    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:04:31 worker1 kubelet[2103]: I0124 18:04:31.904259    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:04:31 worker1 kubelet[2103]: E0124 18:04:31.904978    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:04:31 worker1 kubelet[2103]: E0124 18:04:31.905309    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:04:31 worker1 kubelet[2103]: E0124 18:04:31.905566    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:04:32 worker1 kubelet[2103]: W0124 18:04:32.379995    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:04:32 worker1 kubelet[2103]: E0124 18:04:32.380163    2103 kubelet.go:2192] Container runtime network not ready: Ne
lines 646-688Jan 24 18:04:37 worker1 kubelet[2103]: W0124 18:04:37.389539    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:04:37 worker1 kubelet[2103]: E0124 18:04:37.389743    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:04:42 worker1 kubelet[2103]: W0124 18:04:42.405469    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:04:42 worker1 kubelet[2103]: E0124 18:04:42.406453    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:04:42 worker1 kubelet[2103]: I0124 18:04:42.904328    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:04:42 worker1 kubelet[2103]: E0124 18:04:42.904671    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:04:42 worker1 kubelet[2103]: E0124 18:04:42.904695    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:04:42 worker1 kubelet[2103]: E0124 18:04:42.904746    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:04:47 worker1 kubelet[2103]: W0124 18:04:47.421384    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:04:47 worker1 kubelet[2103]: E0124 18:04:47.421836    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:04:52 worker1 kubelet[2103]: W0124 18:04:52.435701    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:04:52 worker1 kubelet[2103]: E0124 18:04:52.435853    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:04:54 worker1 kubelet[2103]: I0124 18:04:54.904083    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:04:54 worker1 kubelet[2103]: E0124 18:04:54.904163    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:04:54 worker1 kubelet[2103]: E0124 18:04:54.904179    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:04:54 worker1 kubelet[2103]: E0124 18:04:54.904224    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:04:57 worker1 kubelet[2103]: W0124 18:04:57.446576    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:04:57 worker1 kubelet[2103]: E0124 18:04:57.446890    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:05:02 worker1 kubelet[2103]: W0124 18:05:02.458815    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:05:02 worker1 kubelet[2103]: E0124 18:05:02.459054    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:05:06 worker1 kubelet[2103]: I0124 18:05:06.903972    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:05:06 worker1 kubelet[2103]: E0124 18:05:06.904045    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:05:06 worker1 kubelet[2103]: E0124 18:05:06.904061    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:05:06 worker1 kubelet[2103]: E0124 18:05:06.904101    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:05:07 worker1 kubelet[2103]: W0124 18:05:07.469298    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:05:07 worker1 kubelet[2103]: E0124 18:05:07.469456    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:05:12 worker1 kubelet[2103]: W0124 18:05:12.479659    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:05:12 worker1 kubelet[2103]: E0124 18:05:12.479958    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:05:17 worker1 kubelet[2103]: W0124 18:05:17.492725    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:05:17 worker1 kubelet[2103]: E0124 18:05:17.496288    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:05:19 worker1 kubelet[2103]: I0124 18:05:19.904191    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:05:19 worker1 kubelet[2103]: E0124 18:05:19.904642    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:05:19 worker1 kubelet[2103]: E0124 18:05:19.904844    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:05:19 worker1 kubelet[2103]: E0124 18:05:19.905124    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:05:22 worker1 kubelet[2103]: W0124 18:05:22.506961    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:05:22 worker1 kubelet[2103]: E0124 18:05:22.507120    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:05:27 worker1 kubelet[2103]: W0124 18:05:27.515812    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:05:27 worker1 kubelet[2103]: E0124 18:05:27.515969    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:05:31 worker1 kubelet[2103]: I0124 18:05:31.904118    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:05:31 worker1 kubelet[2103]: E0124 18:05:31.904195    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:05:31 worker1 kubelet[2103]: E0124 18:05:31.904213    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:05:31 worker1 kubelet[2103]: E0124 18:05:31.904416    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:05:32 worker1 kubelet[2103]: W0124 18:05:32.526475    2103 cni.go:203] Unable to update cni config: No networks fou
lines 689-731Jan 24 18:05:32 worker1 kubelet[2103]: E0124 18:05:32.526631    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:05:37 worker1 kubelet[2103]: W0124 18:05:37.536039    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:05:37 worker1 kubelet[2103]: E0124 18:05:37.536188    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:05:42 worker1 kubelet[2103]: W0124 18:05:42.568123    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:05:42 worker1 kubelet[2103]: E0124 18:05:42.569338    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:05:44 worker1 kubelet[2103]: I0124 18:05:44.904186    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:05:44 worker1 kubelet[2103]: E0124 18:05:44.904270    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:05:44 worker1 kubelet[2103]: E0124 18:05:44.904284    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:05:44 worker1 kubelet[2103]: E0124 18:05:44.904327    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:05:47 worker1 kubelet[2103]: W0124 18:05:47.579317    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:05:47 worker1 kubelet[2103]: E0124 18:05:47.579821    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:05:52 worker1 kubelet[2103]: W0124 18:05:52.596445    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:05:52 worker1 kubelet[2103]: E0124 18:05:52.596849    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:05:55 worker1 kubelet[2103]: I0124 18:05:55.904076    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:05:55 worker1 kubelet[2103]: E0124 18:05:55.904150    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:05:55 worker1 kubelet[2103]: E0124 18:05:55.904166    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:05:55 worker1 kubelet[2103]: E0124 18:05:55.904347    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:05:57 worker1 kubelet[2103]: W0124 18:05:57.608206    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:05:57 worker1 kubelet[2103]: E0124 18:05:57.608733    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:06:02 worker1 kubelet[2103]: W0124 18:06:02.622911    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:06:02 worker1 kubelet[2103]: E0124 18:06:02.623093    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:06:07 worker1 kubelet[2103]: W0124 18:06:07.632690    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:06:07 worker1 kubelet[2103]: E0124 18:06:07.633031    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:06:08 worker1 kubelet[2103]: I0124 18:06:08.904060    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:06:08 worker1 kubelet[2103]: E0124 18:06:08.904148    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:06:08 worker1 kubelet[2103]: E0124 18:06:08.904165    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:06:08 worker1 kubelet[2103]: E0124 18:06:08.904375    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:06:12 worker1 kubelet[2103]: W0124 18:06:12.645619    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:06:12 worker1 kubelet[2103]: E0124 18:06:12.646031    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:06:17 worker1 kubelet[2103]: W0124 18:06:17.656767    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:06:17 worker1 kubelet[2103]: E0124 18:06:17.657029    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:06:22 worker1 kubelet[2103]: W0124 18:06:22.666571    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:06:22 worker1 kubelet[2103]: E0124 18:06:22.666857    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:06:22 worker1 kubelet[2103]: I0124 18:06:22.904046    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:06:22 worker1 kubelet[2103]: E0124 18:06:22.904120    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:06:22 worker1 kubelet[2103]: E0124 18:06:22.904136    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:06:22 worker1 kubelet[2103]: E0124 18:06:22.904178    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:06:27 worker1 kubelet[2103]: W0124 18:06:27.677902    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:06:27 worker1 kubelet[2103]: E0124 18:06:27.679005    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:06:32 worker1 kubelet[2103]: W0124 18:06:32.687202    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:06:32 worker1 kubelet[2103]: E0124 18:06:32.687573    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:06:34 worker1 kubelet[2103]: I0124 18:06:34.903973    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:06:34 worker1 kubelet[2103]: E0124 18:06:34.904060    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
lines 732-774Jan 24 18:06:34 worker1 kubelet[2103]: E0124 18:06:34.904076    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:06:34 worker1 kubelet[2103]: E0124 18:06:34.904117    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:06:37 worker1 kubelet[2103]: W0124 18:06:37.696845    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:06:37 worker1 kubelet[2103]: E0124 18:06:37.697128    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:06:42 worker1 kubelet[2103]: W0124 18:06:42.711094    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:06:42 worker1 kubelet[2103]: E0124 18:06:42.711252    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:06:46 worker1 kubelet[2103]: I0124 18:06:46.904198    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:06:46 worker1 kubelet[2103]: E0124 18:06:46.904277    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:06:46 worker1 kubelet[2103]: E0124 18:06:46.904293    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:06:46 worker1 kubelet[2103]: E0124 18:06:46.904334    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:06:47 worker1 kubelet[2103]: W0124 18:06:47.721166    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:06:47 worker1 kubelet[2103]: E0124 18:06:47.721763    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:06:52 worker1 kubelet[2103]: W0124 18:06:52.737511    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:06:52 worker1 kubelet[2103]: E0124 18:06:52.737740    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:06:56 worker1 kubelet[2103]: I0124 18:06:56.311624    2103 container_manager_linux.go:434] [ContainerManager]: Disc
Jan 24 18:06:57 worker1 kubelet[2103]: W0124 18:06:57.750020    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:06:57 worker1 kubelet[2103]: E0124 18:06:57.750200    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:06:57 worker1 kubelet[2103]: I0124 18:06:57.904168    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:06:57 worker1 kubelet[2103]: E0124 18:06:57.904693    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:06:57 worker1 kubelet[2103]: E0124 18:06:57.904961    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:06:57 worker1 kubelet[2103]: E0124 18:06:57.905403    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:07:02 worker1 kubelet[2103]: W0124 18:07:02.759991    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:07:02 worker1 kubelet[2103]: E0124 18:07:02.760148    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:07:07 worker1 kubelet[2103]: W0124 18:07:07.776991    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:07:07 worker1 kubelet[2103]: E0124 18:07:07.778039    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:07:08 worker1 kubelet[2103]: I0124 18:07:08.904001    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:07:08 worker1 kubelet[2103]: E0124 18:07:08.904074    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:07:08 worker1 kubelet[2103]: E0124 18:07:08.904089    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:07:08 worker1 kubelet[2103]: E0124 18:07:08.904284    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:07:12 worker1 kubelet[2103]: W0124 18:07:12.787323    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:07:12 worker1 kubelet[2103]: E0124 18:07:12.787471    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:07:17 worker1 kubelet[2103]: W0124 18:07:17.797598    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:07:17 worker1 kubelet[2103]: E0124 18:07:17.797776    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:07:22 worker1 kubelet[2103]: W0124 18:07:22.808962    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:07:22 worker1 kubelet[2103]: E0124 18:07:22.809278    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:07:23 worker1 kubelet[2103]: I0124 18:07:23.906121    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:07:23 worker1 kubelet[2103]: E0124 18:07:23.906190    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:07:23 worker1 kubelet[2103]: E0124 18:07:23.906205    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:07:23 worker1 kubelet[2103]: E0124 18:07:23.906247    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:07:27 worker1 kubelet[2103]: W0124 18:07:27.818820    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:07:27 worker1 kubelet[2103]: E0124 18:07:27.819260    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:07:32 worker1 kubelet[2103]: W0124 18:07:32.830009    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:07:32 worker1 kubelet[2103]: E0124 18:07:32.830277    2103 kubelet.go:2192] Container runtime network not ready: Ne
lines 775-817Jan 24 18:07:34 worker1 kubelet[2103]: I0124 18:07:34.904158    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:07:34 worker1 kubelet[2103]: E0124 18:07:34.904239    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:07:34 worker1 kubelet[2103]: E0124 18:07:34.904254    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:07:34 worker1 kubelet[2103]: E0124 18:07:34.904437    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:07:37 worker1 kubelet[2103]: W0124 18:07:37.838704    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:07:37 worker1 kubelet[2103]: E0124 18:07:37.838855    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:07:42 worker1 kubelet[2103]: W0124 18:07:42.858686    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:07:42 worker1 kubelet[2103]: E0124 18:07:42.858893    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:07:47 worker1 kubelet[2103]: W0124 18:07:47.868082    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:07:47 worker1 kubelet[2103]: E0124 18:07:47.868229    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:07:47 worker1 kubelet[2103]: I0124 18:07:47.904237    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:07:47 worker1 kubelet[2103]: E0124 18:07:47.904324    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:07:47 worker1 kubelet[2103]: E0124 18:07:47.904339    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:07:47 worker1 kubelet[2103]: E0124 18:07:47.904401    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:07:52 worker1 kubelet[2103]: W0124 18:07:52.877054    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:07:52 worker1 kubelet[2103]: E0124 18:07:52.877263    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:07:57 worker1 kubelet[2103]: W0124 18:07:57.890385    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:07:57 worker1 kubelet[2103]: E0124 18:07:57.890537    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:08:01 worker1 kubelet[2103]: I0124 18:08:01.904115    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:08:01 worker1 kubelet[2103]: E0124 18:08:01.904646    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:08:01 worker1 kubelet[2103]: E0124 18:08:01.904901    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:08:01 worker1 kubelet[2103]: E0124 18:08:01.905179    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:08:02 worker1 kubelet[2103]: W0124 18:08:02.900118    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:08:02 worker1 kubelet[2103]: E0124 18:08:02.900284    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:08:07 worker1 kubelet[2103]: W0124 18:08:07.912473    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:08:07 worker1 kubelet[2103]: E0124 18:08:07.913193    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:08:12 worker1 kubelet[2103]: I0124 18:08:12.904434    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:08:12 worker1 kubelet[2103]: E0124 18:08:12.904522    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:08:12 worker1 kubelet[2103]: E0124 18:08:12.904538    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:08:12 worker1 kubelet[2103]: E0124 18:08:12.904581    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:08:12 worker1 kubelet[2103]: W0124 18:08:12.924067    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:08:12 worker1 kubelet[2103]: E0124 18:08:12.924289    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:08:17 worker1 kubelet[2103]: W0124 18:08:17.932751    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:08:17 worker1 kubelet[2103]: E0124 18:08:17.932958    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:08:22 worker1 kubelet[2103]: W0124 18:08:22.942050    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:08:22 worker1 kubelet[2103]: E0124 18:08:22.942199    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:08:24 worker1 kubelet[2103]: I0124 18:08:24.904000    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:08:24 worker1 kubelet[2103]: E0124 18:08:24.904073    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:08:24 worker1 kubelet[2103]: E0124 18:08:24.904088    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:08:24 worker1 kubelet[2103]: E0124 18:08:24.904138    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:08:27 worker1 kubelet[2103]: W0124 18:08:27.950651    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:08:27 worker1 kubelet[2103]: E0124 18:08:27.951035    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:08:32 worker1 kubelet[2103]: W0124 18:08:32.961474    2103 cni.go:203] Unable to update cni config: No networks fou
lines 818-860Jan 24 18:08:32 worker1 kubelet[2103]: E0124 18:08:32.961894    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:08:37 worker1 kubelet[2103]: I0124 18:08:37.904163    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:08:37 worker1 kubelet[2103]: E0124 18:08:37.904664    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:08:37 worker1 kubelet[2103]: E0124 18:08:37.904924    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:08:37 worker1 kubelet[2103]: E0124 18:08:37.905209    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:08:37 worker1 kubelet[2103]: W0124 18:08:37.989323    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:08:37 worker1 kubelet[2103]: E0124 18:08:37.989484    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:08:43 worker1 kubelet[2103]: W0124 18:08:42.999689    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:08:43 worker1 kubelet[2103]: E0124 18:08:43.000085    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:08:48 worker1 kubelet[2103]: W0124 18:08:48.010196    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:08:48 worker1 kubelet[2103]: E0124 18:08:48.010361    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:08:51 worker1 kubelet[2103]: I0124 18:08:51.904245    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:08:51 worker1 kubelet[2103]: E0124 18:08:51.904751    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:08:51 worker1 kubelet[2103]: E0124 18:08:51.905000    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:08:51 worker1 kubelet[2103]: E0124 18:08:51.905287    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:08:53 worker1 kubelet[2103]: W0124 18:08:53.031937    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:08:53 worker1 kubelet[2103]: E0124 18:08:53.032667    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:08:58 worker1 kubelet[2103]: W0124 18:08:58.044091    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:08:58 worker1 kubelet[2103]: E0124 18:08:58.045318    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:09:03 worker1 kubelet[2103]: W0124 18:09:03.057745    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:09:03 worker1 kubelet[2103]: E0124 18:09:03.057919    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:09:05 worker1 kubelet[2103]: I0124 18:09:05.904074    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:09:05 worker1 kubelet[2103]: E0124 18:09:05.904146    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:09:05 worker1 kubelet[2103]: E0124 18:09:05.904161    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:09:05 worker1 kubelet[2103]: E0124 18:09:05.904199    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:09:08 worker1 kubelet[2103]: W0124 18:09:08.065785    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:09:08 worker1 kubelet[2103]: E0124 18:09:08.066155    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:09:13 worker1 kubelet[2103]: W0124 18:09:13.075225    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:09:13 worker1 kubelet[2103]: E0124 18:09:13.075377    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:09:17 worker1 kubelet[2103]: I0124 18:09:17.904353    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:09:17 worker1 kubelet[2103]: E0124 18:09:17.905026    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:09:17 worker1 kubelet[2103]: E0124 18:09:17.905337    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:09:17 worker1 kubelet[2103]: E0124 18:09:17.905616    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:09:18 worker1 kubelet[2103]: W0124 18:09:18.087875    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:09:18 worker1 kubelet[2103]: E0124 18:09:18.088216    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:09:23 worker1 kubelet[2103]: W0124 18:09:23.102603    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:09:23 worker1 kubelet[2103]: E0124 18:09:23.102795    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:09:28 worker1 kubelet[2103]: W0124 18:09:28.112545    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:09:28 worker1 kubelet[2103]: E0124 18:09:28.112910    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:09:32 worker1 kubelet[2103]: I0124 18:09:32.903990    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:09:32 worker1 kubelet[2103]: E0124 18:09:32.904063    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:09:32 worker1 kubelet[2103]: E0124 18:09:32.904078    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:09:32 worker1 kubelet[2103]: E0124 18:09:32.904118    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
lines 861-903Jan 24 18:09:33 worker1 kubelet[2103]: W0124 18:09:33.121936    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:09:33 worker1 kubelet[2103]: E0124 18:09:33.122323    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:09:38 worker1 kubelet[2103]: W0124 18:09:38.139692    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:09:38 worker1 kubelet[2103]: E0124 18:09:38.141423    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:09:43 worker1 kubelet[2103]: W0124 18:09:43.159125    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:09:43 worker1 kubelet[2103]: E0124 18:09:43.159405    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:09:43 worker1 kubelet[2103]: I0124 18:09:43.904080    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:09:43 worker1 kubelet[2103]: E0124 18:09:43.904152    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:09:43 worker1 kubelet[2103]: E0124 18:09:43.904166    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:09:43 worker1 kubelet[2103]: E0124 18:09:43.904205    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:09:48 worker1 kubelet[2103]: W0124 18:09:48.169242    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:09:48 worker1 kubelet[2103]: E0124 18:09:48.169403    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:09:53 worker1 kubelet[2103]: W0124 18:09:53.180690    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:09:53 worker1 kubelet[2103]: E0124 18:09:53.180839    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:09:55 worker1 kubelet[2103]: I0124 18:09:55.904483    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:09:55 worker1 kubelet[2103]: E0124 18:09:55.905013    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:09:55 worker1 kubelet[2103]: E0124 18:09:55.905324    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:09:55 worker1 kubelet[2103]: E0124 18:09:55.905887    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:09:58 worker1 kubelet[2103]: W0124 18:09:58.190087    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:09:58 worker1 kubelet[2103]: E0124 18:09:58.190239    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:10:03 worker1 kubelet[2103]: W0124 18:10:03.204990    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:10:03 worker1 kubelet[2103]: E0124 18:10:03.205426    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:10:06 worker1 kubelet[2103]: I0124 18:10:06.904014    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:10:06 worker1 kubelet[2103]: E0124 18:10:06.904085    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:10:06 worker1 kubelet[2103]: E0124 18:10:06.904101    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:10:06 worker1 kubelet[2103]: E0124 18:10:06.904145    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:10:08 worker1 kubelet[2103]: W0124 18:10:08.218585    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:10:08 worker1 kubelet[2103]: E0124 18:10:08.218757    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:10:13 worker1 kubelet[2103]: W0124 18:10:13.230258    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:10:13 worker1 kubelet[2103]: E0124 18:10:13.230650    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:10:18 worker1 kubelet[2103]: W0124 18:10:18.239895    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:10:18 worker1 kubelet[2103]: E0124 18:10:18.240097    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:10:18 worker1 kubelet[2103]: I0124 18:10:18.904139    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:10:18 worker1 kubelet[2103]: E0124 18:10:18.904532    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:10:18 worker1 kubelet[2103]: E0124 18:10:18.904557    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:10:18 worker1 kubelet[2103]: E0124 18:10:18.904604    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:10:23 worker1 kubelet[2103]: W0124 18:10:23.249057    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:10:23 worker1 kubelet[2103]: E0124 18:10:23.249407    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:10:28 worker1 kubelet[2103]: W0124 18:10:28.262047    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:10:28 worker1 kubelet[2103]: E0124 18:10:28.262494    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:10:33 worker1 kubelet[2103]: W0124 18:10:33.275056    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:10:33 worker1 kubelet[2103]: E0124 18:10:33.275231    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:10:33 worker1 kubelet[2103]: I0124 18:10:33.904110    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
lines 904-946Jan 24 18:10:33 worker1 kubelet[2103]: E0124 18:10:33.904632    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:10:33 worker1 kubelet[2103]: E0124 18:10:33.904885    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:10:33 worker1 kubelet[2103]: E0124 18:10:33.905414    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:10:38 worker1 kubelet[2103]: W0124 18:10:38.285972    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:10:38 worker1 kubelet[2103]: E0124 18:10:38.286136    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:10:43 worker1 kubelet[2103]: W0124 18:10:43.301655    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:10:43 worker1 kubelet[2103]: E0124 18:10:43.301843    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:10:45 worker1 kubelet[2103]: I0124 18:10:45.904118    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:10:45 worker1 kubelet[2103]: E0124 18:10:45.904192    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:10:45 worker1 kubelet[2103]: E0124 18:10:45.904208    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:10:45 worker1 kubelet[2103]: E0124 18:10:45.904251    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:10:48 worker1 kubelet[2103]: W0124 18:10:48.311748    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:10:48 worker1 kubelet[2103]: E0124 18:10:48.311906    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:10:53 worker1 kubelet[2103]: W0124 18:10:53.333517    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:10:53 worker1 kubelet[2103]: E0124 18:10:53.333717    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:10:58 worker1 kubelet[2103]: W0124 18:10:58.348093    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:10:58 worker1 kubelet[2103]: E0124 18:10:58.348698    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:10:59 worker1 kubelet[2103]: I0124 18:10:59.906119    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:10:59 worker1 kubelet[2103]: E0124 18:10:59.906468    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:10:59 worker1 kubelet[2103]: E0124 18:10:59.906492    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:10:59 worker1 kubelet[2103]: E0124 18:10:59.906539    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:11:03 worker1 kubelet[2103]: W0124 18:11:03.358414    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:11:03 worker1 kubelet[2103]: E0124 18:11:03.358584    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:11:08 worker1 kubelet[2103]: W0124 18:11:08.371154    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:11:08 worker1 kubelet[2103]: E0124 18:11:08.371504    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:11:11 worker1 kubelet[2103]: I0124 18:11:11.905460    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:11:11 worker1 kubelet[2103]: E0124 18:11:11.905896    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:11:11 worker1 kubelet[2103]: E0124 18:11:11.906150    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:11:11 worker1 kubelet[2103]: E0124 18:11:11.906419    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:11:13 worker1 kubelet[2103]: W0124 18:11:13.384002    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:11:13 worker1 kubelet[2103]: E0124 18:11:13.384731    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:11:18 worker1 kubelet[2103]: W0124 18:11:18.394431    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:11:18 worker1 kubelet[2103]: E0124 18:11:18.394579    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:11:22 worker1 kubelet[2103]: I0124 18:11:22.904052    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:11:22 worker1 kubelet[2103]: E0124 18:11:22.904126    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:11:22 worker1 kubelet[2103]: E0124 18:11:22.904141    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:11:22 worker1 kubelet[2103]: E0124 18:11:22.904347    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:11:23 worker1 kubelet[2103]: W0124 18:11:23.402921    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:11:23 worker1 kubelet[2103]: E0124 18:11:23.403115    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:11:28 worker1 kubelet[2103]: W0124 18:11:28.416744    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:11:28 worker1 kubelet[2103]: E0124 18:11:28.416918    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:11:33 worker1 kubelet[2103]: W0124 18:11:33.427574    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:11:33 worker1 kubelet[2103]: E0124 18:11:33.427742    2103 kubelet.go:2192] Container runtime network not ready: Ne
lines 947-989Jan 24 18:11:35 worker1 kubelet[2103]: I0124 18:11:35.904225    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:11:35 worker1 kubelet[2103]: E0124 18:11:35.904733    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:11:35 worker1 kubelet[2103]: E0124 18:11:35.904974    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:11:35 worker1 kubelet[2103]: E0124 18:11:35.905334    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:11:38 worker1 kubelet[2103]: W0124 18:11:38.440007    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:11:38 worker1 kubelet[2103]: E0124 18:11:38.440320    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:11:43 worker1 kubelet[2103]: W0124 18:11:43.449198    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:11:43 worker1 kubelet[2103]: E0124 18:11:43.449337    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:11:48 worker1 kubelet[2103]: W0124 18:11:48.469389    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:11:48 worker1 kubelet[2103]: E0124 18:11:48.470002    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:11:48 worker1 kubelet[2103]: I0124 18:11:48.904026    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:11:48 worker1 kubelet[2103]: E0124 18:11:48.904630    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:11:48 worker1 kubelet[2103]: E0124 18:11:48.904656    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:11:48 worker1 kubelet[2103]: E0124 18:11:48.904703    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:11:53 worker1 kubelet[2103]: W0124 18:11:53.478739    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:11:53 worker1 kubelet[2103]: E0124 18:11:53.478908    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:11:56 worker1 kubelet[2103]: I0124 18:11:56.312237    2103 container_manager_linux.go:434] [ContainerManager]: Disc
Jan 24 18:11:58 worker1 kubelet[2103]: W0124 18:11:58.486612    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:11:58 worker1 kubelet[2103]: E0124 18:11:58.486996    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:12:01 worker1 kubelet[2103]: I0124 18:12:01.904108    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:12:01 worker1 kubelet[2103]: E0124 18:12:01.904616    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:12:01 worker1 kubelet[2103]: E0124 18:12:01.904863    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:12:01 worker1 kubelet[2103]: E0124 18:12:01.905164    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:12:03 worker1 kubelet[2103]: W0124 18:12:03.500663    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:12:03 worker1 kubelet[2103]: E0124 18:12:03.501256    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:12:08 worker1 kubelet[2103]: W0124 18:12:08.519252    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:12:08 worker1 kubelet[2103]: E0124 18:12:08.521033    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:12:12 worker1 kubelet[2103]: I0124 18:12:12.904041    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:12:12 worker1 kubelet[2103]: E0124 18:12:12.904119    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:12:12 worker1 kubelet[2103]: E0124 18:12:12.904135    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:12:12 worker1 kubelet[2103]: E0124 18:12:12.904173    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:12:13 worker1 kubelet[2103]: W0124 18:12:13.531363    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:12:13 worker1 kubelet[2103]: E0124 18:12:13.531525    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:12:18 worker1 kubelet[2103]: W0124 18:12:18.542377    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:12:18 worker1 kubelet[2103]: E0124 18:12:18.542892    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:12:23 worker1 kubelet[2103]: W0124 18:12:23.551209    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:12:23 worker1 kubelet[2103]: E0124 18:12:23.551621    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:12:26 worker1 kubelet[2103]: I0124 18:12:26.904098    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:12:26 worker1 kubelet[2103]: E0124 18:12:26.904184    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:12:26 worker1 kubelet[2103]: E0124 18:12:26.904199    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:12:26 worker1 kubelet[2103]: E0124 18:12:26.904245    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:12:28 worker1 kubelet[2103]: W0124 18:12:28.561169    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:12:28 worker1 kubelet[2103]: E0124 18:12:28.562447    2103 kubelet.go:2192] Container runtime network not ready: Ne
lines 990-1032Jan 24 18:12:33 worker1 kubelet[2103]: W0124 18:12:33.571994    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:12:33 worker1 kubelet[2103]: E0124 18:12:33.572144    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:12:38 worker1 kubelet[2103]: W0124 18:12:38.581956    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:12:38 worker1 kubelet[2103]: E0124 18:12:38.582110    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:12:41 worker1 kubelet[2103]: I0124 18:12:41.904157    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:12:41 worker1 kubelet[2103]: E0124 18:12:41.904657    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:12:41 worker1 kubelet[2103]: E0124 18:12:41.904900    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:12:41 worker1 kubelet[2103]: E0124 18:12:41.905311    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:12:43 worker1 kubelet[2103]: W0124 18:12:43.591407    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:12:43 worker1 kubelet[2103]: E0124 18:12:43.591688    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:12:48 worker1 kubelet[2103]: W0124 18:12:48.600181    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:12:48 worker1 kubelet[2103]: E0124 18:12:48.600341    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:12:53 worker1 kubelet[2103]: W0124 18:12:53.614762    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:12:53 worker1 kubelet[2103]: E0124 18:12:53.614925    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:12:53 worker1 kubelet[2103]: I0124 18:12:53.904349    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:12:53 worker1 kubelet[2103]: E0124 18:12:53.904436    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:12:53 worker1 kubelet[2103]: E0124 18:12:53.904452    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:12:53 worker1 kubelet[2103]: E0124 18:12:53.904519    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:12:58 worker1 kubelet[2103]: W0124 18:12:58.623806    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:12:58 worker1 kubelet[2103]: E0124 18:12:58.624235    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:13:03 worker1 kubelet[2103]: W0124 18:13:03.635002    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:13:03 worker1 kubelet[2103]: E0124 18:13:03.635463    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:13:08 worker1 kubelet[2103]: W0124 18:13:08.644313    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:13:08 worker1 kubelet[2103]: E0124 18:13:08.644483    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:13:08 worker1 kubelet[2103]: I0124 18:13:08.904076    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:13:08 worker1 kubelet[2103]: E0124 18:13:08.904155    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:13:08 worker1 kubelet[2103]: E0124 18:13:08.904172    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:13:08 worker1 kubelet[2103]: E0124 18:13:08.904225    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:13:13 worker1 kubelet[2103]: W0124 18:13:13.653794    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:13:13 worker1 kubelet[2103]: E0124 18:13:13.653997    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:13:18 worker1 kubelet[2103]: W0124 18:13:18.664299    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:13:18 worker1 kubelet[2103]: E0124 18:13:18.664442    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:13:21 worker1 kubelet[2103]: I0124 18:13:21.904317    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:13:21 worker1 kubelet[2103]: E0124 18:13:21.904718    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:13:21 worker1 kubelet[2103]: E0124 18:13:21.905010    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:13:21 worker1 kubelet[2103]: E0124 18:13:21.905327    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:13:23 worker1 kubelet[2103]: W0124 18:13:23.673368    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:13:23 worker1 kubelet[2103]: E0124 18:13:23.673537    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:13:28 worker1 kubelet[2103]: W0124 18:13:28.682864    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:13:28 worker1 kubelet[2103]: E0124 18:13:28.683184    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:13:33 worker1 kubelet[2103]: W0124 18:13:33.701390    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:13:33 worker1 kubelet[2103]: E0124 18:13:33.701826    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:13:36 worker1 kubelet[2103]: I0124 18:13:36.903989    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
lines 1033-1075Jan 24 18:13:36 worker1 kubelet[2103]: E0124 18:13:36.904062    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:13:36 worker1 kubelet[2103]: E0124 18:13:36.904077    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:13:36 worker1 kubelet[2103]: E0124 18:13:36.904130    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:13:38 worker1 kubelet[2103]: W0124 18:13:38.710456    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:13:38 worker1 kubelet[2103]: E0124 18:13:38.710731    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:13:43 worker1 kubelet[2103]: W0124 18:13:43.718796    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:13:43 worker1 kubelet[2103]: E0124 18:13:43.718957    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:13:47 worker1 kubelet[2103]: I0124 18:13:47.904134    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:13:47 worker1 kubelet[2103]: E0124 18:13:47.904557    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:13:47 worker1 kubelet[2103]: E0124 18:13:47.904795    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:13:47 worker1 kubelet[2103]: E0124 18:13:47.905031    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:13:48 worker1 kubelet[2103]: W0124 18:13:48.734965    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:13:48 worker1 kubelet[2103]: E0124 18:13:48.735305    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:13:53 worker1 kubelet[2103]: W0124 18:13:53.744957    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:13:53 worker1 kubelet[2103]: E0124 18:13:53.745235    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:13:58 worker1 kubelet[2103]: W0124 18:13:58.758891    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:13:58 worker1 kubelet[2103]: E0124 18:13:58.759056    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:14:01 worker1 kubelet[2103]: I0124 18:14:01.904240    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:14:01 worker1 kubelet[2103]: E0124 18:14:01.904315    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:14:01 worker1 kubelet[2103]: E0124 18:14:01.904329    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:14:01 worker1 kubelet[2103]: E0124 18:14:01.904369    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:14:03 worker1 kubelet[2103]: W0124 18:14:03.767782    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:14:03 worker1 kubelet[2103]: E0124 18:14:03.768160    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:14:08 worker1 kubelet[2103]: W0124 18:14:08.778914    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:14:08 worker1 kubelet[2103]: E0124 18:14:08.779059    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:14:13 worker1 kubelet[2103]: W0124 18:14:13.815355    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:14:13 worker1 kubelet[2103]: E0124 18:14:13.815519    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:14:13 worker1 kubelet[2103]: I0124 18:14:13.904118    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:14:13 worker1 kubelet[2103]: E0124 18:14:13.904192    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:14:13 worker1 kubelet[2103]: E0124 18:14:13.904208    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:14:13 worker1 kubelet[2103]: E0124 18:14:13.904250    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:14:18 worker1 kubelet[2103]: W0124 18:14:18.825691    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:14:18 worker1 kubelet[2103]: E0124 18:14:18.825841    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:14:23 worker1 kubelet[2103]: W0124 18:14:23.836322    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:14:23 worker1 kubelet[2103]: E0124 18:14:23.836838    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:14:25 worker1 kubelet[2103]: I0124 18:14:25.904126    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:14:25 worker1 kubelet[2103]: E0124 18:14:25.904201    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:14:25 worker1 kubelet[2103]: E0124 18:14:25.904217    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:14:25 worker1 kubelet[2103]: E0124 18:14:25.904256    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:14:28 worker1 kubelet[2103]: W0124 18:14:28.863742    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:14:28 worker1 kubelet[2103]: E0124 18:14:28.863897    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:14:33 worker1 kubelet[2103]: W0124 18:14:33.873253    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:14:33 worker1 kubelet[2103]: E0124 18:14:33.873408    2103 kubelet.go:2192] Container runtime network not ready: Ne
lines 1076-1118Jan 24 18:14:38 worker1 kubelet[2103]: W0124 18:14:38.882353    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:14:38 worker1 kubelet[2103]: E0124 18:14:38.882680    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:14:39 worker1 kubelet[2103]: I0124 18:14:39.904198    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:14:39 worker1 kubelet[2103]: E0124 18:14:39.904745    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:14:39 worker1 kubelet[2103]: E0124 18:14:39.905031    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:14:39 worker1 kubelet[2103]: E0124 18:14:39.905469    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:14:43 worker1 kubelet[2103]: W0124 18:14:43.891227    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:14:43 worker1 kubelet[2103]: E0124 18:14:43.891369    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:14:48 worker1 kubelet[2103]: W0124 18:14:48.900184    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:14:48 worker1 kubelet[2103]: E0124 18:14:48.900341    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:14:53 worker1 kubelet[2103]: W0124 18:14:53.920275    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:14:53 worker1 kubelet[2103]: E0124 18:14:53.921185    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:14:54 worker1 kubelet[2103]: I0124 18:14:54.904060    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:14:54 worker1 kubelet[2103]: E0124 18:14:54.904136    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:14:54 worker1 kubelet[2103]: E0124 18:14:54.904396    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:14:54 worker1 kubelet[2103]: E0124 18:14:54.904468    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:14:58 worker1 kubelet[2103]: W0124 18:14:58.933316    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:14:58 worker1 kubelet[2103]: E0124 18:14:58.933641    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:15:03 worker1 kubelet[2103]: W0124 18:15:03.953260    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:15:03 worker1 kubelet[2103]: E0124 18:15:03.953486    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:15:07 worker1 kubelet[2103]: I0124 18:15:07.904128    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:15:07 worker1 kubelet[2103]: E0124 18:15:07.904648    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:15:07 worker1 kubelet[2103]: E0124 18:15:07.904922    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:15:07 worker1 kubelet[2103]: E0124 18:15:07.905218    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:15:08 worker1 kubelet[2103]: W0124 18:15:08.971726    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:15:08 worker1 kubelet[2103]: E0124 18:15:08.971930    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:15:13 worker1 kubelet[2103]: W0124 18:15:13.982746    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:15:13 worker1 kubelet[2103]: E0124 18:15:13.983029    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:15:18 worker1 kubelet[2103]: I0124 18:15:18.904041    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:15:18 worker1 kubelet[2103]: E0124 18:15:18.904116    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:15:18 worker1 kubelet[2103]: E0124 18:15:18.904131    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:15:18 worker1 kubelet[2103]: E0124 18:15:18.904180    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:15:19 worker1 kubelet[2103]: W0124 18:15:19.003065    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:15:19 worker1 kubelet[2103]: E0124 18:15:19.003221    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:15:24 worker1 kubelet[2103]: W0124 18:15:24.012330    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:15:24 worker1 kubelet[2103]: E0124 18:15:24.012674    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:15:29 worker1 kubelet[2103]: W0124 18:15:29.022876    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:15:29 worker1 kubelet[2103]: E0124 18:15:29.023038    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:15:33 worker1 kubelet[2103]: I0124 18:15:33.904119    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:15:33 worker1 kubelet[2103]: E0124 18:15:33.904614    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:15:33 worker1 kubelet[2103]: E0124 18:15:33.904855    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:15:33 worker1 kubelet[2103]: E0124 18:15:33.905337    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:15:34 worker1 kubelet[2103]: W0124 18:15:34.034513    2103 cni.go:203] Unable to update cni config: No networks fou
lines 1119-1161Jan 24 18:15:34 worker1 kubelet[2103]: E0124 18:15:34.034683    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:15:39 worker1 kubelet[2103]: W0124 18:15:39.043450    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:15:39 worker1 kubelet[2103]: E0124 18:15:39.043611    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:15:44 worker1 kubelet[2103]: W0124 18:15:44.055349    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:15:44 worker1 kubelet[2103]: E0124 18:15:44.055807    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:15:47 worker1 kubelet[2103]: I0124 18:15:47.904399    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:15:47 worker1 kubelet[2103]: E0124 18:15:47.905041    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:15:47 worker1 kubelet[2103]: E0124 18:15:47.905360    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:15:47 worker1 kubelet[2103]: E0124 18:15:47.905650    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:15:49 worker1 kubelet[2103]: W0124 18:15:49.076239    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:15:49 worker1 kubelet[2103]: E0124 18:15:49.077245    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:15:54 worker1 kubelet[2103]: W0124 18:15:54.088448    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:15:54 worker1 kubelet[2103]: E0124 18:15:54.088615    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:15:59 worker1 kubelet[2103]: W0124 18:15:59.103692    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:15:59 worker1 kubelet[2103]: E0124 18:15:59.103863    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:16:02 worker1 kubelet[2103]: I0124 18:16:02.904193    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:16:02 worker1 kubelet[2103]: E0124 18:16:02.904270    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:16:02 worker1 kubelet[2103]: E0124 18:16:02.904284    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:16:02 worker1 kubelet[2103]: E0124 18:16:02.904470    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:16:04 worker1 kubelet[2103]: W0124 18:16:04.112207    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:16:04 worker1 kubelet[2103]: E0124 18:16:04.112368    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:16:09 worker1 kubelet[2103]: W0124 18:16:09.126836    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:16:09 worker1 kubelet[2103]: E0124 18:16:09.127072    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:16:14 worker1 kubelet[2103]: W0124 18:16:14.136662    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:16:14 worker1 kubelet[2103]: E0124 18:16:14.137114    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:16:15 worker1 kubelet[2103]: I0124 18:16:15.904098    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:16:15 worker1 kubelet[2103]: E0124 18:16:15.904503    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:16:15 worker1 kubelet[2103]: E0124 18:16:15.904758    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:16:15 worker1 kubelet[2103]: E0124 18:16:15.905011    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:16:19 worker1 kubelet[2103]: W0124 18:16:19.146914    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:16:19 worker1 kubelet[2103]: E0124 18:16:19.147292    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:16:24 worker1 kubelet[2103]: W0124 18:16:24.157102    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:16:24 worker1 kubelet[2103]: E0124 18:16:24.157364    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:16:27 worker1 kubelet[2103]: I0124 18:16:27.906282    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:16:27 worker1 kubelet[2103]: E0124 18:16:27.906349    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:16:27 worker1 kubelet[2103]: E0124 18:16:27.906363    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:16:27 worker1 kubelet[2103]: E0124 18:16:27.906402    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:16:29 worker1 kubelet[2103]: W0124 18:16:29.170758    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:16:29 worker1 kubelet[2103]: E0124 18:16:29.170953    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:16:34 worker1 kubelet[2103]: W0124 18:16:34.179365    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:16:34 worker1 kubelet[2103]: E0124 18:16:34.179700    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:16:39 worker1 kubelet[2103]: W0124 18:16:39.190758    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:16:39 worker1 kubelet[2103]: E0124 18:16:39.190907    2103 kubelet.go:2192] Container runtime network not ready: Ne
lines 1162-1204Jan 24 18:16:42 worker1 kubelet[2103]: I0124 18:16:42.904107    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:16:42 worker1 kubelet[2103]: E0124 18:16:42.904180    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:16:42 worker1 kubelet[2103]: E0124 18:16:42.904195    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:16:42 worker1 kubelet[2103]: E0124 18:16:42.904238    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:16:44 worker1 kubelet[2103]: W0124 18:16:44.199193    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:16:44 worker1 kubelet[2103]: E0124 18:16:44.199335    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:16:49 worker1 kubelet[2103]: W0124 18:16:49.217192    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:16:49 worker1 kubelet[2103]: E0124 18:16:49.217601    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:16:54 worker1 kubelet[2103]: W0124 18:16:54.226708    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:16:54 worker1 kubelet[2103]: E0124 18:16:54.226880    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:16:55 worker1 kubelet[2103]: I0124 18:16:55.906122    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:16:55 worker1 kubelet[2103]: E0124 18:16:55.906188    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:16:55 worker1 kubelet[2103]: E0124 18:16:55.906790    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:16:55 worker1 kubelet[2103]: E0124 18:16:55.907353    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:16:56 worker1 kubelet[2103]: I0124 18:16:56.312599    2103 container_manager_linux.go:434] [ContainerManager]: Disc
Jan 24 18:16:59 worker1 kubelet[2103]: W0124 18:16:59.249411    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:16:59 worker1 kubelet[2103]: E0124 18:16:59.249717    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:17:04 worker1 kubelet[2103]: W0124 18:17:04.259768    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:17:04 worker1 kubelet[2103]: E0124 18:17:04.260055    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:17:08 worker1 kubelet[2103]: I0124 18:17:08.903993    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:17:08 worker1 kubelet[2103]: E0124 18:17:08.904559    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:17:08 worker1 kubelet[2103]: E0124 18:17:08.904585    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:17:08 worker1 kubelet[2103]: E0124 18:17:08.904745    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:17:09 worker1 kubelet[2103]: W0124 18:17:09.273576    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:17:09 worker1 kubelet[2103]: E0124 18:17:09.273966    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:17:14 worker1 kubelet[2103]: W0124 18:17:14.289403    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:17:14 worker1 kubelet[2103]: E0124 18:17:14.289575    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:17:19 worker1 kubelet[2103]: W0124 18:17:19.298175    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:17:19 worker1 kubelet[2103]: E0124 18:17:19.298323    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:17:23 worker1 kubelet[2103]: I0124 18:17:23.904509    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:17:23 worker1 kubelet[2103]: E0124 18:17:23.904580    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:17:23 worker1 kubelet[2103]: E0124 18:17:23.904595    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:17:23 worker1 kubelet[2103]: E0124 18:17:23.904638    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:17:24 worker1 kubelet[2103]: W0124 18:17:24.316288    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:17:24 worker1 kubelet[2103]: E0124 18:17:24.316569    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:17:29 worker1 kubelet[2103]: W0124 18:17:29.339744    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:17:29 worker1 kubelet[2103]: E0124 18:17:29.339894    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:17:34 worker1 kubelet[2103]: W0124 18:17:34.348435    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:17:34 worker1 kubelet[2103]: E0124 18:17:34.348619    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:17:38 worker1 kubelet[2103]: I0124 18:17:38.904021    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:17:38 worker1 kubelet[2103]: E0124 18:17:38.904619    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:17:38 worker1 kubelet[2103]: E0124 18:17:38.904868    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:17:38 worker1 kubelet[2103]: E0124 18:17:38.905126    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
lines 1205-1247Jan 24 18:17:39 worker1 kubelet[2103]: W0124 18:17:39.361509    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:17:39 worker1 kubelet[2103]: E0124 18:17:39.361655    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:17:44 worker1 kubelet[2103]: W0124 18:17:44.372001    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:17:44 worker1 kubelet[2103]: E0124 18:17:44.372211    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:17:49 worker1 kubelet[2103]: W0124 18:17:49.381514    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:17:49 worker1 kubelet[2103]: E0124 18:17:49.381784    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:17:51 worker1 kubelet[2103]: I0124 18:17:51.904329    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:17:51 worker1 kubelet[2103]: E0124 18:17:51.904397    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:17:51 worker1 kubelet[2103]: E0124 18:17:51.904413    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:17:51 worker1 kubelet[2103]: E0124 18:17:51.904454    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:17:54 worker1 kubelet[2103]: W0124 18:17:54.393267    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:17:54 worker1 kubelet[2103]: E0124 18:17:54.394038    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:17:59 worker1 kubelet[2103]: W0124 18:17:59.406876    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:17:59 worker1 kubelet[2103]: E0124 18:17:59.407223    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:18:04 worker1 kubelet[2103]: W0124 18:18:04.416381    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:18:04 worker1 kubelet[2103]: E0124 18:18:04.416680    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:18:05 worker1 kubelet[2103]: I0124 18:18:05.904105    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:18:05 worker1 kubelet[2103]: E0124 18:18:05.904175    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:18:05 worker1 kubelet[2103]: E0124 18:18:05.904191    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:18:05 worker1 kubelet[2103]: E0124 18:18:05.904231    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:18:09 worker1 kubelet[2103]: W0124 18:18:09.441749    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:18:09 worker1 kubelet[2103]: E0124 18:18:09.441914    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:18:14 worker1 kubelet[2103]: W0124 18:18:14.465649    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:18:14 worker1 kubelet[2103]: E0124 18:18:14.465804    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:18:19 worker1 kubelet[2103]: W0124 18:18:19.486704    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:18:19 worker1 kubelet[2103]: E0124 18:18:19.486848    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:18:20 worker1 kubelet[2103]: I0124 18:18:20.904053    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:18:20 worker1 kubelet[2103]: E0124 18:18:20.904130    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:18:20 worker1 kubelet[2103]: E0124 18:18:20.904146    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:18:20 worker1 kubelet[2103]: E0124 18:18:20.904191    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:18:24 worker1 kubelet[2103]: W0124 18:18:24.496118    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:18:24 worker1 kubelet[2103]: E0124 18:18:24.496265    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:18:29 worker1 kubelet[2103]: W0124 18:18:29.506731    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:18:29 worker1 kubelet[2103]: E0124 18:18:29.506908    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:18:31 worker1 kubelet[2103]: I0124 18:18:31.904100    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:18:31 worker1 kubelet[2103]: E0124 18:18:31.904171    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:18:31 worker1 kubelet[2103]: E0124 18:18:31.904188    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:18:31 worker1 kubelet[2103]: E0124 18:18:31.904231    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:18:34 worker1 kubelet[2103]: W0124 18:18:34.516359    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:18:34 worker1 kubelet[2103]: E0124 18:18:34.516501    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:18:39 worker1 kubelet[2103]: W0124 18:18:39.539353    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:18:39 worker1 kubelet[2103]: E0124 18:18:39.539514    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:18:42 worker1 kubelet[2103]: I0124 18:18:42.904041    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
lines 1248-1290Jan 24 18:18:42 worker1 kubelet[2103]: E0124 18:18:42.904118    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:18:42 worker1 kubelet[2103]: E0124 18:18:42.904134    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:18:42 worker1 kubelet[2103]: E0124 18:18:42.904182    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:18:44 worker1 kubelet[2103]: W0124 18:18:44.551368    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:18:44 worker1 kubelet[2103]: E0124 18:18:44.552562    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:18:49 worker1 kubelet[2103]: W0124 18:18:49.563750    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:18:49 worker1 kubelet[2103]: E0124 18:18:49.563916    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:18:54 worker1 kubelet[2103]: W0124 18:18:54.573470    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:18:54 worker1 kubelet[2103]: E0124 18:18:54.573614    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:18:55 worker1 kubelet[2103]: I0124 18:18:55.904524    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:18:55 worker1 kubelet[2103]: E0124 18:18:55.904593    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:18:55 worker1 kubelet[2103]: E0124 18:18:55.904608    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:18:55 worker1 kubelet[2103]: E0124 18:18:55.904648    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:18:59 worker1 kubelet[2103]: W0124 18:18:59.596956    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:18:59 worker1 kubelet[2103]: E0124 18:18:59.597133    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:19:04 worker1 kubelet[2103]: W0124 18:19:04.623801    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:19:04 worker1 kubelet[2103]: E0124 18:19:04.624066    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:19:09 worker1 kubelet[2103]: W0124 18:19:09.632184    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:19:09 worker1 kubelet[2103]: E0124 18:19:09.632611    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:19:10 worker1 kubelet[2103]: I0124 18:19:10.904031    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:19:10 worker1 kubelet[2103]: E0124 18:19:10.904103    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:19:10 worker1 kubelet[2103]: E0124 18:19:10.904118    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:19:10 worker1 kubelet[2103]: E0124 18:19:10.904165    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:19:14 worker1 kubelet[2103]: W0124 18:19:14.641766    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:19:14 worker1 kubelet[2103]: E0124 18:19:14.642235    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:19:19 worker1 kubelet[2103]: W0124 18:19:19.668781    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:19:19 worker1 kubelet[2103]: E0124 18:19:19.671124    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:19:24 worker1 kubelet[2103]: W0124 18:19:24.680625    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:19:24 worker1 kubelet[2103]: E0124 18:19:24.680874    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:19:25 worker1 kubelet[2103]: I0124 18:19:25.904488    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:19:25 worker1 kubelet[2103]: E0124 18:19:25.904562    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:19:25 worker1 kubelet[2103]: E0124 18:19:25.904579    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:19:25 worker1 kubelet[2103]: E0124 18:19:25.904625    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:19:29 worker1 kubelet[2103]: W0124 18:19:29.690678    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:19:29 worker1 kubelet[2103]: E0124 18:19:29.690880    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:19:34 worker1 kubelet[2103]: W0124 18:19:34.701314    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:19:34 worker1 kubelet[2103]: E0124 18:19:34.701457    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:19:36 worker1 kubelet[2103]: I0124 18:19:36.904103    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:19:36 worker1 kubelet[2103]: E0124 18:19:36.904178    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:19:36 worker1 kubelet[2103]: E0124 18:19:36.904193    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:19:36 worker1 kubelet[2103]: E0124 18:19:36.904234    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:19:39 worker1 kubelet[2103]: W0124 18:19:39.711220    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:19:39 worker1 kubelet[2103]: E0124 18:19:39.711463    2103 kubelet.go:2192] Container runtime network not ready: Ne
lines 1291-1333Jan 24 18:19:44 worker1 kubelet[2103]: W0124 18:19:44.720832    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:19:44 worker1 kubelet[2103]: E0124 18:19:44.721271    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:19:48 worker1 kubelet[2103]: I0124 18:19:48.904532    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:19:48 worker1 kubelet[2103]: E0124 18:19:48.904612    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:19:48 worker1 kubelet[2103]: E0124 18:19:48.904627    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:19:48 worker1 kubelet[2103]: E0124 18:19:48.904679    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:19:49 worker1 kubelet[2103]: W0124 18:19:49.733892    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:19:49 worker1 kubelet[2103]: E0124 18:19:49.734068    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:19:54 worker1 kubelet[2103]: W0124 18:19:54.755927    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:19:54 worker1 kubelet[2103]: E0124 18:19:54.756314    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:19:59 worker1 kubelet[2103]: W0124 18:19:59.765713    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:19:59 worker1 kubelet[2103]: E0124 18:19:59.765861    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:20:01 worker1 kubelet[2103]: I0124 18:20:01.904122    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:20:01 worker1 kubelet[2103]: E0124 18:20:01.904194    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:20:01 worker1 kubelet[2103]: E0124 18:20:01.904210    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:20:01 worker1 kubelet[2103]: E0124 18:20:01.904254    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:20:04 worker1 kubelet[2103]: W0124 18:20:04.774749    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:20:04 worker1 kubelet[2103]: E0124 18:20:04.774917    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:20:09 worker1 kubelet[2103]: W0124 18:20:09.784765    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:20:09 worker1 kubelet[2103]: E0124 18:20:09.785540    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:20:14 worker1 kubelet[2103]: W0124 18:20:14.810619    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:20:14 worker1 kubelet[2103]: E0124 18:20:14.811076    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:20:15 worker1 kubelet[2103]: I0124 18:20:15.904169    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:20:15 worker1 kubelet[2103]: E0124 18:20:15.904241    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:20:15 worker1 kubelet[2103]: E0124 18:20:15.904258    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:20:15 worker1 kubelet[2103]: E0124 18:20:15.904301    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:20:19 worker1 kubelet[2103]: W0124 18:20:19.832077    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:20:19 worker1 kubelet[2103]: E0124 18:20:19.832219    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:20:24 worker1 kubelet[2103]: W0124 18:20:24.848757    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:20:24 worker1 kubelet[2103]: E0124 18:20:24.848902    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:20:29 worker1 kubelet[2103]: W0124 18:20:29.860244    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:20:29 worker1 kubelet[2103]: E0124 18:20:29.860393    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:20:29 worker1 kubelet[2103]: I0124 18:20:29.904310    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:20:29 worker1 kubelet[2103]: E0124 18:20:29.904383    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:20:29 worker1 kubelet[2103]: E0124 18:20:29.904401    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:20:29 worker1 kubelet[2103]: E0124 18:20:29.904446    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:20:34 worker1 kubelet[2103]: W0124 18:20:34.872084    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:20:34 worker1 kubelet[2103]: E0124 18:20:34.872515    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:20:39 worker1 kubelet[2103]: W0124 18:20:39.886913    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:20:39 worker1 kubelet[2103]: E0124 18:20:39.887299    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:20:42 worker1 kubelet[2103]: I0124 18:20:42.904046    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:20:42 worker1 kubelet[2103]: E0124 18:20:42.904120    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:20:42 worker1 kubelet[2103]: E0124 18:20:42.904136    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
lines 1334-1376Jan 24 18:20:42 worker1 kubelet[2103]: E0124 18:20:42.904182    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:20:44 worker1 kubelet[2103]: W0124 18:20:44.923549    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:20:44 worker1 kubelet[2103]: E0124 18:20:44.923897    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:20:49 worker1 kubelet[2103]: W0124 18:20:49.935973    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:20:49 worker1 kubelet[2103]: E0124 18:20:49.936651    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:20:54 worker1 kubelet[2103]: W0124 18:20:54.949965    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:20:54 worker1 kubelet[2103]: E0124 18:20:54.950113    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:20:57 worker1 kubelet[2103]: I0124 18:20:57.904123    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:20:57 worker1 kubelet[2103]: E0124 18:20:57.904194    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:20:57 worker1 kubelet[2103]: E0124 18:20:57.904210    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:20:57 worker1 kubelet[2103]: E0124 18:20:57.904253    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:20:59 worker1 kubelet[2103]: W0124 18:20:59.961743    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:20:59 worker1 kubelet[2103]: E0124 18:20:59.961956    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:21:04 worker1 kubelet[2103]: W0124 18:21:04.973788    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:21:04 worker1 kubelet[2103]: E0124 18:21:04.974077    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:21:09 worker1 kubelet[2103]: W0124 18:21:09.991034    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:21:09 worker1 kubelet[2103]: E0124 18:21:09.991591    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:21:11 worker1 kubelet[2103]: I0124 18:21:11.904371    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:21:11 worker1 kubelet[2103]: E0124 18:21:11.904454    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:21:11 worker1 kubelet[2103]: E0124 18:21:11.904470    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:21:11 worker1 kubelet[2103]: E0124 18:21:11.904523    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:21:14 worker1 kubelet[2103]: W0124 18:21:14.999730    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:21:15 worker1 kubelet[2103]: E0124 18:21:14.999907    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:21:20 worker1 kubelet[2103]: W0124 18:21:20.018737    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:21:20 worker1 kubelet[2103]: E0124 18:21:20.022854    2103 kubelet.go:2192] Container runtime network not ready: Ne
Jan 24 18:21:23 worker1 kubelet[2103]: I0124 18:21:23.906637    2103 kuberuntime_manager.go:397] No sandbox for pod "weave-ne
Jan 24 18:21:23 worker1 kubelet[2103]: E0124 18:21:23.906711    2103 kuberuntime_sandbox.go:41] GeneratePodSandboxConfig for 
Jan 24 18:21:23 worker1 kubelet[2103]: E0124 18:21:23.906728    2103 kuberuntime_manager.go:662] createPodSandbox for pod "we
Jan 24 18:21:23 worker1 kubelet[2103]: E0124 18:21:23.906772    2103 pod_workers.go:190] Error syncing pod adaf478c-3ecf-11ea
Jan 24 18:21:25 worker1 kubelet[2103]: W0124 18:21:25.043495    2103 cni.go:203] Unable to update cni config: No networks fou
Jan 24 18:21:25 worker1 kubelet[2103]: E0124 18:21:25.043683    2103 kubelet.go:2192] Container runtime network not ready: Ne
lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)lines 1365-1407/1407 (END)root@worker1:~# 
root@worker1:~# ls- l /ls -l /etc/cni/inet.d/
total 0
root@worker1:~# 
root@worker1:~# logout
Connection to worker1 closed.
root@master1:~/keys# ssh master3
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 18:11:53 2020 from 172.31.122.25
root@master3:~# ls -l /etc/cni/net.d/
total 4
-rw-r--r-- 1 root root 318 Jan 24 17:45 10-weave.conflist
root@master3:~# logout
Connection to master3 closed.
root@master1:~/keys# ls -l /etc/cni/net.d/
ls: cannot access '/etc/cni/net.d/': No such file or directory
root@master1:~/keys# ssh worker1
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 18:20:41 2020 from 172.31.122.25
root@worker1:~# cd ../pcaccd ^C
root@worker1:~# cd package/
root@worker1:~/package# ls -l
total 16712
-rw-r--r-- 1 root root 17109361 Mar 15  2019 cni-plugins-amd64-v0.7.5.tgz
root@worker1:~/package# 
root@worker1:~/package# 
root@worker1:~/package# tar -xzvf cni-plugins-amd64-v0.7.5.tgz --directory /opt/cni/bin/
root@worker1:~/package# ls -l /opt/cni/bin/
total 49008
-rwxr-xr-x 1 root root  4028260 Mar 15  2019 bridge
-rwxr-xr-x 1 root root 10232415 Mar 15  2019 dhcp
-rwxr-xr-x 1 root root  2856252 Mar 15  2019 flannel
-rwxr-xr-x 1 root root  3127363 Mar 15  2019 host-device
-rwxr-xr-x 1 root root  3036768 Mar 15  2019 host-local
-rwxr-xr-x 1 root root  3572685 Mar 15  2019 ipvlan
-rwxr-xr-x 1 root root  3084347 Mar 15  2019 loopback
-rwxr-xr-x 1 root root  3613497 Mar 15  2019 macvlan
-rwxr-xr-x 1 root root  3551125 Mar 15  2019 portmap
-rwxr-xr-x 1 root root  3993428 Mar 15  2019 ptp
-rwxr-xr-x 1 root root  2641877 Mar 15  2019 sample
-rwxr-xr-x 1 root root  2850029 Mar 15  2019 tuning
-rwxr-xr-x 1 root root  3568537 Mar 15  2019 vlan
root@worker1:~/package# 
root@worker1:~/package# ls -l /etc/cni/net.d/
total 0
root@worker1:~/package# logout
Connection to worker1 closed.
root@master1:~/keys# ssh worker1ls -l /etc/cni/net.d/ssh master3
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 18:21:46 2020 from 172.31.122.25
root@master3:~# ls -l /etc/cni/net.d/
total 4
-rw-r--r-- 1 root root 318 Jan 24 17:45 10-weave.conflist
root@master3:~# cat 1-scp /etc/cni/net.d/10-weave.conflist wo172.31.122.147:/etc/cni/net.d/
The authenticity of host '172.31.122.147 (172.31.122.147)' can't be established.
ECDSA key fingerprint is SHA256:b9+tXnEeU9AriIvlTLPXa0Pe9b4Nqfvmfxd2aVHaFuA.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '172.31.122.147' (ECDSA) to the list of known hosts.
10-weave.conflist                                                                            0%    0     0.0KB/s   --:-- ETA10-weave.conflist                                                                          100%  318     0.3KB/s   00:00    
root@master3:~# ssh 172.31.122.147
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 18:22:12 2020 from 172.31.122.25
root@worker1:~# logout
Connection to 172.31.122.147 closed.
root@master3:~# kubectl get nodes
NAME      STATUS   ROLES    AGE   VERSION
master3   Ready    <none>   39m   v1.13.0
worker1   Ready    <none>   58m   v1.13.0
root@master3:~# kubectl get nodespods -on kube-system
NAME              READY   STATUS              RESTARTS   AGE
weave-net-v2d29   0/2     ContainerCreating   0          51m
weave-net-xfnfn   2/2     Running             3          39m
root@master3:~# kubectl gteet et desciribe weave-net-v2d29 -n kube-system
error: the server doesn't have a resource type "describe"
root@master3:~# kubectl get describe weave-net-v2d29 -n kube-system pod
Name:               weave-net-v2d29
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               worker1/172.31.122.147
Start Time:         Fri, 24 Jan 2020 17:33:47 +0000
Labels:             controller-revision-hash=c5665dc8
                    name=weave-net
                    pod-template-generation=1
Annotations:        <none>
Status:             Pending
IP:                 172.31.122.147
Controlled By:      DaemonSet/weave-net
Containers:
  weave:
    Container ID:  
    Image:         docker.io/weaveworks/weave-kube:2.6.0
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      /home/weave/launch.sh
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:      10m
    Readiness:  http-get http://127.0.0.1:6784/status delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      HOSTNAME:   (v1:spec.nodeName)
    Mounts:
      /host/etc from cni-conf (rw)
      /host/home from cni-bin2 (rw)
      /host/opt from cni-bin (rw)
      /host/var/lib/dbus from dbus (rw)
      /lib/modules from lib-modules (rw)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from weave-net-token-nn9gx (ro)
      /weavedb from weavedb (rw)
  weave-npc:
    Container ID:   
    Image:          docker.io/weaveworks/weave-npc:2.6.0
    Image ID:       
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:  10m
    Environment:
      HOSTNAME:   (v1:spec.nodeName)
    Mounts:
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from weave-net-token-nn9gx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  weavedb:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/weave
    HostPathType:  
  cni-bin:
    Type:          HostPath (bare host directory volume)
    Path:          /opt
    HostPathType:  
  cni-bin2:
    Type:          HostPath (bare host directory volume)
    Path:          /home
    HostPathType:  
  cni-conf:
    Type:          HostPath (bare host directory volume)
    Path:          /etc
    HostPathType:  
  dbus:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/dbus
    HostPathType:  
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  weave-net-token-nn9gx:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  weave-net-token-nn9gx
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason                  Age                    From               Message
  ----     ------                  ----                   ----               -------
  Normal   Scheduled               51m                    default-scheduler  Successfully assigned kube-system/weave-net-v2d29 to worker1
  Warning  FailedCreatePodSandBox  41m (x48 over 51m)     kubelet, worker1   Failed create pod sandbox: open /run/systemd/resolve/resolv.conf: no such file or directory
  Warning  FailedCreatePodSandBox  35m (x18 over 39m)     kubelet, worker1   Failed create pod sandbox: open /run/systemd/resolve/resolv.conf: no such file or directory
  Warning  FailedCreatePodSandBox  3m25s (x116 over 28m)  kubelet, worker1   Failed create pod sandbox: open /run/systemd/resolve/resolv.conf: no such file or directory
root@master3:~# ssh woerker1
ssh: Could not resolve hostname worker1: Name or service not known
root@master3:~# logout
Connection to master3 closed.
root@master1:~/keys# ssh master3woker1worker1
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 18:24:34 2020 from 172.31.115.2
root@worker1:~# ls -l /run/systemd/resolve/resolv.conf
ls: cannot access '/run/systemd/resolve/resolv.conf': No such file or directory
root@worker1:~# systemctl status sytestem.d-
systemd-ask-password-console.path           systemd-networkd-resolvconf-update.service
systemd-ask-password-console.service        systemd-networkd.service
systemd-ask-password-plymouth.path          systemd-networkd.socket
systemd-ask-password-plymouth.service       systemd-networkd-wait-online.service
systemd-ask-password-wall.path              systemd-poweroff.service
systemd-ask-password-wall.service           systemd-quotacheck.service
systemd-binfmt.service                      systemd-random-seed.service
systemd-bootchart.service                   systemd-reboot.service
systemd-bus-proxyd.service                  systemd-remount-fs.service
systemd-bus-proxyd.socket                   systemd-resolved.service
systemd-exit.service                        systemd-rfkill.service
systemd-fsckd.service                       systemd-rfkill.socket
systemd-fsckd.socket                        systemd-suspend.service
systemd-fsck-root.service                   systemd-sysctl.service
systemd-halt.service                        systemd-sysusers.service
systemd-hibernate.service                   systemd-timedated.service
systemd-hostnamed.service                   systemd-timesyncd.service
systemd-hwdb-update.service                 systemd-tmpfiles-clean.service
systemd-hybrid-sleep.service                systemd-tmpfiles-clean.timer
systemd-initctl.service                     systemd-tmpfiles-setup-dev.service
systemd-initctl.socket                      systemd-tmpfiles-setup.service
systemd-journald-audit.socket               systemd-udevd-control.socket
systemd-journald-dev-log.socket             systemd-udevd-kernel.socket
systemd-journald.service                    systemd-udevd.service
systemd-journald.socket                     systemd-udev-settle.service
systemd-journal-flush.service               systemd-udev-trigger.service
systemd-kexec.service                       systemd-update-done.service
systemd-localed.service                     systemd-update-utmp-runlevel.service
systemd-logind.service                      systemd-update-utmp.service
systemd-machine-id-commit.service           systemd-user-sessions.service
systemd-modules-load.service                systemd-vconsole-setup.service
systemd-networkd-resolvconf-update.path     
root@worker1:~# systemctl status systemd-re
systemd-reboot.service      systemd-remount-fs.service  systemd-resolved.service    
root@worker1:~# systemctl status systemd-re
systemd-reboot.service      systemd-remount-fs.service  systemd-resolved.service    
root@worker1:~# systemctl status systemd-re
systemd-reboot.service      systemd-remount-fs.service  systemd-resolved.service    
root@worker1:~# systemctl status systemd-resolved.service 
 systemd-resolved.service - Network Name Resolution
   Loaded: loaded (/lib/systemd/system/systemd-resolved.service; disabled; vendor preset: enabled)
  Drop-In: /lib/systemd/system/systemd-resolved.service.d
           resolvconf.conf
   Active: inactive (dead)
     Docs: man:systemd-resolved.service(8)
root@worker1:~# systemctl status systemd-resolved.service start 
root@worker1:~# systemctl start systemd-resolved.service tus
 systemd-resolved.service - Network Name Resolution
   Loaded: loaded (/lib/systemd/system/systemd-resolved.service; disabled; vendor preset: enabled)
  Drop-In: /lib/systemd/system/systemd-resolved.service.d
           resolvconf.conf
   Active: active (running) since Fri 2020-01-24 18:26:18 UTC; 2s ago
     Docs: man:systemd-resolved.service(8)
 Main PID: 10644 (systemd-resolve)
   Status: "Processing requests..."
    Tasks: 1
   Memory: 984.0K
      CPU: 10ms
   CGroup: /system.slice/systemd-resolved.service
           10644 /lib/systemd/systemd-resolved

Jan 24 18:26:18 worker1 systemd[1]: Starting Network Name Resolution...
Jan 24 18:26:18 worker1 systemd-resolved[10644]: Positive Trust Anchors:
Jan 24 18:26:18 worker1 systemd-resolved[10644]: . IN DS    19036 8 2 49aac11d7b6f6446702e54a1607371607a1a41855200fd2ce1cdde3
Jan 24 18:26:18 worker1 systemd-resolved[10644]: Negative trust anchors: 10.in-addr.arpa 16.172.in-addr.arpa 17.172.in-addr.a
Jan 24 18:26:18 worker1 systemd-resolved[10644]: Using system hostname 'worker1'.
Jan 24 18:26:18 worker1 systemd-resolved[10644]: Switching to system DNS server 172.31.0.2.
Jan 24 18:26:18 worker1 systemd[1]: Started Network Name Resolution.
lines 1-21/21 (END)root@worker1:~# systemctl status systemd-resolved.service rttusls -l /run/systemd/resolve/resolv.confetc/cni/net.d/run/systemd/resolve/resolv.conf
-rw-r--r-- 1 systemd-resolve systemd-resolve 340 Jan 24 18:26 /run/systemd/resolve/resolv.conf
root@worker1:~# 
root@worker1:~# 
root@worker1:~# logout
Connection to worker1 closed.
root@master1:~/keys# ssh worker1master3
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 18:23:38 2020 from 172.31.122.25
root@master3:~# ssh worker1kubectl  describe pod weave-net-v2d29 -n kube-systemget describepods
NAME              READY   STATUS              RESTARTS   AGE
weave-net-v2d29   0/2     ContainerCreating   0          52m
weave-net-xfnfn   2/2     Running             3          41m
root@master3:~# kubectl get pods -n kube-systemssh worker1kubectl  describe pod weave-net-v2d29 -n kube-systemget describe
error: the server doesn't have a resource type "describe"
root@master3:~# kubectl get describe weave-net-v2d29 -n kube-systempodsssh worker1kubectl  describe pod weave-net-v2d29 -n kube-system
Name:               weave-net-v2d29
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               worker1/172.31.122.147
Start Time:         Fri, 24 Jan 2020 17:33:47 +0000
Labels:             controller-revision-hash=c5665dc8
                    name=weave-net
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 172.31.122.147
Controlled By:      DaemonSet/weave-net
Containers:
  weave:
    Container ID:  docker://13dc9e04642ea9a5d4d3a6ad3b0ac00f04709d5b6bf96e31575f7dab46b607b5
    Image:         docker.io/weaveworks/weave-kube:2.6.0
    Image ID:      docker-pullable://weaveworks/weave-kube@sha256:e4a3a5b9bf605a7ff5ad5473c7493d7e30cbd1ed14c9c2630a4e409b4dbfab1c
    Port:          <none>
    Host Port:     <none>
    Command:
      /home/weave/launch.sh
    State:          Running
      Started:      Fri, 24 Jan 2020 18:26:34 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      10m
    Readiness:  http-get http://127.0.0.1:6784/status delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      HOSTNAME:   (v1:spec.nodeName)
    Mounts:
      /host/etc from cni-conf (rw)
      /host/home from cni-bin2 (rw)
      /host/opt from cni-bin (rw)
      /host/var/lib/dbus from dbus (rw)
      /lib/modules from lib-modules (rw)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from weave-net-token-nn9gx (ro)
      /weavedb from weavedb (rw)
  weave-npc:
    Container ID:   docker://9978f5aa43758ea040dfb370eac0bdaf3e62a2bac835e58b1fb7fcd68153c3e4
    Image:          docker.io/weaveworks/weave-npc:2.6.0
    Image ID:       docker-pullable://weaveworks/weave-npc@sha256:985de9ff201677a85ce78703c515466fe45c9c73da6ee21821e89d902c21daf8
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Fri, 24 Jan 2020 18:26:37 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:  10m
    Environment:
      HOSTNAME:   (v1:spec.nodeName)
    Mounts:
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from weave-net-token-nn9gx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  weavedb:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/weave
    HostPathType:  
  cni-bin:
    Type:          HostPath (bare host directory volume)
    Path:          /opt
    HostPathType:  
  cni-bin2:
    Type:          HostPath (bare host directory volume)
    Path:          /home
    HostPathType:  
  cni-conf:
    Type:          HostPath (bare host directory volume)
    Path:          /etc
    HostPathType:  
  dbus:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/dbus
    HostPathType:  
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  weave-net-token-nn9gx:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  weave-net-token-nn9gx
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason                  Age                    From               Message
  ----     ------                  ----                   ----               -------
  Normal   Scheduled               52m                    default-scheduler  Successfully assigned kube-system/weave-net-v2d29 to worker1
  Warning  FailedCreatePodSandBox  42m (x48 over 52m)     kubelet, worker1   Failed create pod sandbox: open /run/systemd/resolve/resolv.conf: no such file or directory
  Warning  FailedCreatePodSandBox  36m (x18 over 40m)     kubelet, worker1   Failed create pod sandbox: open /run/systemd/resolve/resolv.conf: no such file or directory
  Warning  FailedCreatePodSandBox  4m43s (x116 over 29m)  kubelet, worker1   Failed create pod sandbox: open /run/systemd/resolve/resolv.conf: no such file or directory
root@master3:~# kubectl get pods -n kube-system
NAME              READY   STATUS    RESTARTS   AGE
weave-net-v2d29   2/2     Running   0          53m
weave-net-xfnfn   2/2     Running   3          41m
root@master3:~# 
root@master3:~# 
root@master3:~# kubectl get pods -n kube-system describe pod weave-net-v2d29
Name:               weave-net-v2d29
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               worker1/172.31.122.147
Start Time:         Fri, 24 Jan 2020 17:33:47 +0000
Labels:             controller-revision-hash=c5665dc8
                    name=weave-net
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 172.31.122.147
Controlled By:      DaemonSet/weave-net
Containers:
  weave:
    Container ID:  docker://13dc9e04642ea9a5d4d3a6ad3b0ac00f04709d5b6bf96e31575f7dab46b607b5
    Image:         docker.io/weaveworks/weave-kube:2.6.0
    Image ID:      docker-pullable://weaveworks/weave-kube@sha256:e4a3a5b9bf605a7ff5ad5473c7493d7e30cbd1ed14c9c2630a4e409b4dbfab1c
    Port:          <none>
    Host Port:     <none>
    Command:
      /home/weave/launch.sh
    State:          Running
      Started:      Fri, 24 Jan 2020 18:26:34 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      10m
    Readiness:  http-get http://127.0.0.1:6784/status delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      HOSTNAME:   (v1:spec.nodeName)
    Mounts:
      /host/etc from cni-conf (rw)
      /host/home from cni-bin2 (rw)
      /host/opt from cni-bin (rw)
      /host/var/lib/dbus from dbus (rw)
      /lib/modules from lib-modules (rw)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from weave-net-token-nn9gx (ro)
      /weavedb from weavedb (rw)
  weave-npc:
    Container ID:   docker://9978f5aa43758ea040dfb370eac0bdaf3e62a2bac835e58b1fb7fcd68153c3e4
    Image:          docker.io/weaveworks/weave-npc:2.6.0
    Image ID:       docker-pullable://weaveworks/weave-npc@sha256:985de9ff201677a85ce78703c515466fe45c9c73da6ee21821e89d902c21daf8
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Fri, 24 Jan 2020 18:26:37 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:  10m
    Environment:
      HOSTNAME:   (v1:spec.nodeName)
    Mounts:
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from weave-net-token-nn9gx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  weavedb:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/weave
    HostPathType:  
  cni-bin:
    Type:          HostPath (bare host directory volume)
    Path:          /opt
    HostPathType:  
  cni-bin2:
    Type:          HostPath (bare host directory volume)
    Path:          /home
    HostPathType:  
  cni-conf:
    Type:          HostPath (bare host directory volume)
    Path:          /etc
    HostPathType:  
  dbus:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/dbus
    HostPathType:  
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  weave-net-token-nn9gx:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  weave-net-token-nn9gx
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason                  Age                   From               Message
  ----     ------                  ----                  ----               -------
  Normal   Scheduled               53m                   default-scheduler  Successfully assigned kube-system/weave-net-v2d29 to worker1
  Warning  FailedCreatePodSandBox  43m (x48 over 53m)    kubelet, worker1   Failed create pod sandbox: open /run/systemd/resolve/resolv.conf: no such file or directory
  Warning  FailedCreatePodSandBox  36m (x18 over 40m)    kubelet, worker1   Failed create pod sandbox: open /run/systemd/resolve/resolv.conf: no such file or directory
  Warning  FailedCreatePodSandBox  5m1s (x116 over 30m)  kubelet, worker1   Failed create pod sandbox: open /run/systemd/resolve/resolv.conf: no such file or directory
root@master3:~# 
root@master3:~# 
root@master3:~# c
root@master3:~# 
root@master3:~# cd
root@master3:~# 
root@master3:~# logout
Connection to master3 closed.
root@master1:~/keys# 
root@master1:~/keys# 
root@master1:~/keys# ssh master3cxdcd
root@master1:~# 
root@master1:~# kubectl get pods
No resources found.
root@master1:~# kubectl get nodes
NAME      STATUS   ROLES    AGE   VERSION
master3   Ready    <none>   42m   v1.13.0
worker1   Ready    <none>   61m   v1.13.0
root@master1:~# kubectl get nodes -o wide
NAME      STATUS   ROLES    AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME
master3   Ready    <none>   42m   v1.13.0   172.31.115.2     <none>        Ubuntu 16.04.6 LTS   4.4.0-1100-aws   docker://19.3.5
worker1   Ready    <none>   61m   v1.13.0   172.31.122.147   <none>        Ubuntu 16.04.6 LTS   4.4.0-1100-aws   docker://19.3.5
root@master1:~# 
root@master1:~# 
root@master1:~# kubectl get nodes -o widepods ll-all-namespaces
Flag --show-all has been deprecated, will be removed in an upcoming release
No resources found.
root@master1:~# kubectl get pods -all-namespaces
Flag --show-all has been deprecated, will be removed in an upcoming release
No resources found.
root@master1:~# 
root@master1:~# 
root@master1:~# kubectl get pods -all-namespace-all-namespace
Error: unknown flag: --all-namespace


Examples:
  # List all pods in ps output format.
  kubectl get pods
  
  # List all pods in ps output format with more information (such as node name).
  kubectl get pods -o wide
  
  # List a single replication controller with specified NAME in ps output format.
  kubectl get replicationcontroller web
  
  # List deployments in JSON output format, in the "v1" version of the "apps" API group:
  kubectl get deployments.v1.apps -o json
  
  # List a single pod in JSON output format.
  kubectl get -o json pod web-pod-13je7
  
  # List a pod identified by type and name specified in "pod.yaml" in JSON output format.
  kubectl get -f pod.yaml -o json
  
  # Return only the phase value of the specified pod.
  kubectl get -o template pod/web-pod-13je7 --template={{.status.phase}}
  
  # List all replication controllers and services together in ps output format.
  kubectl get rc,services
  
  # List one or more resources by their type and names.
  kubectl get rc/web service/frontend pods/web-pod-13je7

Options:
      --all-namespaces=false: If present, list the requested object(s) across all namespaces. Namespace in current context is ignored even if specified with --namespace.
      --allow-missing-template-keys=true: If true, ignore any errors in templates when a field or map key is missing in the template. Only applies to golang and jsonpath output formats.
      --chunk-size=500: Return large lists in chunks rather than all at once. Pass 0 to disable. This flag is beta and may change in the future.
      --export=false: If true, use 'export' for the resources.  Exported resources are stripped of cluster-specific information.
      --field-selector='': Selector (field query) to filter on, supports '=', '==', and '!='.(e.g. --field-selector key1=value1,key2=value2). The server only supports a limited number of field queries per type.
  -f, --filename=[]: Filename, directory, or URL to files identifying the resource to get from a server.
      --ignore-not-found=false: If the requested object does not exist the command will return exit code 0.
      --include-uninitialized=false: If true, the kubectl command applies to uninitialized objects. If explicitly set to false, this flag overrides other flags that make the kubectl commands apply to uninitialized objects, e.g., "--all". Objects with empty metadata.initializers are regarded as initialized.
  -L, --label-columns=[]: Accepts a comma separated list of labels that are going to be presented as columns. Names are case-sensitive. You can also use multiple flag options like -L label1 -L label2...
      --no-headers=false: When using the default or custom-column output format, don't print headers (default print headers).
  -o, --output='': Output format. One of: json|yaml|wide|name|custom-columns=...|custom-columns-file=...|go-template=...|go-template-file=...|jsonpath=...|jsonpath-file=... See custom columns [http://kubernetes.io/docs/user-guide/kubectl-overview/#custom-columns], golang template [http://golang.org/pkg/text/template/#pkg-overview] and jsonpath template [http://kubernetes.io/docs/user-guide/jsonpath].
      --raw='': Raw URI to request from the server.  Uses the transport specified by the kubeconfig file.
  -R, --recursive=false: Process the directory used in -f, --filename recursively. Useful when you want to manage related manifests organized within the same directory.
  -l, --selector='': Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2)
      --server-print=true: If true, have the server return the appropriate table output. Supports extension APIs and CRDs.
      --show-kind=false: If present, list the resource type for the requested object(s).
      --show-labels=false: When printing, show all labels as the last column (default hide labels column)
      --sort-by='': If non-empty, sort list types using this field specification.  The field specification is expressed as a JSONPath expression (e.g. '{.metadata.name}'). The field in the API resource specified by this JSONPath expression must be an integer or a string.
      --template='': Template string or path to template file to use when -o=go-template, -o=go-template-file. The template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview].
  -w, --watch=false: After listing/getting the requested object, watch for changes. Uninitialized objects are excluded if no object name is provided.
      --watch-only=false: Watch for changes to the requested object(s), without listing/getting first.

Usage:
  kubectl get [(-o|--output=)json|yaml|wide|custom-columns=...|custom-columns-file=...|go-template=...|go-template-file=...|jsonpath=...|jsonpath-file=...] (TYPE[.VERSION][.GROUP] [NAME | -l label] | TYPE[.VERSION][.GROUP]/NAME ...) [flags] [options]

Use "kubectl options" for a list of global command-line options (applies to all commands).

unknown flag: --all-namespace
root@master1:~# kubectl get pods --all-namespaces
NAMESPACE     NAME              READY   STATUS    RESTARTS   AGE
kube-system   weave-net-v2d29   2/2     Running   0          54m
kube-system   weave-net-xfnfn   2/2     Running   3          42m
root@master1:~# 
root@master1:~# 
root@master1:~# 
root@master1:~# kubectl get pods -l k8s-app=kube-dns -n kube-system
No resources found.
root@master1:~# kubectl apply -f https://raw.githubusercontent.com/mmumshad/kubernetes-the-hard-way/master/deployments/coredn s.yaml
serviceaccount/coredns created
clusterrole.rbac.authorization.k8s.io/system:coredns created
clusterrolebinding.rbac.authorization.k8s.io/system:coredns created
configmap/coredns created
deployment.extensions/coredns created
service/kube-dns created
root@master1:~# kubectl apply -f https://raw.githubusercontent.com/mmumshad/kubernetes-the-hard-way/master/deployments/corednss.yamlroot@master1:~# get pods -l k8s-app=kube-dns -n kube-system

NAME                       READY   STATUS              RESTARTS   AGE
coredns-69cbb76ff8-lg7cn   1/1     Running             0          4s
coredns-69cbb76ff8-pmvk8   0/1     ContainerCreating   0          4s
root@master1:~# 
root@master1:~# 
root@master1:~# kubectl get pods -l k8s-app=kube-dns -n kube-system
NAME                       READY   STATUS    RESTARTS   AGE
coredns-69cbb76ff8-lg7cn   1/1     Running   0          18s
coredns-69cbb76ff8-pmvk8   1/1     Running   0          18s
root@master1:~# 
root@master1:~# 
root@master1:~# kubectl get pods -l k8s-app=kube-dns -n kube-system -o wide
NAME                       READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
coredns-69cbb76ff8-lg7cn   1/1     Running   0          23s   10.44.0.1   worker1   <none>           <none>
coredns-69cbb76ff8-pmvk8   1/1     Running   0          23s   10.32.0.2   master3   <none>           <none>
root@master1:~# 
root@master1:~# 
root@master1:~# kubectl run --generator=run-pod/v1  busybox --image=busybox:1.28 --command -- sleep 3600
pod/busybox created
root@master1:~# kubectl tgeget pods
NAME      READY   STATUS    RESTARTS   AGE
busybox   1/1     Running   0          15s
root@master1:~# kubectl get podssvc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   73m
root@master1:~# kubectl exec -ti busybox -- nslookup kubernetes
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes
Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local
root@master1:~# 
root@master1:~# kubectl create deployment nginx --image=nginx
deployment.apps/nginx created
root@master1:~# kuebctl get deploy
kuebctl: command not found
root@master1:~# kuecbectl get deploy
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
nginx   1/1     1            1           11s
root@master1:~# 
root@master1:~# kubectl get pods
NAME                   READY   STATUS    RESTARTS   AGE
busybox                1/1     Running   0          82s
nginx-5c7588df-jqqm8   1/1     Running   0          19s
root@master1:~# 
root@master1:~# kubectl expose deploy nginx --type=NodePort --port 80
service/nginx exposed
root@master1:~# kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP        75m
nginx        NodePort    10.96.0.57   <none>        80:30443/TCP   5s
root@master1:~# 
root@master1:~# 
root@master1:~# kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP        75m
nginx        NodePort    10.96.0.57   <none>        80:30443/TCP   11s
root@master1:~# kubetlcctl get svc nginx -o yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2020-01-24T18:31:58Z"
  labels:
    app: nginx
  name: nginx
  namespace: default
  resourceVersion: "6801"
  selfLink: /api/v1/namespaces/default/services/nginx
  uid: cde566d7-3ed7-11ea-b409-020f08380e46
spec:
  clusterIP: 10.96.0.57
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 30443
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
root@master1:~# kubectl get svc nginx -o yaml > exter
root@master1:~# kubectl delete svcdelete svc nginx
service "nginx" deleted
root@master1:~# kubectl delete svc nginxget svc nginx -o yaml > exter > exterdelete svc nginxvi esxxter 
  "exter" 25L, 504CapiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2020-01-24T18:31:58Z"
  labels:
    app: nginx
  name: nginx
  namespace: default
  resourceVersion: "6801"
  selfLink: /api/v1/namespaces/default/services/nginx
  uid: cde566d7-3ed7-11ea-b409-020f08380e46
spec:
  clusterIP: 10.96.0.57
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 30443
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            1,1All2345678910,112323lusterIP: 10.96.0.57usterIP: 10.96.0.57sterIP: 10.96.0.57terIP: 10.96.0.57erIP: 10.96.0.57rIP: 10.96.0.57IP: 10.96.0.57-- INSERT --13,3AlleIP: 10.96.0.574xIP: 10.96.0.575tIP: 10.96.0.576eIP: 10.96.0.577rIP: 10.96.0.578nIP: 10.96.0.579aIP: 10.96.0.5710lIP: 10.96.0.571234567892012345x654321019876534.210.54.2272813,27All:wq!"exter" 25L, 508C written
root@master1:~# kujbecjkkubectl create -f exter 
error: error validating "exter": error validating data: ValidationError(Service.spec): unknown field "externalIP" in io.k8s.api.core.v1.ServiceSpec; if you choose to ignore these errors, turn validation off with --validate=false
root@master1:~# vi exter
  "exter" 25L, 508CapiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2020-01-24T18:31:58Z"
  labels:
    app: nginx
  name: nginx
  namespace: default
  resourceVersion: "6801"
  selfLink: /api/v1/namespaces/default/services/nginx
  uid: cde566d7-3ed7-11ea-b409-020f08380e46
spec:
  externalIP: 34.210.54.227
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 30443
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            1,1All2345678910,1123456789201223:q!root@master1:~# kubectl expose deployment hello-world --type=LoadBalancer --name=my-servicevi exterkubectl create -f exter vikubectl delete svc nginxget svc nginx -o yaml > exterexpose deploy nginx --type=NodePort --port 80get podsdeploypodsexpose deploy nginx --type=NodePort --port 80LoadBalancer 
service/nginx exposed
root@master1:~# kubectl get svc
NAME         TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP      10.96.0.1    <none>        443/TCP        79m
nginx        LoadBalancer   10.96.0.31   <pending>     80:30194/TCP   4s
root@master1:~# 
root@master1:~# 
root@master1:~# kubectl get svc
NAME         TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP      10.96.0.1    <none>        443/TCP        79m
nginx        LoadBalancer   10.96.0.31   <pending>     80:30194/TCP   7s
root@master1:~# kubectl get svc
NAME         TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP      10.96.0.1    <none>        443/TCP        79m
nginx        LoadBalancer   10.96.0.31   <pending>     80:30194/TCP   12s
root@master1:~# kubectl get svc
NAME         TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP      10.96.0.1    <none>        443/TCP        79m
nginx        LoadBalancer   10.96.0.31   <pending>     80:30194/TCP   17s
root@master1:~# kubect getkubectl get endpoints
NAME         ENDPOINTS                                                  AGE
kubernetes   172.31.115.2:6443,172.31.122.25:6443,172.31.125.181:6443   80m
nginx        10.44.0.2:80                                               25s
root@master1:~# 
root@master1:~# curl http://10.44.0.2:80
^C
root@master1:~# kubectl get pods
NAME                   READY   STATUS    RESTARTS   AGE
busybox                1/1     Running   0          7m9s
nginx-5c7588df-jqqm8   1/1     Running   0          6m6s
root@master1:~# kubectl get pods -o wide
NAME                   READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
busybox                1/1     Running   0          7m15s   10.32.0.3   master3   <none>           <none>
nginx-5c7588df-jqqm8   1/1     Running   0          6m12s   10.44.0.2   worker1   <none>           <none>
root@master1:~# ssjh worker1
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 18:25:53 2020 from 172.31.122.25
root@worker1:~# curl http://10.44.0.2:80
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
root@worker1:~# logout
Connection to worker1 closed.
root@master1:~# curl http://10.44.0.2:80
^C
root@master1:~# kubectl get pods
NAME                   READY   STATUS    RESTARTS   AGE
busybox                1/1     Running   0          7m46s
nginx-5c7588df-jqqm8   1/1     Running   0          6m43s
root@master1:~# kubectl get exec -it nginx-5c7588df-jqqm8 bash
root@nginx-5c7588df-jqqm8:/# root@nginx-5c7588df-jqqm8:/# 
root@nginx-5c7588df-jqqm8:/# cat /etc/resolv.conf 
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local us-west-2.compute.internal
options ndots:5
root@nginx-5c7588df-jqqm8:/# exit
root@master1:~# cat /etc/resaolv
cat: /etc/resolv: No such file or directory
root@master1:~# cat /etc/resolv.conf 
# Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)
#     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN
nameserver 172.31.0.2
search us-west-2.compute.internal
root@master1:~# ssh master3
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 18:26:30 2020 from 172.31.122.25
root@master3:~# curl http://10.44.0.2:80
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
root@master3:~# kubectl get svc
NAME         TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP      10.96.0.1    <none>        443/TCP        82m
nginx        LoadBalancer   10.96.0.31   <pending>     80:30194/TCP   2m47s
root@master3:~# kubectl get svc
NAME         TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP      10.96.0.1    <none>        443/TCP        82m
nginx        LoadBalancer   10.96.0.31   <pending>     80:30194/TCP   3m6s
root@master3:~# 
root@master3:~# 
root@master3:~# kubectl get svc nginx -o yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2020-01-24T18:36:22Z"
  labels:
    app: nginx
  name: nginx
  namespace: default
  resourceVersion: "7204"
  selfLink: /api/v1/namespaces/default/services/nginx
  uid: 6b6fd281-3ed8-11ea-b409-020f08380e46
spec:
  clusterIP: 10.96.0.31
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 30194
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  sessionAffinity: None
  type: LoadBalancer
status:
  loadBalancer: {}
root@master3:~# kubectl get svc nginx -o yaml
NAME         TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP      10.96.0.1    <none>        443/TCP        83m
nginx        LoadBalancer   10.96.0.31   <pending>     80:30194/TCP   3m51s
root@master3:~# kubectl get svc nginx -o yamlcurl http://10.44.0.2:80dkubectl  describe pod weave-net-v2d29 -n kube-systemget pods describe pod weave-net-v2d29get describepodsssh worker1kubectl  describe pod weave-net-v2d29 -n kube-systemget describepodsnodesssh 172.31.122.147cp /etc/cni/net.d/10-weave.conflist 172.31.122.147:/etc/cni/net.d/ls -l /etc/cni/net.d/scp /etc/cni/net.d/10-weave.conflist 172.31.122.147:/etc/cni/net.d/sh 172.31.122.147kubectl get nodespods -n kube-systemdescribe weave-net-v2d29 describe podssh worker1kubectl get pods -n kube-systemdescribe weave-net-v2d29 describe podget pods describe pod weave-net-v2d29cdurl http://10.44.0.2:80kubectl get svc nginx -o yamlkubectl delete svc nginx
service "nginx" deleted
root@master3:~# ls -l necxx^C
root@master3:~# logout
Connection to master3 closed.
root@master1:~# ssh master3cat /etc/resolv.conf kubectl exec -it nginx-5c7588df-jqqm8 bashget podsexec -it nginx-5c7588df-jqqm8 bashget podscurl http://10.44.0.2:80ssh worker1kubectl get pods -o widecurl http://10.44.0.2:80kubectl get endpointssvcexpose deploy nginx --type=LoadBalancer --port 80vi exterkubectl expose deploy nginx --type=LoadBalancer --port 80^C
root@master1:~# v i exter 
  "exter" 25L, 508CapiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2020-01-24T18:31:58Z"
  labels:
    app: nginx
  name: nginx
  namespace: default
  resourceVersion: "6801"
  selfLink: /api/v1/namespaces/default/services/nginx
  uid: cde566d7-3ed7-11ea-b409-020f08380e46
spec:
  externalIP: 34.210.54.227
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 30443
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            1,1All2345678910,112233456789101234565432-- INSERT --13,12All: 34.210.54.22713,11All1 change; before #1  2 seconds agoP: 34.210.54.22713,12All-- INSERT --13,13Alls: 34.210.54.227432109 876543456789101234567813,17All:wq!"exter" 25L, 509C written
root@master1:~# vi exter ssh master3vi exter ssh master3cat /etc/resolv.conf kubectl exec -it nginx-5c7588df-jqqm8 bashget podscurl http://10.44.0.2:80ssh worker1kubectl get pods -o widecurl http://10.44.0.2:80kubectl get endpointssvcexpose deploy nginx --type=LoadBalancer --port 80vi exterkubectl create -f exter vikubectl create -f
error: error validating "exter": error validating data: ValidationError(Service.spec.externalIPs): invalid type for io.k8s.api.core.v1.ServiceSpec.externalIPs: got "string", expected "array"; if you choose to ignore these errors, turn validation off with --validate=false
root@master1:~# kubectl create -f exter vi
  "exter" 25L, 509CapiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2020-01-24T18:31:58Z"
  labels:
    app: nginx
  name: nginx
  namespace: default
  resourceVersion: "6801"
  selfLink: /api/v1/namespaces/default/services/nginx
  uid: cde566d7-3ed7-11ea-b409-020f08380e46
spec:
  externalIPs: 34.210.54.227
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 30443
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            1,1All2345678910,112234536789101234567876-- INSERT --13,16All34.210.54.22714,1All 34.210.54.2272 34.210.54.2273-34.210.54.2274 34.210.54.227514,4All:wq!"exter" 26L, 514C written
root@master1:~# vi exter kubectl create -f
service/nginx created
root@master1:~# kubectl get svc
NAME         TYPE        CLUSTER-IP    EXTERNAL-IP     PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1     <none>          443/TCP        86m
nginx        NodePort    10.96.0.151   34.210.54.227   80:30443/TCP   4s
root@master1:~# 
root@master1:~# 
root@master1:~# kubectl get svccreate -f exter delete
service "nginx" deleted
root@master1:~# cat /etc/hosts
127.0.0.1 localhost

# The following lines are desirable for IPv6 capable hosts
::1 ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
ff02::3 ip6-allhosts
# Cloud Server Hostname mapping
172.31.122.25   rameshhms1c.mylabserver.com
172.31.122.25master1
172.31.125.181master2
172.31.115.2master3
172.31.122.147worker1
172.31.123.142loadbalancer
root@master1:~# cat /etc/hostskubectl delete -f exter get svccreate -f exter vikubectl create -fvikubectl create -fvi
  "exter" 26L, 514CapiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2020-01-24T18:31:58Z"
  labels:
    app: nginx
  name: nginx
  namespace: default
  resourceVersion: "6801"
  selfLink: /api/v1/namespaces/default/services/nginx
  uid: cde566d7-3ed7-11ea-b409-020f08380e46
spec:
  externalIPs:
  - 34.210.54.227
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 30443
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            1,1All2345678910,11234567892019876524345-- INSERT --14,5All.210.54.22714,4All5-- INSERT --14,5All210.54.22714,4All5-- INSERT --14,5All.54.227614,5All-- INSERT --14,5All54.227614,5All4-- INSERT --14,4All54.22714,3All44.227.2272272773-- INSERT --14,4All534.222.41.861714,16All:wq!"exter" 26L, 513C written
root@master1:~# vi exter cat /etc/hostskubectl delete -f exter 
Error from server (NotFound): error when deleting "exter": services "nginx" not found
root@master1:~# cat exter 
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2020-01-24T18:31:58Z"
  labels:
    app: nginx
  name: nginx
  namespace: default
  resourceVersion: "6801"
  selfLink: /api/v1/namespaces/default/services/nginx
  uid: cde566d7-3ed7-11ea-b409-020f08380e46
spec:
  externalIPs: 
  - 34.222.41.86
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 30443
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
root@master1:~# kubect l get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   87m
root@master1:~# kubectl get svc[popods
NAME                   READY   STATUS    RESTARTS   AGE
busybox                1/1     Running   0          14m
nginx-5c7588df-jqqm8   1/1     Running   0          13m
root@master1:~# kubectl get podsdeployu
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
nginx   1/1     1            1           13m
root@master1:~# kubectl get deploypodssvccat exter kubectl delete -fvi extercat /etc/hostskubectl delete -f exter get svccreate -f exter 
service/nginx created
root@master1:~# kubectl create -f exter get deploypodssvc
NAME         TYPE        CLUSTER-IP    EXTERNAL-IP    PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1     <none>         443/TCP        88m
nginx        NodePort    10.96.0.254   34.222.41.86   80:30443/TCP   4s
root@master1:~# curl http://34.222.41.86:80
curl: (7) Failed to connect to 34.222.41.86 port 80: Connection refused
root@master1:~# curl http://34.222.41.86:80:80:80:80:80:80:80:80:80:80:80:80:801:800:80.:809:806:80.:800:80.:802:805:804:80
^C
root@master1:~# ssh master1
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 17:58:10 2020 from 49.37.206.100
root@master1:~# logout
Connection to master1 closed.
root@master1:~# ssh master13
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 18:38:54 2020 from 172.31.122.25
root@master3:~# curl http://10.96.0.254:80
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
root@master3:~# curl http://10.96.0.254:80kubectl delete svc nginxcurl http://10.96.0.254:80curl http://10.96.0.254:80:80:80:80:80:80:80:80:80:80:80:803:804:80.:802:802:802:80.:804:801:80.:808:806:80
curl: (7) Failed to connect to 34.222.41.86 port 80: Connection refused
root@master3:~# curl http://34.222.41.86:8030443/TCP
^C
root@master3:~# 
root@master3:~# 
root@master3:~# nslookup 34.222.41.86
Server:172.31.0.2
Address:172.31.0.2#53

Non-authoritative answer:
86.41.222.34.in-addr.arpaname = ec2-34-222-41-86.us-west-2.compute.amazonaws.com.

Authoritative answers can be found from:

root@master3:~# logout
Connection to master3 closed.
root@master1:~# 
root@master1:~# vi exter 
  "exter" 26L, 513CapiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2020-01-24T18:31:58Z"
  labels:
    app: nginx
  name: nginx
  namespace: default
  resourceVersion: "6801"
  selfLink: /api/v1/namespaces/default/services/nginx
  uid: cde566d7-3ed7-11ea-b409-020f08380e46
spec:
  externalIPs:
  - 34.222.41.86
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 30443
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            1,1All2345678910,112345678987234567891012345678987654-- INSERT --17,15All0617,15All-- INSERT --17,15All80617,15All:wq!"exter" 26L, 510C written
root@master1:~# kuebctl delet e e-f exter 
kuebctl: command not found
root@master1:~# kubectl getdelete -rf exter 
service "nginx" deleted
root@master1:~# kubectl create -f exter 
The Service "nginx" is invalid: spec.ports[0].nodePort: Invalid value: 80: provided port is not in the valid range. The range of valid ports is 30000-32767
root@master1:~# 
root@master1:~# 
root@master1:~# kubectl sget svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   91m
root@master1:~# 
root@master1:~# 
root@master1:~# poping 10.96.0.1
PING 10.96.0.1 (10.96.0.1) 56(84) bytes of data.
^C
--- 10.96.0.1 ping statistics ---
2 packets transmitted, 0 received, 100% packet loss, time 1007ms

root@master1:~# ssh master23
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 18:45:51 2020 from 172.31.122.25
root@master3:~# ping 10.96.0.1
PING 10.96.0.1 (10.96.0.1) 56(84) bytes of data.
^C
--- 10.96.0.1 ping statistics ---
2 packets transmitted, 0 received, 100% packet loss, time 999ms

root@master3:~# nslookpup 10.96.0.1
Server:172.31.0.2
Address:172.31.0.2#53

** server can't find 1.0.96.10.in-addr.arpa: NXDOMAIN

root@master3:~# kubectl get pods -oi wide
NAME                   READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
busybox                1/1     Running   0          18m   10.32.0.3   master3   <none>           <none>
nginx-5c7588df-jqqm8   1/1     Running   0          17m   10.44.0.2   worker1   <none>           <none>
root@master3:~# pin g g 10.32.0.3
PING 10.32.0.3 (10.32.0.3) 56(84) bytes of data.
64 bytes from 10.32.0.3: icmp_seq=1 ttl=64 time=0.415 ms
64 bytes from 10.32.0.3: icmp_seq=2 ttl=64 time=0.059 ms
^C
--- 10.32.0.3 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 999ms
rtt min/avg/max/mdev = 0.059/0.237/0.415/0.178 ms
root@master3:~# ping 10.44.0.2
PING 10.44.0.2 (10.44.0.2) 56(84) bytes of data.
64 bytes from 10.44.0.2: icmp_seq=1 ttl=64 time=0.298 ms
^C
--- 10.44.0.2 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.298/0.298/0.298/0.000 ms
root@master3:~# logout
Connection to master3 closed.
root@master1:~# ping 10.32.0.3
PING 10.32.0.3 (10.32.0.3) 56(84) bytes of data.
^C
--- 10.32.0.3 ping statistics ---
2 packets transmitted, 0 received, 100% packet loss, time 999ms

root@master1:~# ping 10.44.0.2
PING 10.44.0.2 (10.44.0.2) 56(84) bytes of data.
^C
--- 10.44.0.2 ping statistics ---
2 packets transmitted, 0 received, 100% packet loss, time 1007ms

root@master1:~# 
root@master1:~# 
root@master1:~# Using username "root".
Authenticating with public key "rsa-key-20200124" from agent
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 18:45:45 2020 from 172.31.122.25
root@master1:~# ip route
default via 172.31.112.1 dev ens5 
169.254.0.0/16 dev ens5  scope link  metric 1000 
172.31.112.0/20 dev ens5  proto kernel  scope link  src 172.31.122.25 
root@master1:~# 
root@master1:~# 
root@master1:~# ssh master3
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 18:48:46 2020 from 172.31.122.25
root@master3:~# ip route
default via 172.31.112.1 dev ens5 
10.32.0.0/12 dev weave  proto kernel  scope link  src 10.32.0.1 
169.254.0.0/16 dev ens5  scope link  metric 1000 
172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1 linkdown 
172.31.112.0/20 dev ens5  proto kernel  scope link  src 172.31.115.2 
root@master3:~# logout
Connection to master3 closed.
root@master1:~# 
root@master1:~# 
root@master1:~# ssmaster2
ssmaster2: command not found
root@master1:~# ssh master2
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 17:15:35 2020 from 172.31.122.25
root@master2:~# apt-get update
0% [Working]            Hit:1 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial InRelease
0% [Waiting for headers] [Connecting to security.ubuntu.com (2001:67c:1560:8001::14)]                                                                                     Get:2 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]
                                                                                     Get:3 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]
                                                                                     0% [Connecting to security.ubuntu.com (2001:67c:1560:8001::14)]                                                               0% [Waiting for headers]0% [2 InRelease gpgv 109 kB] [Waiting for headers]                                                  0% [Waiting for headers]0% [3 InRelease gpgv 107 kB] [Waiting for headers] [Waiting for headers]                                                                        Get:4 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 DEP-11 Metadata [322 kB]
                                                                        0% [3 InRelease gpgv 107 kB] [Waiting for headers]                                                  Get:5 http://security.ubuntu.com/ubuntu xenial-security InRelease [109 kB]
0% [3 InRelease gpgv 107 kB] [5 InRelease 5,401 B/109 kB 5%]                                                            Get:6 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-updates/main DEP-11 64x64 Icons [241 kB]
0% [3 InRelease gpgv 107 kB] [6 icons-64x64 50.4 kB/241 kB 21%] [5 InRelease 5,401 B/109 kB 5%]                                                                                               0% [3 InRelease gpgv 107 kB] [5 InRelease 8,257 B/109 kB 8%]0% [4 Components-amd64 store 0 B] [3 InRelease gpgv 107 kB] [5 InRelease 8,257 B/109 kB 8%]                                                                                           Get:7 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 DEP-11 Metadata [274 kB]
0% [4 Components-amd64 store 0 B] [3 InRelease gpgv 107 kB] [7 Components-amd64 0 B/274 kB 0%] [5 InRelease 8,257 B/109 kB 8                                                                                                                            0% [4 Components-amd64 store 0 B] [3 InRelease gpgv 107 kB] [5 InRelease 11.1 kB/109 kB 10%]                                                                                            Get:8 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-updates/universe DEP-11 64x64 Icons [411 kB]
0% [4 Components-amd64 store 0 B] [3 InRelease gpgv 107 kB] [8 icons-64x64 0 B/411 kB 0%] [5 InRelease 11.1 kB/109 kB 10%]                                                                                                                          0% [4 Components-amd64 store 0 B] [3 InRelease gpgv 107 kB] [5 InRelease 14.0 kB/109 kB 13%]                                                                                            Get:9 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-updates/multiverse amd64 DEP-11 Metadata [5,968 B]
0% [4 Components-amd64 store 0 B] [3 InRelease gpgv 107 kB] [9 Components-amd64 0 B/5,968 B 0%] [5 InRelease 14.0 kB/109 kB                                                                                                                             0% [4 Components-amd64 store 0 B] [3 InRelease gpgv 107 kB] [5 InRelease 14.0 kB/109 kB 13%]                                                                                            Get:10 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-updates/multiverse DEP-11 64x64 Icons [14.3 kB]
0% [4 Components-amd64 store 0 B] [3 InRelease gpgv 107 kB] [10 icons-64x64 0 B/14.3 kB 0%] [5 InRelease 14.0 kB/109 kB 13%]                                                                                                                            0% [4 Components-amd64 store 0 B] [3 InRelease gpgv 107 kB] [5 InRelease 14.0 kB/109 kB 13%]                                                                                            0% [4 Components-amd64 store 0 B] [5 InRelease 14.0 kB/109 kB 13%]                                                                  Get:11 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-backports/main amd64 DEP-11 Metadata [3,324 B]
0% [4 Components-amd64 store 0 B] [5 InRelease 14.0 kB/109 kB 13%]                                                                  Get:12 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-backports/universe amd64 DEP-11 Metadata [5,324 B]
0% [4 Components-amd64 store 0 B] [5 InRelease 14.0 kB/109 kB 13%]                                                                  0% [5 InRelease 15.4 kB/109 kB 14%]0% [6 icons-64x64 store 0 B] [5 InRelease 15.4 kB/109 kB 14%]                                                             0% [5 InRelease 15.4 kB/109 kB 14%]0% [7 Components-amd64 store 0 B] [5 InRelease 15.4 kB/109 kB 14%]                                                                  0% [5 InRelease 34.0 kB/109 kB 31%]0% [8 icons-64x64 store 0 B] [5 InRelease 34.0 kB/109 kB 31%]                                                             0% [5 InRelease 34.0 kB/109 kB 31%]0% [9 Components-amd64 store 0 B] [5 InRelease 34.0 kB/109 kB 31%]                                                                  0% [5 InRelease 36.8 kB/109 kB 34%]0% [10 icons-64x64 store 0 B] [5 InRelease 36.8 kB/109 kB 34%]                                                              0% [5 InRelease 36.8 kB/109 kB 34%]0% [11 Components-amd64 store 0 B] [5 InRelease 36.8 kB/109 kB 34%]                                                                   0% [5 InRelease 36.8 kB/109 kB 34%]0% [5 InRelease 36.8 kB/109 kB 34%]                                   0% [Working]0% [5 InRelease gpgv 109 kB]                            81% [Working]             Get:13 http://security.ubuntu.com/ubuntu xenial-security/main amd64 DEP-11 Metadata [74.8 kB]
81% [13 Components-amd64 2,648 B/74.8 kB 4%]                                            83% [Working]83% [13 Components-amd64 store 0 B]                                   84% [Waiting for headers]                         Get:14 http://security.ubuntu.com/ubuntu xenial-security/main DEP-11 64x64 Icons [83.8 kB]
84% [14 icons-64x64 2,648 B/83.8 kB 3%]                                       87% [Working]             Get:15 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 DEP-11 Metadata [124 kB]
87% [15 Components-amd64 32 B/124 kB 0%]87% [14 icons-64x64 store 0 B] [15 Components-amd64 32 B/124 kB 0%]                                                                   87% [15 Components-amd64 32 B/124 kB 0%]                                        92% [Working]             Get:16 http://security.ubuntu.com/ubuntu xenial-security/universe DEP-11 64x64 Icons [194 kB]
92% [16 icons-64x64 67 B/194 kB 0%]92% [15 Components-amd64 store 0 B] [16 icons-64x64 67 B/194 kB 0%]                                                                   93% [16 icons-64x64 35.8 kB/194 kB 18%]                                       100% [Working]              Get:17 http://security.ubuntu.com/ubuntu xenial-security/multiverse amd64 DEP-11 Metadata [2,464 B]
100% [17 Components-amd64 1,919 B/2,464 B 78%]100% [16 icons-64x64 store 0 B] [17 Components-amd64 1,919 B/2,464 B 78%]                                                                         100% [16 icons-64x64 store 0 B]                               100% [Working]100% [17 Components-amd64 store 0 B]                                    100% [Working]              Fetched 2,081 kB in 1s (1,270 kB/s)

Reading package lists... 0%Reading package lists... 0%Reading package lists... 1%Reading package lists... 6%Reading package lists... 6%Reading package lists... 9%Reading package lists... 9%Reading package lists... 9%Reading package lists... 9%Reading package lists... 9%Reading package lists... 9%Reading package lists... 44%Reading package lists... 44%Reading package lists... 64%Reading package lists... 64%Reading package lists... 64%Reading package lists... 64%Reading package lists... 65%Reading package lists... 65%Reading package lists... 68%Reading package lists... 71%Reading package lists... 71%Reading package lists... 77%Reading package lists... 77%Reading package lists... 77%Reading package lists... 77%Reading package lists... 77%Reading package lists... 77%Reading package lists... 81%Reading package lists... 81%Reading package lists... 83%Reading package lists... 83%Reading package lists... 84%Reading package lists... 84%Reading package lists... 84%Reading package lists... 84%Reading package lists... 84%Reading package lists... 84%Reading package lists... 84%Reading package lists... 84%Reading package lists... 84%Reading package lists... 84%Reading package lists... 84%Reading package lists... 84%Reading package lists... 88%Reading package lists... 88%Reading package lists... 93%Reading package lists... 93%Reading package lists... 93%Reading package lists... 93%Reading package lists... 93%Reading package lists... 93%Reading package lists... 96%Reading package lists... 96%Reading package lists... 98%Reading package lists... 98%Reading package lists... 98%Reading package lists... 98%Reading package lists... 98%Reading package lists... 98%Reading package lists... Done
root@master2:~# apt-get install     apt-transport-https     ca-certificates     curl     gnupg-agent     software-properties- common
Reading package lists... 0%Reading package lists... 100%Reading package lists... Done
Building dependency tree... 0%Building dependency tree... 0%Building dependency tree... 50%Building dependency tree... 50%Building dependency tree       
Reading state information... 0%Reading state information... 0%Reading state information... Done
apt-transport-https is already the newest version (1.2.32).
ca-certificates is already the newest version (20170717~16.04.2).
curl is already the newest version (7.47.0-1ubuntu2.14).
gnupg-agent is already the newest version (2.1.11-6ubuntu2.1).
gnupg-agent set to manually installed.
software-properties-common is already the newest version (0.96.20.9).
The following packages were automatically installed and are no longer required:
  linux-aws-headers-4.4.0-1095 linux-headers-4.4.0-1095-aws linux-headers-4.4.0-170 linux-headers-4.4.0-170-generic
  linux-image-4.4.0-1095-aws linux-image-4.4.0-170-generic linux-modules-4.4.0-1095-aws linux-modules-4.4.0-170-generic
Use 'apt autoremove' to remove them.
0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.
root@master2:~# curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
OK
root@master2:~# 
root@master2:~# apt-key fingerprint 0EBFCD88
pub   4096R/0EBFCD88 2017-02-22
      Key fingerprint = 9DC8 5822 9FC7 DD38 854A  E2D8 8D81 803C 0EBF CD88
uid                  Docker Release (CE deb) <docker@docker.com>
sub   4096R/F273FCD8 2017-02-22

root@master2:~# add-apt-repository    "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) 122     s table"

root@master2:~# apt-get update
0% [Working]            Hit:1 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial InRelease
0% [Connecting to security.ubuntu.com (2001:67c:1562::16)]                                                          Hit:2 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-updates InRelease
                                                          Hit:3 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-backports InRelease
0% [Connecting to security.ubuntu.com (2001:67c:1562::16)]0% [1 InRelease gpgv 247 kB] [Connecting to security.ubuntu.com (2001:67c:1562::16)]                                                                                    0% [Waiting for headers]                        Get:4 https://download.docker.com/linux/ubuntu xenial InRelease [66.2 kB]
0% [Waiting for headers] [4 InRelease 16.4 kB/66.2 kB 25%]0% [2 InRelease gpgv 109 kB] [Waiting for headers] [4 InRelease 16.4 kB/66.2 kB 25%]                                                                                    0% [2 InRelease gpgv 109 kB] [Waiting for headers]                                                  Hit:5 http://security.ubuntu.com/ubuntu xenial-security InRelease
                                                  0% [2 InRelease gpgv 109 kB]                            0% [Working]0% [3 InRelease gpgv 107 kB]                            0% [Working]0% [4 InRelease gpgv 66.2 kB]                             0% [Working]0% [5 InRelease gpgv 109 kB]                            Get:6 https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages [12.0 kB]
0% [5 InRelease gpgv 109 kB]0% [6 Packages store 0 B] [5 InRelease gpgv 109 kB]                                                   0% [5 InRelease gpgv 109 kB]                            100% [Working]              Fetched 78.2 kB in 0s (195 kB/s)
Reading package lists... 0%Reading package lists... 0%Reading package lists... 1%Reading package lists... 6%Reading package lists... 6%Reading package lists... 9%Reading package lists... 9%Reading package lists... 9%Reading package lists... 9%Reading package lists... 9%Reading package lists... 9%Reading package lists... 44%Reading package lists... 44%Reading package lists... 64%Reading package lists... 64%Reading package lists... 64%Reading package lists... 64%Reading package lists... 65%Reading package lists... 65%Reading package lists... 71%Reading package lists... 71%Reading package lists... 72%Reading package lists... 77%Reading package lists... 77%Reading package lists... 77%Reading package lists... 77%Reading package lists... 77%Reading package lists... 77%Reading package lists... 81%Reading package lists... 81%Reading package lists... 83%Reading package lists... 83%Reading package lists... 83%Reading package lists... 83%Reading package lists... 84%Reading package lists... 84%Reading package lists... 84%Reading package lists... 84%Reading package lists... 84%Reading package lists... 84%Reading package lists... 84%Reading package lists... 84%Reading package lists... 84%Reading package lists... 84%Reading package lists... 88%Reading package lists... 88%Reading package lists... 93%Reading package lists... 93%Reading package lists... 93%Reading package lists... 93%Reading package lists... 93%Reading package lists... 93%Reading package lists... 96%Reading package lists... 96%Reading package lists... 98%Reading package lists... 98%Reading package lists... 98%Reading package lists... 98%Reading package lists... 98%Reading package lists... 98%Reading package lists... 98%Reading package lists... 98%Reading package lists... Done
root@master2:~# 
root@master2:~# apt-get install docker-ce docker-ce-cli containerd.io
Reading package lists... 0%Reading package lists... 100%Reading package lists... Done
Building dependency tree... 0%Building dependency tree... 0%Building dependency tree... 50%Building dependency tree... 50%Building dependency tree       
Reading state information... 0%Reading state information... 0%Reading state information... Done
The following packages were automatically installed and are no longer required:
  linux-aws-headers-4.4.0-1095 linux-headers-4.4.0-1095-aws linux-headers-4.4.0-170 linux-headers-4.4.0-170-generic
  linux-image-4.4.0-1095-aws linux-image-4.4.0-170-generic linux-modules-4.4.0-1095-aws linux-modules-4.4.0-170-generic
Use 'apt autoremove' to remove them.
The following additional packages will be installed:
  aufs-tools cgroupfs-mount pigz
The following NEW packages will be installed:
  aufs-tools cgroupfs-mount containerd.io docker-ce docker-ce-cli pigz
0 upgraded, 6 newly installed, 0 to remove and 15 not upgraded.
Need to get 85.3 MB of archives.
After this operation, 384 MB of additional disk space will be used.
Do you want to continue? [Y/n] y
0% [Working]            Get:1 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial/universe amd64 pigz amd64 2.3.1-2 [61.1 kB]
0% [1 pigz 14.1 kB/61.1 kB 23%]                               3% [Working]            Get:2 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial/universe amd64 aufs-tools amd64 1:3.2+20130722-1.1ubuntu1 [92.9 kB]
3% [2 aufs-tools 3,631 B/92.9 kB 4%]                                    7% [Working]            Get:3 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial/universe amd64 cgroupfs-mount all 1.2 [4,970 B]
7% [3 cgroupfs-mount 4,096 B/4,970 B 82%]                                         10% [Working]             Get:4 https://download.docker.com/linux/ubuntu xenial/stable amd64 containerd.io amd64 1.2.10-3 [19.9 MB]
10% [4 containerd.io 0 B/19.9 MB 0%]                                    32% [Working]             Get:5 https://download.docker.com/linux/ubuntu xenial/stable amd64 docker-ce-cli amd64 5:19.03.5~3-0~ubuntu-xenial [42.4 MB]
32% [5 docker-ce-cli 0 B/42.4 MB 0%]60% [5 docker-ce-cli 30.2 MB/42.4 MB 71%]                                         75% [Working]             Get:6 https://download.docker.com/linux/ubuntu xenial/stable amd64 docker-ce amd64 5:19.03.5~3-0~ubuntu-xenial [22.8 MB]
75% [6 docker-ce 16.4 kB/22.8 MB 0%]                                    100% [Working]              Fetched 85.3 MB in 1s (50.6 MB/s)
Selecting previously unselected package pigz.
(Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 303104 files and directories currently installed.)
Preparing to unpack .../pigz_2.3.1-2_amd64.deb ...
Unpacking pigz (2.3.1-2) ...
Selecting previously unselected package aufs-tools.
Preparing to unpack .../aufs-tools_1%3a3.2+20130722-1.1ubuntu1_amd64.deb ...
Unpacking aufs-tools (1:3.2+20130722-1.1ubuntu1) ...
Selecting previously unselected package cgroupfs-mount.
Preparing to unpack .../cgroupfs-mount_1.2_all.deb ...
Unpacking cgroupfs-mount (1.2) ...
Selecting previously unselected package containerd.io.
Preparing to unpack .../containerd.io_1.2.10-3_amd64.deb ...
Unpacking containerd.io (1.2.10-3) ...
Selecting previously unselected package docker-ce-cli.
Preparing to unpack .../docker-ce-cli_5%3a19.03.5~3-0~ubuntu-xenial_amd64.deb ...
Unpacking docker-ce-cli (5:19.03.5~3-0~ubuntu-xenial) ...
Selecting previously unselected package docker-ce.
Preparing to unpack .../docker-ce_5%3a19.03.5~3-0~ubuntu-xenial_amd64.deb ...
Unpacking docker-ce (5:19.03.5~3-0~ubuntu-xenial) ...
Processing triggers for man-db (2.7.5-1) ...
Processing triggers for libc-bin (2.23-0ubuntu11) ...
Processing triggers for ureadahead (0.100.0-19.1) ...
Processing triggers for systemd (229-4ubuntu21.22) ...
Setting up pigz (2.3.1-2) ...
Setting up aufs-tools (1:3.2+20130722-1.1ubuntu1) ...
Setting up cgroupfs-mount (1.2) ...
Setting up containerd.io (1.2.10-3) ...
Setting up docker-ce-cli (5:19.03.5~3-0~ubuntu-xenial) ...
Setting up docker-ce (5:19.03.5~3-0~ubuntu-xenial) ...
Processing triggers for libc-bin (2.23-0ubuntu11) ...
Processing triggers for systemd (229-4ubuntu21.22) ...
Processing triggers for ureadahead (0.100.0-19.1) ...
root@master2:~# ip route
default via 172.31.112.1 dev ens5 
169.254.0.0/16 dev ens5  scope link  metric 1000 
172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1 linkdown 
172.31.112.0/20 dev ens5  proto kernel  scope link  src 172.31.125.181 
root@master2:~# kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   102m
root@master2:~# logout
Connection to master2 closed.
root@master1:~# ssh master23
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 18:55:32 2020 from 172.31.122.25
root@master3:~# kubectl eget svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   102m
root@master3:~# kubectl get deplouyy
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
nginx   1/1     1            1           28m
root@master3:~# logout
Connection to master3 closed.
root@master1:~# kubectl create -f exter 
The Service "nginx" is invalid: spec.ports[0].nodePort: Invalid value: 80: provided port is not in the valid range. The range of valid ports is 30000-32767
root@master1:~# vi exter 
  "exter" 26L, 510CapiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2020-01-24T18:31:58Z"
  labels:
    app: nginx
  name: nginx
  namespace: default
  resourceVersion: "6801"
  selfLink: /api/v1/namespaces/default/services/nginx
  uid: cde566d7-3ed7-11ea-b409-020f08380e46
spec:
  externalIPs:
  - 34.222.41.86
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 80
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            1,1All2345678910,11234567-- INSERT --17,17All17,16All54-- INSERT --17,15All3607084932017,19All:wq!"exter" 26L, 513C written
root@master1:~# vi exter kubectl create -f
service/nginx created
root@master1:~# ssh master2
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 18:56:06 2020 from 172.31.122.25
root@master2:~# kubectl eget svc
NAME         TYPE        CLUSTER-IP    EXTERNAL-IP    PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1     <none>         443/TCP        103m
nginx        NodePort    10.96.0.153   34.222.41.86   80:30043/TCP   12s
root@master2:~# dokcker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
root@master2:~# 
root@master2:~# 
root@master2:~# ping 10.96.0.153
PING 10.96.0.153 (10.96.0.153) 56(84) bytes of data.
^C
--- 10.96.0.153 ping statistics ---
2 packets transmitted, 0 received, 100% packet loss, time 1006ms

root@master2:~# curl kubectl get endpoints
NAME         ENDPOINTS                                                  AGE
kubernetes   172.31.115.2:6443,172.31.122.25:6443,172.31.125.181:6443   104m
nginx        10.44.0.2:80                                               37s
root@master2:~# ping 10.44.0.2
PING 10.44.0.2 (10.44.0.2) 56(84) bytes of data.
^C
--- 10.44.0.2 ping statistics ---
2 packets transmitted, 0 received, 100% packet loss, time 1007ms

root@master2:~# kubectl get svc
NAME         TYPE        CLUSTER-IP    EXTERNAL-IP    PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1     <none>         443/TCP        104m
nginx        NodePort    10.96.0.153   34.222.41.86   80:30043/TCP   81s
root@master2:~# 
root@master2:~# 
root@master2:~# 
root@master2:~# 
root@master2:~# 
root@master2:~# ls -l /opt/cn
root@master2:~# docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
c7ad499ca0f4        bridge              bridge              local
a25ba08e5bbb        host                host                local
504f36891407        none                null                local
root@master2:~# 
root@master2:~# scp;  ,sdmkdrir -pm /opt/cni/bin/
mkdir: missing operand
Try 'mkdir --help' for more information.
root@master2:~# mkdir -pm /opt/cni/bin/7/opt/cni/bin/5/opt/cni/bin/5/opt/cni/bin/ /opt/cni/bin/
root@master2:~# dscsc p -r master233:/opt/cni/bin/ /opt/cni/bin/
ssh: Could not resolve hostname master3: Name or service not known
root@master2:~# scp -r master3:/opt/cni/bin /opt/cni/scp -r master3:/opt/cni/bin /opt/cni/scp -r master3:/opt/cni/bin /opt/cni/scp -r master3:/opt/cni/bin /opt/cni/172.31.115.2
The authenticity of host '172.31.115.2 (172.31.115.2)' can't be established.
ECDSA key fingerprint is SHA256:jGqxJlyw5oqIVViv1HQbrgCVs5yaGHOMpNZmtfvysX4.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '172.31.115.2' (ECDSA) to the list of known hosts.
sample                                                                                       0%    0     0.0KB/s   --:-- ETAsample                                                                                     100% 2580KB   2.5MB/s   00:00    
tuning                                                                                       0%    0     0.0KB/s   --:-- ETAtuning                                                                                     100% 2783KB   2.7MB/s   00:00    
weave-ipam                                                                                   0%    0     0.0KB/s   --:-- ETAweave-ipam                                                                                 100%   26MB  26.3MB/s   00:01    
host-local                                                                                   0%    0     0.0KB/s   --:-- ETAhost-local                                                                                 100% 2966KB   2.9MB/s   00:00    
macvlan                                                                                      0%    0     0.0KB/s   --:-- ETAmacvlan                                                                                    100% 3529KB   3.5MB/s   00:00    
ptp                                                                                          0%    0     0.0KB/s   --:-- ETAptp                                                                                        100% 3900KB   3.8MB/s   00:00    
bridge                                                                                       0%    0     0.0KB/s   --:-- ETAbridge                                                                                     100% 3934KB   3.8MB/s   00:00    
host-device                                                                                  0%    0     0.0KB/s   --:-- ETAhost-device                                                                                100% 3054KB   3.0MB/s   00:00    
flannel                                                                                      0%    0     0.0KB/s   --:-- ETAflannel                                                                                    100% 2789KB   2.7MB/s   00:00    
weave-net                                                                                    0%    0     0.0KB/s   --:-- ETAweave-net                                                                                  100%   26MB  26.3MB/s   00:01    
ipvlan                                                                                       0%    0     0.0KB/s   --:-- ETAipvlan                                                                                     100% 3489KB   3.4MB/s   00:00    
dhcp                                                                                         0%    0     0.0KB/s   --:-- ETAdhcp                                                                                       100% 9993KB   9.8MB/s   00:00    
portmap                                                                                      0%    0     0.0KB/s   --:-- ETAportmap                                                                                    100% 3468KB   3.4MB/s   00:00    
weave-plugin-2.6.0                                                                           0%    0     0.0KB/s   --:-- ETAweave-plugin-2.6.0                                                                         100%   26MB  26.3MB/s   00:00    
vlan                                                                                         0%    0     0.0KB/s   --:-- ETAvlan                                                                                       100% 3485KB   3.4MB/s   00:01    
loopback                                                                                     0%    0     0.0KB/s   --:-- ETAloopback                                                                                   100% 3012KB   2.9MB/s   00:00    
root@master2:~# cd /opt/cni/
root@master2:/opt/cni# ls -la
total 12
drwxr-xr-x 3 root root 4096 Jan 24 19:02 .
drwxr-xr-x 5 root root 4096 Jan 24 19:02 ..
drwxr-xr-x 2 root root 4096 Jan 24 19:03 bin
root@master2:/opt/cni# cd bin
root@master2:/opt/cni/bin# ls -la
total 129716
drwxr-xr-x 2 root root     4096 Jan 24 19:03 .
drwxr-xr-x 3 root root     4096 Jan 24 19:02 ..
-rwxr-xr-x 1 root root  4028260 Jan 24 19:03 bridge
-rwxr-xr-x 1 root root 10232415 Jan 24 19:03 dhcp
-rwxr-xr-x 1 root root  2856252 Jan 24 19:03 flannel
-rwxr-xr-x 1 root root  3127363 Jan 24 19:03 host-device
-rwxr-xr-x 1 root root  3036768 Jan 24 19:03 host-local
-rwxr-xr-x 1 root root  3572685 Jan 24 19:03 ipvlan
-rwxr-xr-x 1 root root  3084347 Jan 24 19:03 loopback
-rwxr-xr-x 1 root root  3613497 Jan 24 19:03 macvlan
-rwxr-xr-x 1 root root  3551125 Jan 24 19:03 portmap
-rwxr-xr-x 1 root root  3993428 Jan 24 19:03 ptp
-rwxr-xr-x 1 root root  2641877 Jan 24 19:03 sample
-rwxr-xr-x 1 root root  2850029 Jan 24 19:03 tuning
-rwxr-xr-x 1 root root  3568537 Jan 24 19:03 vlan
-rwxr-xr-x 1 root root 27544296 Jan 24 19:03 weave-ipam
-rwxr-xr-x 1 root root 27544296 Jan 24 19:03 weave-net
-rwxr-xr-x 1 root root 27544296 Jan 24 19:03 weave-plugin-2.6.0
root@master2:/opt/cni/bin# 
root@master2:/opt/cni/bin# cd
root@master2:~# 
root@master2:~# cdls -lacd binls -lacd /opt/cni/scp -r 172.31.115.2:/opt/cni/bin /opt/cni/ /opt/cni/ /opt/cni/ /opt/cni/ /opt/cni/ /opt/cni/ /opt/cni/ /opt/cni/ /opt/cni/ /opt/cni/ /opt/cni/e /opt/cni/t /opt/cni/c /opt/cni// /opt/cni/c /opt/cni/n /opt/cni/i/ /opt/cni/net.d/ /opt/cni/1 /opt/cni/0-weave.conflist  /opt/cni//etc/cni/net.d/mkdir /etc/cni/net.d/;
mkdir: cannot create directory /etc/cni/net.d/: No such file or directory
/etc/cni/net.d/: No such file or directory
root@master2:~# mkdir /etc/cni/net.d/;scp -r 172.31.115.2:/etc/cni/net.d/10-weave.conflist  /etc/cni/net.d/mkdir /etc/cni/net.d/;scp -r 172.31.115.2:/etc/cni/net.d/10-weave.conflist  /etc/cni/net.d/-pm 755 
10-weave.conflist                                                                            0%    0     0.0KB/s   --:-- ETA10-weave.conflist                                                                          100%  318     0.3KB/s   00:00    
root@master2:~# mkdir -pm 755 /etc/cni/net.d/;scp -r 172.31.115.2:/etc/cni/net.d/10-weave.conflist  /etc/cni/net.d/-pm 755 (reverse-i-search)`':p': mkdir -pm 755 /etc/cni/net.d/;scp -r 172.31.115.2:/etc/cni/net.d/10-weave.conflist  /etc/cni/net.d/i': ping 10.44.0.2root@master2:~#root@master2:~# 
PING 10.44.0.2 (10.44.0.2) 56(84) bytes of data.
^C
--- 10.44.0.2 ping statistics ---
2 packets transmitted, 0 received, 100% packet loss, time 999ms

root@master2:~# 
root@master2:~# ip route
default via 172.31.112.1 dev ens5 
169.254.0.0/16 dev ens5  scope link  metric 1000 
172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1 linkdown 
172.31.112.0/20 dev ens5  proto kernel  scope link  src 172.31.125.181 
root@master2:~# 
root@master2:~# iroutepa
route   routef  routel  
root@master2:~# route
route   routef  routel  
root@master2:~# route
route   routef  routel  
root@master2:~# routetraceroute6 /etc/cni/net.d/10.44.0.2
The program 'traceroute' can be found in the following packages:
 * inetutils-traceroute
 * traceroute
Try: apt install <selected package>
root@master2:~# trace
tracepath            tracepath6           traceroute6          traceroute6.iputils  
root@master2:~# trace
tracepath            tracepath6           traceroute6          traceroute6.iputils  
root@master2:~# trace
tracepath            tracepath6           traceroute6          traceroute6.iputils  
root@master2:~# traceroute6 10.44.0.2
traceroute: unknown host 10.44.0.2
root@master2:~# traceroute6 10.44.0.2path 10.44.0.2
 1?: [LOCALHOST]                                         pmtu 9001
 1:  no reply
^C
root@master2:~# ssh 172.31.115.2
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1100-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

15 packages can be updated.
0 updates are security updates.

New release '18.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Jan 24 18:59:33 2020 from 172.31.122.25
root@master3:~# tracepath 10.44.0.2
 1?: [LOCALHOST]                                         pmtu 1376
 1:  10.44.0.2                                             0.921ms reached
 1:  10.44.0.2                                             0.264ms reached
     Resume: pmtu 1376 hops 1 back 1 
root@master3:~# docker ps
p CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS               NAMES
7577bea41c94        busybox                "sleep 3600"             36 minutes ago      Up 36 minutes                           k8s_busybox_busybox_default_90e54017-3ed7-11ea-b409-020f08380e46_0
0d819869851c        k8s.gcr.io/pause:3.1   "/pause"                 36 minutes ago      Up 36 minutes                           k8s_POD_busybox_default_90e54017-3ed7-11ea-b409-020f08380e46_0
7f821d947a73        coredns/coredns        "/coredns -conf /etc"   36 minutes ago      Up 36 minutes                           k8s_coredns_coredns-69cbb76ff8-pmvk8_kube-system_81ff8d05-3ed7-11ea-8877-0214d9bd2bee_0
45220258f4f7        k8s.gcr.io/pause:3.1   "/pause"                 36 minutes ago      Up 36 minutes                           k8s_POD_coredns-69cbb76ff8-pmvk8_kube-system_81ff8d05-3ed7-11ea-8877-0214d9bd2bee_0
768f4e1fbc27        174e0e8ef23d           "/home/weave/launch."   About an hour ago   Up About an hour                        k8s_weave_weave-net-xfnfn_kube-system_489dab20-3ed1-11ea-9d57-020f08380e46_2
7f652d36ea20        5105e13e253e           "/usr/bin/launch.sh"     About an hour ago   Up About an hour                        k8s_weave-npc_weave-net-xfnfn_kube-system_489dab20-3ed1-11ea-9d57-020f08380e46_1
d81cb9b081aa        k8s.gcr.io/pause:3.1   "/pause"                 About an hour ago   Up About an hour                        k8s_POD_weave-net-xfnfn_kube-system_489dab20-3ed1-11ea-9d57-020f08380e46_1
root@master3:~# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: ens5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc mq state UP group default qlen 1000
    link/ether 02:42:70:30:37:2e brd ff:ff:ff:ff:ff:ff
    inet 172.31.115.2/20 brd 172.31.127.255 scope global ens5
       valid_lft forever preferred_lft forever
    inet6 2600:1f14:ec4:9800:9f22:9c4f:bdf0:5792/128 scope global deprecated 
       valid_lft forever preferred_lft 0sec
    inet6 fe80::42:70ff:fe30:372e/64 scope link 
       valid_lft forever preferred_lft forever
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:59:c1:ee:69 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
4: datapath: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue state UNKNOWN group default qlen 1
    link/ether c6:a4:b7:d8:93:9a brd ff:ff:ff:ff:ff:ff
    inet6 fe80::c4a4:b7ff:fed8:939a/64 scope link 
       valid_lft forever preferred_lft forever
6: weave: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue state UP group default qlen 1000
    link/ether 66:88:cf:01:d0:a3 brd ff:ff:ff:ff:ff:ff
    inet 10.32.0.1/12 brd 10.47.255.255 scope global weave
       valid_lft forever preferred_lft forever
    inet6 fe80::6488:cfff:fe01:d0a3/64 scope link 
       valid_lft forever preferred_lft forever
7: dummy0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether ee:cd:89:79:27:0d brd ff:ff:ff:ff:ff:ff
9: vethwe-datapath@vethwe-bridge: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue master datapath state UP group default 
    link/ether e2:a3:14:3c:d8:08 brd ff:ff:ff:ff:ff:ff
    inet6 fe80::e0a3:14ff:fe3c:d808/64 scope link 
       valid_lft forever preferred_lft forever
10: vethwe-bridge@vethwe-datapath: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue master weave state UP group default 
    link/ether 66:33:ff:33:3e:65 brd ff:ff:ff:ff:ff:ff
    inet6 fe80::6433:ffff:fe33:3e65/64 scope link 
       valid_lft forever preferred_lft forever
11: vxlan-6784: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 65485 qdisc noqueue master datapath state UNKNOWN group default qlen 1000
    link/ether 3e:d2:d2:c9:11:9c brd ff:ff:ff:ff:ff:ff
13: vethwepl4522025@if12: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue master weave state UP group default 
    link/ether 2a:ee:7f:ea:f1:b2 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet6 fe80::28ee:7fff:feea:f1b2/64 scope link 
       valid_lft forever preferred_lft forever
15: vethwepl0d81986@if14: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue master weave state UP group default 
    link/ether 86:0f:2e:34:70:8d brd ff:ff:ff:ff:ff:ff link-netnsid 1
    inet6 fe80::840f:2eff:fe34:708d/64 scope link 
       valid_lft forever preferred_lft forever
root@master3:~# ip route
default via 172.31.112.1 dev ens5 
10.32.0.0/12 dev weave  proto kernel  scope link  src 10.32.0.1 
169.254.0.0/16 dev ens5  scope link  metric 1000 
172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1 linkdown 
172.31.112.0/20 dev ens5  proto kernel  scope link  src 172.31.115.2 
root@master3:~# nslookup 10.44.0.2
Server:172.31.0.2
Address:172.31.0.2#53

** server can't find 2.0.44.10.in-addr.arpa: NXDOMAIN

root@master3:~# traceroute610.44.0.2 10.44.0.2
traceroute: unknown host 10.44.0.2
root@master3:~# 
root@master3:~# logout
Connection to 172.31.115.2 closed.
root@master2:~# ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: ens5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc mq state UP group default qlen 1000
    link/ether 02:14:d9:bd:2b:ee brd ff:ff:ff:ff:ff:ff
    inet 172.31.125.181/20 brd 172.31.127.255 scope global ens5
       valid_lft forever preferred_lft forever
    inet6 2600:1f14:ec4:9800:33f4:b86:8a75:8d42/128 scope global 
       valid_lft forever preferred_lft forever
    inet6 fe80::14:d9ff:febd:2bee/64 scope link 
       valid_lft forever preferred_lft forever
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:14:b1:e3:64 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
root@master2:~# iop aip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: ens5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc mq state UP group default qlen 1000
    link/ether 02:14:d9:bd:2b:ee brd ff:ff:ff:ff:ff:ff
    inet 172.31.125.181/20 brd 172.31.127.255 scope global ens5
       valid_lft forever preferred_lft forever
    inet6 2600:1f14:ec4:9800:33f4:b86:8a75:8d42/128 scope global 
       valid_lft forever preferred_lft forever
    inet6 fe80::14:d9ff:febd:2bee/64 scope link 
       valid_lft forever preferred_lft forever
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:14:b1:e3:64 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
root@master2:~# 
root@master2:~# 
root@master2:~# 
root@master2:~# 
root@master2:~# 
root@master2:~# 
root@master2:~# apt-get install python
Reading package lists... 0%Reading package lists... 100%Reading package lists... Done
Building dependency tree... 0%Building dependency tree... 0%Building dependency tree... 50%Building dependency tree... 50%Building dependency tree       
Reading state information... 0%Reading state information... 0%Reading state information... Done
python is already the newest version (2.7.12-1~16.04).
The following packages were automatically installed and are no longer required:
  linux-aws-headers-4.4.0-1095 linux-headers-4.4.0-1095-aws linux-headers-4.4.0-170 linux-headers-4.4.0-170-generic
  linux-image-4.4.0-1095-aws linux-image-4.4.0-170-generic linux-modules-4.4.0-1095-aws linux-modules-4.4.0-170-generic
Use 'apt autoremove' to remove them.
0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.
root@master2:~# python -v
# installing zipimport hook
import zipimport # builtin
# installed zipimport hook
# /usr/lib/python2.7/site.pyc matches /usr/lib/python2.7/site.py
import site # precompiled from /usr/lib/python2.7/site.pyc
# /usr/lib/python2.7/os.pyc matches /usr/lib/python2.7/os.py
import os # precompiled from /usr/lib/python2.7/os.pyc
import errno # builtin
import posix # builtin
# /usr/lib/python2.7/posixpath.pyc matches /usr/lib/python2.7/posixpath.py
import posixpath # precompiled from /usr/lib/python2.7/posixpath.pyc
# /usr/lib/python2.7/stat.pyc matches /usr/lib/python2.7/stat.py
import stat # precompiled from /usr/lib/python2.7/stat.pyc
# /usr/lib/python2.7/genericpath.pyc matches /usr/lib/python2.7/genericpath.py
import genericpath # precompiled from /usr/lib/python2.7/genericpath.pyc
# /usr/lib/python2.7/warnings.pyc matches /usr/lib/python2.7/warnings.py
import warnings # precompiled from /usr/lib/python2.7/warnings.pyc
# /usr/lib/python2.7/linecache.pyc matches /usr/lib/python2.7/linecache.py
import linecache # precompiled from /usr/lib/python2.7/linecache.pyc
# /usr/lib/python2.7/types.pyc matches /usr/lib/python2.7/types.py
import types # precompiled from /usr/lib/python2.7/types.pyc
# /usr/lib/python2.7/UserDict.pyc matches /usr/lib/python2.7/UserDict.py
import UserDict # precompiled from /usr/lib/python2.7/UserDict.pyc
# /usr/lib/python2.7/_abcoll.pyc matches /usr/lib/python2.7/_abcoll.py
import _abcoll # precompiled from /usr/lib/python2.7/_abcoll.pyc
# /usr/lib/python2.7/abc.pyc matches /usr/lib/python2.7/abc.py
import abc # precompiled from /usr/lib/python2.7/abc.pyc
# /usr/lib/python2.7/_weakrefset.pyc matches /usr/lib/python2.7/_weakrefset.py
import _weakrefset # precompiled from /usr/lib/python2.7/_weakrefset.pyc
import _weakref # builtin
# /usr/lib/python2.7/copy_reg.pyc matches /usr/lib/python2.7/copy_reg.py
import copy_reg # precompiled from /usr/lib/python2.7/copy_reg.pyc
# /usr/lib/python2.7/traceback.pyc matches /usr/lib/python2.7/traceback.py
import traceback # precompiled from /usr/lib/python2.7/traceback.pyc
# /usr/lib/python2.7/sysconfig.pyc matches /usr/lib/python2.7/sysconfig.py
import sysconfig # precompiled from /usr/lib/python2.7/sysconfig.pyc
# /usr/lib/python2.7/re.pyc matches /usr/lib/python2.7/re.py
import re # precompiled from /usr/lib/python2.7/re.pyc
# /usr/lib/python2.7/sre_compile.pyc matches /usr/lib/python2.7/sre_compile.py
import sre_compile # precompiled from /usr/lib/python2.7/sre_compile.pyc
import _sre # builtin
# /usr/lib/python2.7/sre_parse.pyc matches /usr/lib/python2.7/sre_parse.py
import sre_parse # precompiled from /usr/lib/python2.7/sre_parse.pyc
# /usr/lib/python2.7/sre_constants.pyc matches /usr/lib/python2.7/sre_constants.py
import sre_constants # precompiled from /usr/lib/python2.7/sre_constants.pyc
import _locale # builtin
# /usr/lib/python2.7/_sysconfigdata.pyc matches /usr/lib/python2.7/_sysconfigdata.py
import _sysconfigdata # precompiled from /usr/lib/python2.7/_sysconfigdata.pyc
# /usr/lib/python2.7/plat-x86_64-linux-gnu/_sysconfigdata_nd.pyc matches /usr/lib/python2.7/plat-x86_64-linux-gnu/_sysconfigdata_nd.py
import _sysconfigdata_nd # precompiled from /usr/lib/python2.7/plat-x86_64-linux-gnu/_sysconfigdata_nd.pyc
# /usr/lib/python2.7/sitecustomize.pyc matches /usr/lib/python2.7/sitecustomize.py
import sitecustomize # precompiled from /usr/lib/python2.7/sitecustomize.pyc
import encodings # directory /usr/lib/python2.7/encodings
# /usr/lib/python2.7/encodings/__init__.pyc matches /usr/lib/python2.7/encodings/__init__.py
import encodings # precompiled from /usr/lib/python2.7/encodings/__init__.pyc
# /usr/lib/python2.7/codecs.pyc matches /usr/lib/python2.7/codecs.py
import codecs # precompiled from /usr/lib/python2.7/codecs.pyc
import _codecs # builtin
# /usr/lib/python2.7/encodings/aliases.pyc matches /usr/lib/python2.7/encodings/aliases.py
import encodings.aliases # precompiled from /usr/lib/python2.7/encodings/aliases.pyc
# /usr/lib/python2.7/encodings/utf_8.pyc matches /usr/lib/python2.7/encodings/utf_8.py
import encodings.utf_8 # precompiled from /usr/lib/python2.7/encodings/utf_8.pyc
Python 2.7.12 (default, Oct  8 2019, 14:14:10) 
[GCC 5.4.0 20160609] on linux2
Type "help", "copyright", "credits" or "license" for more information.
dlopen("/usr/lib/python2.7/lib-dynload/readline.x86_64-linux-gnu.so", 2);
import readline # dynamically loaded from /usr/lib/python2.7/lib-dynload/readline.x86_64-linux-gnu.so
>>> exit
Use exit() or Ctrl-D (i.e. EOF) to exit
>>> exit(0
... )
# clear __builtin__._
# clear sys.path
# clear sys.argv
# clear sys.ps1
# clear sys.ps2
# clear sys.exitfunc
# clear sys.exc_type
# clear sys.exc_value
# clear sys.exc_traceback
# clear sys.last_type
# clear sys.last_value
# clear sys.last_traceback
# clear sys.path_hooks
# clear sys.path_importer_cache
# clear sys.meta_path
# clear sys.flags
# clear sys.float_info
# restore sys.stdin
# restore sys.stdout
# restore sys.stderr
# cleanup __main__
# cleanup[1] encodings
# cleanup[1] site
# cleanup[1] sysconfig
# cleanup[1] abc
# cleanup[1] _weakrefset
# cleanup[1] sre_constants
# cleanup[1] re
# cleanup[1] _codecs
# cleanup[1] _warnings
# cleanup[1] zipimport
# cleanup[1] _sysconfigdata
# cleanup[1] encodings.utf_8
# cleanup[1] codecs
# cleanup[1] readline
# cleanup[1] _sysconfigdata_nd
# cleanup[1] _locale
# cleanup[1] sitecustomize
# cleanup[1] signal
# cleanup[1] traceback
# cleanup[1] posix
# cleanup[1] encodings.aliases
# cleanup[1] exceptions
# cleanup[1] _weakref
# cleanup[1] sre_compile
# cleanup[1] _sre
# cleanup[1] sre_parse
# cleanup[2] copy_reg
# cleanup[2] posixpath
# cleanup[2] errno
# cleanup[2] _abcoll
# cleanup[2] types
# cleanup[2] genericpath
# cleanup[2] stat
# cleanup[2] warnings
# cleanup[2] UserDict
# cleanup[2] os.path
# cleanup[2] linecache
# cleanup[2] os
# cleanup sys
# cleanup __builtin__
# cleanup ints: 19 unfreed ints
# cleanup floats
root@master2:~# python -vversion
Unknown option: -e
usage: python [option] ... [-c cmd | -m mod | file | -] [arg] ...
Try `python -h' for more information.
root@master2:~# 
root@master2:~# python -versionV
Python 2.7.12
root@master2:~# 
root@master2:~# pip
pi1toppm         pico             pigz             pilfile          pinentry         ping             pivot_root
pi3topbm         piconv           pilconvert       pilfont          pinentry-gnome3  ping6            pixeltool
pic              pidof            pildriver        pilprint         pinentry-x11     pinky            
root@master2:~# pipippython -Vversionapt-get install python-pip
Reading package lists... 0%Reading package lists... 100%Reading package lists... Done
Building dependency tree... 0%Building dependency tree... 0%Building dependency tree... 50%Building dependency tree... 50%Building dependency tree       
Reading state information... 0%Reading state information... 0%Reading state information... Done
The following packages were automatically installed and are no longer required:
  linux-aws-headers-4.4.0-1095 linux-headers-4.4.0-1095-aws linux-headers-4.4.0-170 linux-headers-4.4.0-170-generic
  linux-image-4.4.0-1095-aws linux-image-4.4.0-170-generic linux-modules-4.4.0-1095-aws linux-modules-4.4.0-170-generic
Use 'apt autoremove' to remove them.
The following additional packages will be installed:
  libexpat1-dev libpython-all-dev libpython-dev libpython2.7-dev python-all python-all-dev python-dev python-pip-whl
  python-pkg-resources python-setuptools python-wheel python2.7-dev
Suggested packages:
  python-setuptools-doc
The following NEW packages will be installed:
  libexpat1-dev libpython-all-dev libpython-dev libpython2.7-dev python-all python-all-dev python-dev python-pip
  python-pip-whl python-pkg-resources python-setuptools python-wheel python2.7-dev
0 upgraded, 13 newly installed, 0 to remove and 15 not upgraded.
Need to get 29.8 MB of archives.
After this operation, 45.1 MB of additional disk space will be used.
Do you want to continue? [Y/n] y
0% [Working]            Get:1 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libexpat1-dev amd64 2.1.0-7ubuntu0.16.04.5 [115 kB]
0% [1 libexpat1-dev 14.1 kB/115 kB 12%]                                       2% [Working]            Get:2 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpython2.7-dev amd64 2.7.12-1ubuntu0~16.04.9 [27.8 MB]
2% [2 libpython2.7-dev 15.1 kB/27.8 MB 0%]                                          78% [Working]             Get:3 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpython-dev amd64 2.7.12-1~16.04 [7,840 B]
             Get:4 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpython-all-dev amd64 2.7.12-1~16.04 [1,006 B]
80% [4 libpython-all-dev 1,006 B/1,006 B 100%]                                              Get:5 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 python-all amd64 2.7.12-1~16.04 [996 B]
                                              81% [5 python-all 996 B/996 B 100%]                                   83% [Working]             Get:6 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 python2.7-dev amd64 2.7.12-1ubuntu0~16.04.9 [276 kB]
83% [6 python2.7-dev 12.3 kB/276 kB 4%]                                       85% [Working]             Get:7 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 python-dev amd64 2.7.12-1~16.04 [1,186 B]
85% [7 python-dev 1,186 B/1,186 B 100%]                                       87% [Waiting for headers]                         Get:8 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 python-all-dev amd64 2.7.12-1~16.04 [1,016 B]
87% [8 python-all-dev 1,016 B/1,016 B 100%]                                           88% [Waiting for headers]                         Get:9 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 python-pip-whl all 8.1.1-2ubuntu0.4 [1,110 kB]
88% [9 python-pip-whl 39.6 kB/1,110 kB 4%]                                          93% [Working]             Get:10 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 python-pip all 8.1.1-2ubuntu0.4 [144 kB]
93% [10 python-pip 16.4 kB/144 kB 11%]                                      95% [Working]             Get:11 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial/main amd64 python-pkg-resources all 20.7.0-1 [108 kB]
95% [11 python-pkg-resources 29.2 kB/108 kB 27%]                                                96% [Working]             Get:12 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial/main amd64 python-setuptools all 20.7.0-1 [169 kB]
96% [12 python-setuptools 51.5 kB/169 kB 30%]                                             98% [Working]             Get:13 http://us-west-2.ec2.archive.ubuntu.com/ubuntu xenial/universe amd64 python-wheel all 0.29.0-1 [48.0 kB]
98% [13 python-wheel 48.0 kB/48.0 kB 100%]                                          100% [Working]              Fetched 29.8 MB in 0s (68.6 MB/s)
Selecting previously unselected package libexpat1-dev:amd64.
(Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 303401 files and directories currently installed.)
Preparing to unpack .../libexpat1-dev_2.1.0-7ubuntu0.16.04.5_amd64.deb ...
Unpacking libexpat1-dev:amd64 (2.1.0-7ubuntu0.16.04.5) ...
Selecting previously unselected package libpython2.7-dev:amd64.
Preparing to unpack .../libpython2.7-dev_2.7.12-1ubuntu0~16.04.9_amd64.deb ...
Unpacking libpython2.7-dev:amd64 (2.7.12-1ubuntu0~16.04.9) ...
Selecting previously unselected package libpython-dev:amd64.
Preparing to unpack .../libpython-dev_2.7.12-1~16.04_amd64.deb ...
Unpacking libpython-dev:amd64 (2.7.12-1~16.04) ...
Selecting previously unselected package libpython-all-dev:amd64.
Preparing to unpack .../libpython-all-dev_2.7.12-1~16.04_amd64.deb ...
Unpacking libpython-all-dev:amd64 (2.7.12-1~16.04) ...
Selecting previously unselected package python-all.
Preparing to unpack .../python-all_2.7.12-1~16.04_amd64.deb ...
Unpacking python-all (2.7.12-1~16.04) ...
Selecting previously unselected package python2.7-dev.
Preparing to unpack .../python2.7-dev_2.7.12-1ubuntu0~16.04.9_amd64.deb ...
Unpacking python2.7-dev (2.7.12-1ubuntu0~16.04.9) ...
Selecting previously unselected package python-dev.
Preparing to unpack .../python-dev_2.7.12-1~16.04_amd64.deb ...
Unpacking python-dev (2.7.12-1~16.04) ...
Selecting previously unselected package python-all-dev.
Preparing to unpack .../python-all-dev_2.7.12-1~16.04_amd64.deb ...
Unpacking python-all-dev (2.7.12-1~16.04) ...
Selecting previously unselected package python-pip-whl.
Preparing to unpack .../python-pip-whl_8.1.1-2ubuntu0.4_all.deb ...
Unpacking python-pip-whl (8.1.1-2ubuntu0.4) ...
Selecting previously unselected package python-pip.
Preparing to unpack .../python-pip_8.1.1-2ubuntu0.4_all.deb ...
Unpacking python-pip (8.1.1-2ubuntu0.4) ...
Selecting previously unselected package python-pkg-resources.
Preparing to unpack .../python-pkg-resources_20.7.0-1_all.deb ...
Unpacking python-pkg-resources (20.7.0-1) ...
Selecting previously unselected package python-setuptools.
Preparing to unpack .../python-setuptools_20.7.0-1_all.deb ...
Unpacking python-setuptools (20.7.0-1) ...
Selecting previously unselected package python-wheel.
Preparing to unpack .../python-wheel_0.29.0-1_all.deb ...
Unpacking python-wheel (0.29.0-1) ...
Processing triggers for doc-base (0.10.7) ...
Processing 1 added doc-base file...
Processing triggers for man-db (2.7.5-1) ...
Setting up libexpat1-dev:amd64 (2.1.0-7ubuntu0.16.04.5) ...
Setting up libpython2.7-dev:amd64 (2.7.12-1ubuntu0~16.04.9) ...
Setting up libpython-dev:amd64 (2.7.12-1~16.04) ...
Setting up libpython-all-dev:amd64 (2.7.12-1~16.04) ...
Setting up python-all (2.7.12-1~16.04) ...
Setting up python2.7-dev (2.7.12-1ubuntu0~16.04.9) ...
Setting up python-dev (2.7.12-1~16.04) ...
Setting up python-all-dev (2.7.12-1~16.04) ...
Setting up python-pip-whl (8.1.1-2ubuntu0.4) ...
Setting up python-pip (8.1.1-2ubuntu0.4) ...
Setting up python-pkg-resources (20.7.0-1) ...
Setting up python-setuptools (20.7.0-1) ...
Setting up python-wheel (0.29.0-1) ...
root@master2:~# pip
pip   pip2  
root@master2:~# pip
pip   pip2  
root@master2:~# pip isinst install flaskask
Collecting flask
  Downloading https://files.pythonhosted.org/packages/9b/93/628509b8d5dc749656a9641f4caf13540e2cdec85276964ff8f43bbb1d3b/Flask-1.1.1-py2.py3-none-any.whl (94kB)
    10% |                            | 10kB 46.7MB/s eta 0:00:01    21% |                         | 20kB 3.2MB/s eta 0:00:01    32% |                     | 30kB 4.2MB/s eta 0:00:01    43% |                  | 40kB 3.0MB/s eta 0:00:01    54% |              | 51kB 3.3MB/s eta 0:00:01    65% |           | 61kB 4.0MB/s eta 0:00:01    75% |       | 71kB 4.2MB/s eta 0:00:01    86% |    | 81kB 4.6MB/s eta 0:00:01    97% || 92kB 5.1MB/s eta 0:00:01    100% || 102kB 4.0MB/s 
Collecting itsdangerous>=0.24 (from flask)
  Downloading https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl
Collecting Jinja2>=2.10.1 (from flask)
  Downloading https://files.pythonhosted.org/packages/65/e0/eb35e762802015cab1ccee04e8a277b03f1d8e53da3ec3106882ec42558b/Jinja2-2.10.3-py2.py3-none-any.whl (125kB)
    8% |                             | 10kB 61.7MB/s eta 0:00:01    16% |                          | 20kB 17.8MB/s eta 0:00:01    24% |                        | 30kB 23.7MB/s eta 0:00:01    32% |                     | 40kB 13.3MB/s eta 0:00:01    40% |                   | 51kB 10.1MB/s eta 0:00:01    49% |                | 61kB 11.7MB/s eta 0:00:01    57% |             | 71kB 10.2MB/s eta 0:00:01    65% |           | 81kB 9.3MB/s eta 0:00:01    73% |        | 92kB 10.3MB/s eta 0:00:01    81% |     | 102kB 9.6MB/s eta 0:00:01    89% |   | 112kB 9.6MB/s eta 0:00:01    98% || 122kB 9.8MB/s eta 0:00:01    100% || 133kB 5.1MB/s 
Collecting click>=5.1 (from flask)
  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)
    12% |                            | 10kB 66.9MB/s eta 0:00:01    25% |                        | 20kB 33.7MB/s eta 0:00:01    37% |                    | 30kB 41.1MB/s eta 0:00:01    50% |               | 40kB 25.7MB/s eta 0:00:01    62% |           | 51kB 14.3MB/s eta 0:00:01    75% |       | 61kB 16.5MB/s eta 0:00:01    88% |   | 71kB 15.6MB/s eta 0:00:01    100% || 81kB 7.5MB/s 
Collecting Werkzeug>=0.15 (from flask)
  Downloading https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl (327kB)
    3% |                               | 10kB 60.9MB/s eta 0:00:01    6% |                              | 20kB 55.9MB/s eta 0:00:01    9% |                             | 30kB 61.1MB/s eta 0:00:01    12% |                            | 40kB 21.0MB/s eta 0:00:01    15% |                           | 51kB 17.6MB/s eta 0:00:01    18% |                          | 61kB 20.2MB/s eta 0:00:01    21% |                         | 71kB 14.8MB/s eta 0:00:01    25% |                        | 81kB 12.8MB/s eta 0:00:01    28% |                       | 92kB 14.1MB/s eta 0:00:01    31% |                      | 102kB 14.0MB/s eta 0:00:01    34% |                     | 112kB 14.0MB/s eta 0:00:01    37% |                    | 122kB 12.9MB/s eta 0:00:01    40% |                   | 133kB 10.5MB/s eta 0:00:01    43% |                  | 143kB 12.1MB/s eta 0:00:01    46% |                 | 153kB 12.2MB/s eta 0:00:01    50% |                | 163kB 11.0MB/s eta 0:00:01    53% |               | 174kB 13.4MB/s eta 0:00:01    56% |              | 184kB 12.5MB/s eta 0:00:01    59% |             | 194kB 12.5MB/s eta 0:00:01    62% |            | 204kB 12.5MB/s eta 0:00:01    65% |           | 215kB 11.2MB/s eta 0:00:01    68% |          | 225kB 12.0MB/s eta 0:00:01    71% |         | 235kB 14.1MB/s eta 0:00:01    75% |        | 245kB 12.5MB/s eta 0:00:01    78% |       | 256kB 13.7MB/s eta 0:00:01    81% |      | 266kB 14.0MB/s eta 0:00:01    84% |     | 276kB 13.8MB/s eta 0:00:01    87% |    | 286kB 16.1MB/s eta 0:00:01    90% |   | 296kB 12.4MB/s eta 0:00:01    93% |  | 307kB 13.4MB/s eta 0:00:01    96% | | 317kB 14.2MB/s eta 0:00:01    100% || 327kB 3.2MB/s 
Collecting MarkupSafe>=0.23 (from Jinja2>=2.10.1->flask)
  Downloading https://files.pythonhosted.org/packages/fb/40/f3adb7cf24a8012813c5edb20329eb22d5d8e2a0ecf73d21d6b85865da11/MarkupSafe-1.1.1-cp27-cp27mu-manylinux1_x86_64.whl
Installing collected packages: itsdangerous, MarkupSafe, Jinja2, click, Werkzeug, flask
Successfully installed Jinja2-2.10.3 MarkupSafe-1.1.1 Werkzeug-0.16.0 click-7.0 flask-1.1.1 itsdangerous-1.1.0
You are using pip version 8.1.1, however version 20.0.2 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
root@master2:~# civi app.py
  "app.py" [New File]~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            0,0-1All-- INSERT --0,1Allimport os
from flask import Flaskfrom flask import render_templateimport socketimport randomimport osapp = Flask(__name__)color_codes = {    "red": "#e74c3c",    "green": "#16a085",    "blue": "#2980b9",    "blue2": "#30336b",    "pink": "#be2edd",    "darkblue": "#130f40"}color = os.environ.get('APP_COLOR') or random.choice(["red","green","blue","blue2","darkblue","pink"])@app.route("/")def main():    #return 'Hello'    print(color)    return render_template('hello.html', name=socket.gethostname(), color=color_codes[color])@app.route('/color/<new_color>')def new_color(new_color):    return render_template('hello.html', name=socket.gethostname(), color=color_codes[new_color])@app.route('/read_file')def read_file():    f = open("/data/testfile.txt")    contents = f.read()    return render_template('hello.html', name=socket.gethostname(), contents=contents, color=color_codes[color])if __name__ == "__main__":    app.run(host="0.0.0.0", port="8080")39,139,0-1All~                                                                                                                            8,5  :wq!"app.py" [New] 38L, 996C written
root@master2:~# python app.py 
 * Serving Flask app "app" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
Traceback (most recent call last):
  File "app.py", line 38, in <module>
    app.run(host="0.0.0.0", port="8080")
  File "/usr/local/lib/python2.7/dist-packages/flask/app.py", line 990, in run
    run_simple(host, port, self, **options)
  File "/usr/local/lib/python2.7/dist-packages/werkzeug/serving.py", line 1010, in run_simple
    inner()
  File "/usr/local/lib/python2.7/dist-packages/werkzeug/serving.py", line 963, in inner
    fd=fd,
  File "/usr/local/lib/python2.7/dist-packages/werkzeug/serving.py", line 806, in make_server
    host, port, app, request_handler, passthrough_errors, ssl_context, fd=fd
  File "/usr/local/lib/python2.7/dist-packages/werkzeug/serving.py", line 699, in __init__
    HTTPServer.__init__(self, server_address, handler)
  File "/usr/lib/python2.7/SocketServer.py", line 417, in __init__
    self.server_bind()
  File "/usr/lib/python2.7/BaseHTTPServer.py", line 108, in server_bind
    SocketServer.TCPServer.server_bind(self)
  File "/usr/lib/python2.7/SocketServer.py", line 431, in server_bind
    self.socket.bind(self.server_address)
  File "/usr/lib/python2.7/socket.py", line 228, in meth
    return getattr(self._sock,name)(*args)
socket.error: [Errno 98] Address already in use
root@master2:~# netstat -rn 
Kernel IP routing table
Destination     Gateway         Genmask         Flags   MSS Window  irtt Iface
0.0.0.0         172.31.112.1    0.0.0.0         UG        0 0          0 ens5
169.254.0.0     0.0.0.0         255.255.0.0     U         0 0          0 ens5
172.17.0.0      0.0.0.0         255.255.0.0     U         0 0          0 docker0
172.31.112.0    0.0.0.0         255.255.240.0   U         0 0          0 ens5
root@master2:~#  grenetstat -rn tlump
netstat: invalid option -- 'm'
usage: netstat [-vWeenNcCF] [<Af>] -r         netstat {-V|--version|-h|--help}
       netstat [-vWnNcaeol] [<Socket> ...]
       netstat { [-vWeenNac] -i | [-cWnNe] -M | -s }

        -r, --route              display routing table
        -i, --interfaces         display interface table
        -g, --groups             display multicast group memberships
        -s, --statistics         display networking statistics (like SNMP)
        -M, --masquerade         display masqueraded connections

        -v, --verbose            be verbose
        -W, --wide               don't truncate IP addresses
        -n, --numeric            don't resolve names
        --numeric-hosts          don't resolve host names
        --numeric-ports          don't resolve port names
        --numeric-users          don't resolve user names
        -N, --symbolic           resolve hardware names
        -e, --extend             display other/more information
        -p, --programs           display PID/Program name for sockets
        -c, --continuous         continuous listing

        -l, --listening          display listening server sockets
        -a, --all, --listening   display all sockets (default: connected)
        -o, --timers             display timers
        -F, --fib                display Forwarding Information Base (default)
        -C, --cache              display routing cache instead of FIB

  <Socket>={-t|--tcp} {-u|--udp} {-w|--raw} {-x|--unix} --ax25 --ipx --netrom
  <AF>=Use '-6|-4' or '-A <af>' or '--<af>'; default: inet
  List of possible address families (which support routing):
    inet (DARPA Internet) inet6 (IPv6) ax25 (AMPR AX.25) 
    netrom (AMPR NET/ROM) ipx (Novell IPX) ddp (Appletalk DDP) 
    x25 (CCITT X.25) 
root@master2:~# netstat -tlumppnp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1540/sshd       
tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      1337/cupsd      
tcp        0      0 0.0.0.0:31297           0.0.0.0:*               LISTEN      1470/node       
tcp        0      0 127.0.0.1:10251         0.0.0.0:*               LISTEN      1383/kube-scheduler
tcp        0      0 172.31.125.181:2379     0.0.0.0:*               LISTEN      1328/etcd       
tcp        0      0 127.0.0.1:2379          0.0.0.0:*               LISTEN      1328/etcd       
tcp        0      0 172.31.125.181:2380     0.0.0.0:*               LISTEN      1328/etcd       
tcp        0      0 0.0.0.0:5901            0.0.0.0:*               LISTEN      1696/Xtightvnc  
tcp        0      0 127.0.0.1:8080          0.0.0.0:*               LISTEN      1379/kube-apiserver
tcp        0      0 0.0.0.0:6001            0.0.0.0:*               LISTEN      1696/Xtightvnc  
tcp6       0      0 :::22                   :::*                    LISTEN      1540/sshd       
tcp6       0      0 ::1:631                 :::*                    LISTEN      1337/cupsd      
tcp6       0      0 :::6443                 :::*                    LISTEN      1379/kube-apiserver
tcp6       0      0 :::10252                :::*                    LISTEN      1330/kube-controlle
tcp6       0      0 :::10257                :::*                    LISTEN      1330/kube-controlle
tcp6       0      0 :::10259                :::*                    LISTEN      1383/kube-scheduler
udp        0      0 0.0.0.0:68              0.0.0.0:*                           931/dhclient    
udp        0      0 0.0.0.0:5353            0.0.0.0:*                           1331/avahi-daemon: 
udp        0      0 0.0.0.0:46511           0.0.0.0:*                           1331/avahi-daemon: 
udp        0      0 0.0.0.0:631             0.0.0.0:*                           1375/cups-browsed
udp6       0      0 :::5353                 :::*                                1331/avahi-daemon: 
udp6       0      0 fe80::14:d9ff:febd::546 :::*                                1164/dhclient   
udp6       0      0 :::42635                :::*                                1331/avahi-daemon: 
root@master2:~# netstat -tlunpmpnpnetstat -tlunpmprn python app.pyvi app.py
  "app.py" 38L, 996Cimport os
from flask import Flask
from flask import render_template
import socket
import random
import os

app = Flask(__name__)

color_codes = {
    "red": "#e74c3c",
    "green": "#16a085",
    "blue": "#2980b9",
    "blue2": "#30336b",
    "pink": "#be2edd",
    "darkblue": "#130f40"
}

color = os.environ.get('APP_COLOR') or random.choice(["red","green","blue","blue2","darkblue","pink"])

@app.route("/")
def main():
    #return 'Hello'
    print(color)
    return render_template('hello.html', name=socket.gethostname(), color=color_codes[color])

@app.route('/color/<new_color>')
def new_color(new_color):
    return render_template('hello.html', name=socket.gethostname(), color=color_codes[new_color])

@app.route('/read_file')
def read_file():
    f = open("/data/testfile.txt")
    contents = f.read()
    return render_template('hello.html', name=socket.gethostname(), contents=contents, color=color_codes[color])

if __name__ == "__main__":
    app.run(host="0.0.0.0", port="8080")
~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            1,1All234567,0-18,1  9,0-110,1 123456{}7{}8,0-19,1  20,0-11,1  23456,0-17,1  8930,0-11,1  23456,0-17,1  823456789101n()2()345678920123456789301234565080")80")0")-- INSERT --38,35All30")620")70")840")938,38All:wq!"app.py" 38L, 997C written
root@master2:~# vi app.pynetstat -tlunpmprn python app.py
 * Serving Flask app "app" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on http://0.0.0.0:32040/ (Press CTRL+C to quit)
^Croot@master2:~# python app.py vi app.py
  "app.py" 38L, 997Cimport os
from flask import Flask
from flask import render_template
import socket
import random
import os

app = Flask(__name__)

color_codes = {
    "red": "#e74c3c",
    "green": "#16a085",
    "blue": "#2980b9",
    "blue2": "#30336b",
    "pink": "#be2edd",
    "darkblue": "#130f40"
}

color = os.environ.get('APP_COLOR') or random.choice(["red","green","blue","blue2","darkblue","pink"])

@app.route("/")
def main():
    #return 'Hello'
    print(color)
    return render_template('hello.html', name=socket.gethostname(), color=color_codes[color])

@app.route('/color/<new_color>')
def new_color(new_color):
    return render_template('hello.html', name=socket.gethostname(), color=color_codes[new_color])

@app.route('/read_file')
def read_file():
    f = open("/data/testfile.txt")
    contents = f.read()
    return render_template('hello.html', name=socket.gethostname(), contents=contents, color=color_codes[color])

if __name__ == "__main__":
    app.run(host="0.0.0.0", port="32040")
~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            1,1All234567,0-18,1  9,0-110,1 123456{}7{}8,0-19,1  20,0-11,1  23456,0-17,1  8930,0-11,1  23456,0-17,1  823456789101n()2()3456789.0.0.0", port="32040")0.0.0", port="32040").0.0", port="32040")0.0", port="32040").0", port="32040")0", port="32040")", port="32040")-- INSERT --38,19Allmaster1", port="32040")2638,25All:wq!"app.py" 38L, 997C written
root@master2:~# vi app.pypython app.py 
 * Serving Flask app "app" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
Traceback (most recent call last):
  File "app.py", line 38, in <module>
    app.run(host="master1", port="32040")
  File "/usr/local/lib/python2.7/dist-packages/flask/app.py", line 990, in run
    run_simple(host, port, self, **options)
  File "/usr/local/lib/python2.7/dist-packages/werkzeug/serving.py", line 1010, in run_simple
    inner()
  File "/usr/local/lib/python2.7/dist-packages/werkzeug/serving.py", line 963, in inner
    fd=fd,
  File "/usr/local/lib/python2.7/dist-packages/werkzeug/serving.py", line 806, in make_server
    host, port, app, request_handler, passthrough_errors, ssl_context, fd=fd
  File "/usr/local/lib/python2.7/dist-packages/werkzeug/serving.py", line 699, in __init__
    HTTPServer.__init__(self, server_address, handler)
  File "/usr/lib/python2.7/SocketServer.py", line 417, in __init__
    self.server_bind()
  File "/usr/lib/python2.7/BaseHTTPServer.py", line 108, in server_bind
    SocketServer.TCPServer.server_bind(self)
  File "/usr/lib/python2.7/SocketServer.py", line 431, in server_bind
    self.socket.bind(self.server_address)
  File "/usr/lib/python2.7/socket.py", line 228, in meth
    return getattr(self._sock,name)(*args)
socket.gaierror: [Errno -2] Name or service not known
root@master2:~# python app.py vi app.py
  "app.py" 38L, 997Cimport os
from flask import Flask
from flask import render_template
import socket
import random
import os

app = Flask(__name__)

color_codes = {
    "red": "#e74c3c",
    "green": "#16a085",
    "blue": "#2980b9",
    "blue2": "#30336b",
    "pink": "#be2edd",
    "darkblue": "#130f40"
}

color = os.environ.get('APP_COLOR') or random.choice(["red","green","blue","blue2","darkblue","pink"])

@app.route("/")
def main():
    #return 'Hello'
    print(color)
    return render_template('hello.html', name=socket.gethostname(), color=color_codes[color])

@app.route('/color/<new_color>')
def new_color(new_color):
    return render_template('hello.html', name=socket.gethostname(), color=color_codes[new_color])

@app.route('/read_file')
def read_file():
    f = open("/data/testfile.txt")
    contents = f.read()
    return render_template('hello.html', name=socket.gethostname(), contents=contents, color=color_codes[color])

if __name__ == "__main__":
    app.run(host="master1", port="32040")
~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            1,1All234567,0-18,1  9,0-110,1 123456{}7{}8,0-19,1  20,0-11,1  23456,0-17,1  8930,0-11,1  23456,0-17,1  823456789101n()2()34567892019aster1", port="32040")ster1", port="32040")ter1", port="32040")er1", port="32040")r1", port="32040")1", port="32040")", port="32040")-- INSERT --38,19All172.31.122.25", port="32040")3238,31All:wq!"app.py" 38L, 1003C written
root@master2:~# vi app.pypython app.py 
 * Serving Flask app "app" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
Traceback (most recent call last):
  File "app.py", line 38, in <module>
    app.run(host="172.31.122.25", port="32040")
  File "/usr/local/lib/python2.7/dist-packages/flask/app.py", line 990, in run
    run_simple(host, port, self, **options)
  File "/usr/local/lib/python2.7/dist-packages/werkzeug/serving.py", line 1010, in run_simple
    inner()
  File "/usr/local/lib/python2.7/dist-packages/werkzeug/serving.py", line 963, in inner
    fd=fd,
  File "/usr/local/lib/python2.7/dist-packages/werkzeug/serving.py", line 806, in make_server
    host, port, app, request_handler, passthrough_errors, ssl_context, fd=fd
  File "/usr/local/lib/python2.7/dist-packages/werkzeug/serving.py", line 699, in __init__
    HTTPServer.__init__(self, server_address, handler)
  File "/usr/lib/python2.7/SocketServer.py", line 417, in __init__
    self.server_bind()
  File "/usr/lib/python2.7/BaseHTTPServer.py", line 108, in server_bind
    SocketServer.TCPServer.server_bind(self)
  File "/usr/lib/python2.7/SocketServer.py", line 431, in server_bind
    self.socket.bind(self.server_address)
  File "/usr/lib/python2.7/socket.py", line 228, in meth
    return getattr(self._sock,name)(*args)
socket.error: [Errno 99] Cannot assign requested address
root@master2:~# 
root@master2:~# 
root@master2:~# python app.py vi app.py
  "app.py" 38L, 1003Cimport os
from flask import Flask
from flask import render_template
import socket
import random
import os

app = Flask(__name__)

color_codes = {
    "red": "#e74c3c",
    "green": "#16a085",
    "blue": "#2980b9",
    "blue2": "#30336b",
    "pink": "#be2edd",
    "darkblue": "#130f40"
}

color = os.environ.get('APP_COLOR') or random.choice(["red","green","blue","blue2","darkblue","pink"])

@app.route("/")
def main():
    #return 'Hello'
    print(color)
    return render_template('hello.html', name=socket.gethostname(), color=color_codes[color])

@app.route('/color/<new_color>')
def new_color(new_color):
    return render_template('hello.html', name=socket.gethostname(), color=color_codes[new_color])

@app.route('/read_file')
def read_file():
    f = open("/data/testfile.txt")
    contents = f.read()
    return render_template('hello.html', name=socket.gethostname(), contents=contents, color=color_codes[color])

if __name__ == "__main__":
    app.run(host="172.31.122.25", port="32040")
~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            ~                                                                                                                            1,1All234567,0-18,1  9,0-110,1 123456{}7{}8,0-19,1  20,0-11,1  23456,0-17,1  8930,0-11,1  23456,0-17,1  823456789101n()2()3456789201972.31.122.25", port="32040")2.31.122.25", port="32040").31.122.25", port="32040")31.122.25", port="32040")1.122.25", port="32040").122.25", port="32040")122.25", port="32040")22.25", port="32040").25", port="32040").25", port="32040")25", port="32040")5", port="32040")", port="32040")-- INSERT --38,19All1", port="32040")202", port="32040")17", port="32040")2.", port="32040")30", port="32040")4.", port="32040")50", port="32040")6.", port="32040")71", port="32040")838,27All:wq!"app.py" 38L, 999C written
root@master2:~# vi app.pypython app.py 
 * Serving Flask app "app" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on http://127.0.0.1:32040/ (Press CTRL+C to quit)
